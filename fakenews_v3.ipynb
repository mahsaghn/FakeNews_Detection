{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "add_cred_fakenews_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "jhIWDFaBoEY0",
        "6Mb30c58Byq0",
        "DSZ81ypY8Lud",
        "7uqrNtwUz4so",
        "FMxhC2BqF4Nu",
        "fSzdqzsD46HY",
        "4bv7G7ZjXO8L",
        "eCEeJbhoXSXP",
        "xCWdm8IRXUlW",
        "YlJXHKzRXYkF"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhIWDFaBoEY0"
      },
      "source": [
        "#Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-OGQzlboBKI",
        "outputId": "714c4f95-e813-4bc3-ce63-3ba0ca3733fd"
      },
      "source": [
        "!pip install -U nltk\n",
        "!pip install pandas\n",
        "!pip install hazm\n",
        "!pip install sklearn\n",
        "!pip install numpy\n",
        "!pip install flair\n",
        "!pip install stanfordnlp\n",
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Collecting nltk\n",
            "  Downloading nltk-3.6.3-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.62.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Installing collected packages: nltk\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.3\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[K     |████████████████████████████████| 316 kB 11.1 MB/s \n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[K     |████████████████████████████████| 233 kB 49.3 MB/s \n",
            "\u001b[?25hCollecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4 MB 49.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk==3.3->hazm) (1.15.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394488 sha256=977f21e1a4fd6ce2fcdf4a6c4c006d1ac8e6286abd8040659ed75b907b405699\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/fd/0c/d92302c876e5de87ebd7fc0979d82edb93e2d8d768bf71fac4\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp37-cp37m-linux_x86_64.whl size=154400 sha256=7cb2e4df3510b33c2f41c001b696b8818011e2a7260a4d35e417c0999f1256d4\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/b2/5b/0fe4b8f5c0e65341e8ea7bb3f4a6ebabfe8b1ac31322392dbf\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.6.3\n",
            "    Uninstalling nltk-3.6.3:\n",
            "      Successfully uninstalled nltk-3.6.3\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Collecting flair\n",
            "  Downloading flair-0.9-py3-none-any.whl (319 kB)\n",
            "\u001b[K     |████████████████████████████████| 319 kB 12.8 MB/s \n",
            "\u001b[?25hCollecting mpld3==0.3\n",
            "  Downloading mpld3-0.3.tar.gz (788 kB)\n",
            "\u001b[K     |████████████████████████████████| 788 kB 46.2 MB/s \n",
            "\u001b[?25hCollecting sentencepiece==0.1.95\n",
            "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 17.0 MB/s \n",
            "\u001b[?25hCollecting more-itertools~=8.8.0\n",
            "  Downloading more_itertools-8.8.0-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting sqlitedict>=1.6.0\n",
            "  Downloading sqlitedict-1.7.0.tar.gz (28 kB)\n",
            "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.62.3)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.2)\n",
            "Collecting gdown==3.12.2\n",
            "  Downloading gdown-3.12.2.tar.gz (8.2 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
            "Collecting deprecated>=1.2.4\n",
            "  Downloading Deprecated-1.2.13-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[K     |████████████████████████████████| 981 kB 46.1 MB/s \n",
            "\u001b[?25hCollecting janome\n",
            "  Downloading Janome-0.4.1-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.7 MB 29.5 MB/s \n",
            "\u001b[?25hCollecting wikipedia-api\n",
            "  Downloading Wikipedia-API-0.5.4.tar.gz (18 kB)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.0.19-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 5.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
            "Collecting bpemb>=0.3.2\n",
            "  Downloading bpemb-0.3.3-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
            "Collecting conllu>=4.0\n",
            "  Downloading conllu-4.4.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
            "Collecting ftfy\n",
            "  Downloading ftfy-6.0.3.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.0 MB/s \n",
            "\u001b[?25hCollecting konoha<5.0.0,>=4.0.0\n",
            "  Downloading konoha-4.6.5-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: torch!=1.8,>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.9.0+cu111)\n",
            "Collecting segtok>=1.5.7\n",
            "  Downloading segtok-1.5.10.tar.gz (25 kB)\n",
            "Collecting transformers>=4.0.0\n",
            "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (1.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.3.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (1.19.5)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.2.1)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.6.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.12.0)\n",
            "Collecting requests\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 902 kB/s \n",
            "\u001b[?25hCollecting importlib-metadata<4.0.0,>=3.7.0\n",
            "  Downloading importlib_metadata-3.10.1-py3-none-any.whl (14 kB)\n",
            "Collecting overrides<4.0.0,>=3.0.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.6.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2021.5.30)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 50.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 51.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 45.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.7.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
            "Building wheels for collected packages: gdown, mpld3, overrides, segtok, sqlitedict, ftfy, langdetect, wikipedia-api\n",
            "  Building wheel for gdown (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gdown: filename=gdown-3.12.2-py3-none-any.whl size=9704 sha256=6e7ca9ff1bec5c9d9bbcb29a4265d1b21d033cbd7f4d93c7bd76f8532d65f6bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/e0/7e/726e872a53f7358b4b96a9975b04e98113b005cd8609a63abc\n",
            "  Building wheel for mpld3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpld3: filename=mpld3-0.3-py3-none-any.whl size=116702 sha256=a4344b6ff30bf33a90c8e0899268fec1a4c668d454a0f0f116f3cb8ff9a46b26\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/70/6a/1c79e59951a41b4045497da187b2724f5659ca64033cf4548e\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10186 sha256=11226ad17a935ed5c9a6053759d8570aa761095c70a9313ddbdbcc73c028f1e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/3a/0d/38/01a9bc6e20dcfaf0a6a7b552d03137558ba1c38aea47644682\n",
            "  Building wheel for segtok (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segtok: filename=segtok-1.5.10-py3-none-any.whl size=25030 sha256=f38d2de052b40b1bfbd248be8400f91c46c8814592cdd076351cff861dc6e934\n",
            "  Stored in directory: /root/.cache/pip/wheels/67/b7/d0/a121106e61339eee5ed083bc230b1c8dc422c49a5a28c2addd\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-py3-none-any.whl size=14392 sha256=b308351ad2e609c05e32765c4196534f2c5c84a3c6ae06b7b49aaf5865c63788\n",
            "  Stored in directory: /root/.cache/pip/wheels/af/94/06/18c0e83e9e227da8f3582810b51f319bbfd181e508676a56c8\n",
            "  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41933 sha256=84c212bc4aaed2093924263021a85b08ccf5dcdeed5f88650d50490eb67b4282\n",
            "  Stored in directory: /root/.cache/pip/wheels/19/f5/38/273eb3b5e76dfd850619312f693716ac4518b498f5ffb6f56d\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993242 sha256=c7f616d05375f2a8464476837ba92bd89e50fc36726ea6e507d9789674f39b78\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/96/8a/f90c59ed25d75e50a8c10a1b1c2d4c402e4dacfa87f3aff36a\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.5.4-py3-none-any.whl size=13475 sha256=b421a9cc3101e192de03ef0e0f61917f480955016f0ecea8a7922e980c7f8a88\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/24/56/58ba93cf78be162451144e7a9889603f437976ef1ae7013d04\n",
            "Successfully built gdown mpld3 overrides segtok sqlitedict ftfy langdetect wikipedia-api\n",
            "Installing collected packages: requests, pyyaml, importlib-metadata, tokenizers, sentencepiece, sacremoses, overrides, huggingface-hub, wikipedia-api, transformers, sqlitedict, segtok, mpld3, more-itertools, langdetect, konoha, janome, gdown, ftfy, deprecated, conllu, bpemb, flair\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 4.8.1\n",
            "    Uninstalling importlib-metadata-4.8.1:\n",
            "      Successfully uninstalled importlib-metadata-4.8.1\n",
            "  Attempting uninstall: more-itertools\n",
            "    Found existing installation: more-itertools 8.10.0\n",
            "    Uninstalling more-itertools-8.10.0:\n",
            "      Successfully uninstalled more-itertools-8.10.0\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 3.6.4\n",
            "    Uninstalling gdown-3.6.4:\n",
            "      Successfully uninstalled gdown-3.6.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed bpemb-0.3.3 conllu-4.4.1 deprecated-1.2.13 flair-0.9 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.19 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 more-itertools-8.8.0 mpld3-0.3 overrides-3.1.0 pyyaml-5.4.1 requests-2.26.0 sacremoses-0.0.46 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 transformers-4.11.3 wikipedia-api-0.5.4\n",
            "Collecting stanfordnlp\n",
            "  Downloading stanfordnlp-0.2.0-py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 10.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (4.62.3)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.9.0+cu111)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from stanfordnlp) (2.26.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->stanfordnlp) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->stanfordnlp) (1.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (1.24.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->stanfordnlp) (2021.5.30)\n",
            "Installing collected packages: stanfordnlp\n",
            "Successfully installed stanfordnlp-0.2.0\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Mb30c58Byq0"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLQ0-YGbBoFm"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn import metrics\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import stanfordnlp\n",
        "import numpy as np\n",
        "import dill\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "import os.path\n",
        "import joblib\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from transformers import AutoConfig, AutoTokenizer, TFAutoModel\n",
        "\n",
        "import os\n",
        "import string\n",
        "import pandas as pd\n",
        "from hazm import *\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plot\n",
        "from keras.utils.vis_utils import plot_model\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import dill\n",
        "import math\n",
        "\n",
        "\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.regularizers import l1, l2\n",
        "from keras import regularizers\n",
        "from keras import metrics\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dj_wNk4ibwqm"
      },
      "source": [
        "#Google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDqlb6kRb6aO"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ze86DRxw5RtO"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSZ81ypY8Lud"
      },
      "source": [
        "#PS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPlqj4sb6jDV"
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"PSFeatureExtractor.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1-zwppOfr0Cr1k15U5cOBA8Kd3xdKSXWr\n",
        "\"\"\"\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"BaseLineWithGridSearch.ipynb\n",
        "\n",
        "Automatically generated by Colaboratory.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1cxXw92aQZKvWJGmOdKIYGGXRMpK_XNvQ\n",
        "\n",
        "# Import Required Libraries\n",
        "\"\"\"\n",
        "import nltk\n",
        "from nltk import word_tokenize as nltk_word_tokenize\n",
        "import string\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab import drive\n",
        "import torch\n",
        "import stanfordnlp\n",
        "import itertools\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from hazm import *\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from difflib import SequenceMatcher\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from openpyxl import load_workbook\n",
        "import joblib\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import os.path\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "refute_hedge_reporte_words = ['جعلی',\n",
        "                   'تقلب',\n",
        "                   'فریب',\n",
        "                   'حیله',\n",
        "                   'کلاهبرداری',\n",
        "                   'شیادی',\n",
        "                   'دست انداختن',\n",
        "                   'گول زدن',\n",
        "                   'نادرست',\n",
        "                   'غلط',\n",
        "                   'کذب',\n",
        "                   'ساختگی',\n",
        "                   'قلابی',\n",
        "                   'انکار',\n",
        "                   'رد',\n",
        "                   'تکذیب',\n",
        "                   'تکذیب کردن',\n",
        "                   'تکذیب شد',\n",
        "                   'انکار کردن'\n",
        "                   'انکار می کند',\n",
        "                   'نه',\n",
        "                   'با وجود',\n",
        "                   'علیرغم',\n",
        "                   'با اینکه',\n",
        "                   'شک داشتن',\n",
        "                   'تردید کردن',\n",
        "                   'مظنون بودن',\n",
        "                   'شک',\n",
        "                   'تردید',\n",
        "                   'دو دلی',\n",
        "                   'گمان',\n",
        "                   'به گزارش'\n",
        "                   ,'ادعا شده'\n",
        "                   ,'به قول معروف'\n",
        "                   ,'بنا به گفته'\n",
        "                   , 'ظاهرا'\n",
        "                   ,'به نظر می رسد'\n",
        "                   ,'ادعا'\n",
        "                   ,'میتوانست'\n",
        "                   ,'می تواند'\n",
        "                   ,'از قرار معلوم'\n",
        "                   ,'مشخصا'\n",
        "                   ,'تا حد زیادی'\n",
        "                   ,'احتمال دارد'\n",
        "                   ,'شاید'\n",
        "                   ,'به طور عمده'\n",
        "                   ,'ممکن است'\n",
        "                   ,'گویا'\n",
        "                   ,'ممکن'\n",
        "                   ,'اغلب'\n",
        "                   ,'غالبا'\n",
        "                   ,'احتمالا'\n",
        "                   ,'احتمالاً'\n",
        "                   ,'محتملا'\n",
        "                   ,'گفته شده'\n",
        "                   ,'گزارش داد'\n",
        "                   ,'طبق گزارش'\n",
        "                   ,'شایعه'\n",
        "                   ,'شایعات'\n",
        "                   ,'شایعه شده'\n",
        "                   ,'قدری'\n",
        "                   ,'تا حدی'\n",
        "                   ,'تأیید نشده'\n",
        "]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "   # stuff only to run when not called via 'import' here\n",
        "   pass\n",
        "\n",
        "\"\"\"# Define Class\"\"\"\n",
        "\n",
        "# -------------------------------------\n",
        "class PSFeatureExtractor():\n",
        "  # -------------------------------------\n",
        "  def __init__(self, dataset_path, stopWord_path, polarity_dataset_path, stanford_models_path , use_google_drive = True, important_words = None\n",
        "  \t,claim_name=\"claim\",headline_name=\"headline\",label_name=\"label\",question_name=\"question\",part_name=\"part\",load_data = True, data = None\n",
        "  \t,uniq_claims={}):\n",
        "    self.dataset_path = dataset_path\n",
        "    self.stopWord_path = stopWord_path\n",
        "    self.polarity_dataset_path = polarity_dataset_path\n",
        "    self.use_google_drive = use_google_drive\n",
        "    self.stanford_models_path = stanford_models_path\n",
        "    self.important_words = important_words\n",
        "    self.clean_claims_headlines = []\n",
        "    self.clean_claims = []\n",
        "    self.clean_headlines = []\n",
        "    self.claim_name= claim_name\n",
        "    self.headline_name = headline_name\n",
        "    self.question_name = question_name\n",
        "    self.label_name = label_name\n",
        "    self.part_name = part_name\n",
        "    self.uniq_claims = uniq_claims\n",
        "\n",
        "    self.fa_stop_words = self.__get_stop_words()\n",
        "    print(\"before read\")\n",
        "    if load_data:\n",
        "      self.claims, self.headlines, self.isQuestion,self.hasTowParts, self.labels =  self.__read_dataset(uniq_claims) \n",
        "    else:\n",
        "      self.claims, self.headlines, self.isQuestion,self.hasTowParts, self.labels = self.__generate_dataset(data)\n",
        "    print(\"after read\")\n",
        "    self.fa_punctuations = ['،','«','»',':','؛','ْ','ٌ','ٍ','ُ','ِ','َ','ّ','ٓ','ٰ','-','*']\n",
        "    self.denied_words = self.fa_stop_words + list(string.punctuation) + list(self.fa_punctuations)\n",
        "  # -------------------------------------\n",
        "  def __get_stop_words(self):   \n",
        "      normalizer = Normalizer()\n",
        "      lineList = list()\n",
        "      print(self.stopWord_path)\n",
        "      with open(self.stopWord_path) as f:\n",
        "        for line in f:\n",
        "          lineList.append(normalizer.normalize(line.rstrip(\"\\n\\r\")))\n",
        "      return lineList\n",
        "  # ---------------------------------------------------\n",
        "  def clean_sentence(self, sentence):\n",
        "    normalizer = Normalizer()\n",
        "    shayee = normalizer.normalize(\"شایعه\")\n",
        "    clean_sentences = sentence\n",
        "    re_pattern1 = \"(/(\\s)*\"+ shayee +\"(\\s)*[0-9]+)|(/(\\s)*شایعه(\\s)*[0-9]+)\"\n",
        "    re_pattern2 = \"/(\\s)*[0-9]+\"\n",
        "    re_pattern3 = \"\\\\u200c|\\\\u200d|\\\\u200e|\\\\u200b|\\\\u2067|\\\\u2069\"\n",
        "    x = re.search(re_pattern1, sentence)\n",
        "    if (x):\n",
        "      clean_sentences = re.sub(re_pattern1, \"\", sentence)\n",
        "\n",
        "    x = re.search(re_pattern2, clean_sentences)\n",
        "    if (x):\n",
        "      clean_sentences = re.sub(re_pattern2, \"\", clean_sentences)\n",
        "        \n",
        "    x = re.search(re_pattern3, clean_sentences)\n",
        "    if (x):\n",
        "      clean_sentences = re.sub(re_pattern3, \"\", clean_sentences)   \n",
        "        \n",
        "    punc_regex = re.compile('|'.join(map(re.escape, list(string.punctuation) + list(self.fa_punctuations))))\n",
        "\n",
        "    clean_sentences = punc_regex.sub(\"\", clean_sentences)\n",
        "\n",
        "    return clean_sentences\n",
        "  # ---------------------------------------------\n",
        "  def __generate_dataset(self,data):\n",
        "    claims = data[self.claim_name]\n",
        "    headline = data[self.headline_name]\n",
        "    b = np.char.find(claims, '?')\n",
        "    isQuestion = np.zeros(claims.shape[0])\n",
        "    isQuestion[np.where(b>0)] = 1\n",
        "    hasTowParts = np.zeros(claims.shape[0])\n",
        "    return claims, headlines,isQuestion,hasTowParts ,labels\n",
        "  # ---------------------------------------------\n",
        "  def __read_dataset(self,uniq_claims):\n",
        "    df = pd.read_csv(self.dataset_path, encoding = 'utf-8')\n",
        "    df = self.__remove_from_dataset(df,uniq_claims)\n",
        "    claims = df[self.claim_name].values\n",
        "    headlines = df[self.headline_name].values\n",
        "    print(\"before\")\n",
        "    isQuestion = df[self.question_name].values\n",
        "    print(\"after\")\n",
        "    hasTowParts = df[self.part_name].values\n",
        "    labels = df[self.label_name].values\n",
        "    print(isQuestion)\n",
        "    assert (claims.shape == headlines.shape == isQuestion.shape == labels.shape == hasTowParts.shape), \"The features size are not equal.\"\n",
        "    print(claims.shape , headlines.shape ,isQuestion.shape,hasTowParts.shape ,labels.shape)\n",
        "    return claims, headlines,isQuestion,hasTowParts ,labels\n",
        "  # ---------------------------------------\n",
        "  def __remove_from_dataset(self,df,uniq_claims):\n",
        "    uni_number = 0\n",
        "    if bool(uniq_claims):\n",
        "      df['repeated']= np.zeros(len(df),dtype=int)\n",
        "      for claim_index in range(len(df[self.claim_name])):\n",
        "        if (df[self.claim_name][claim_index] in uniq_claims) and uniq_claims[df[self.claim_name][claim_index]] == df[self.headline_name][claim_index]:\n",
        "          df['repeated'][claim_index] = 1\n",
        "          print(df[self.claim_name][claim_index])\n",
        "          print(df[self.headline_name][claim_index])\n",
        "          uni_number +=1\n",
        "      df.sort_values(by=['repeated'])\n",
        "      print(df['repeated'])\n",
        "    self.uniq_number = uni_number\n",
        "    return df\n",
        "  # ---------------------------------------\n",
        "\n",
        "  def stanford_tokenize(self, root_model_path, just_get_tokenized_words = False): \n",
        "    \n",
        "    nlp = stanfordnlp.Pipeline(lang='fa', models_dir= self.stanford_models_path, treebank=None, use_gpu=True) \n",
        "    # nlp = stanfordnlp.Pipeline(processors='tokenize,lemma', lang='fa', treebank=None, use_gpu=True)\n",
        "\n",
        "    claims_processors_result = []\n",
        "    headlines_processors_result = []\n",
        "    claims_tokenize = []\n",
        "    headlines_tokenize = []\n",
        "\n",
        "    for i, (claim,headline) in enumerate(zip(self.claims,self.headlines)):\n",
        "      clean_claim = self.clean_sentence(claim)\n",
        "      self.clean_claims.append(clean_claim)\n",
        "      doc = nlp(clean_claim) # Run the pipeline on input text\n",
        "      claims_processors_result.append(doc.sentences[0].words)\n",
        "      words = (obj.text for obj in doc.sentences[0].words)\n",
        "      claims_tokenize.append(words)\n",
        "\n",
        "      # headline\n",
        "      clean_headline = self.clean_sentence(headline)\n",
        "      self.clean_headlines.append(clean_headline)\n",
        "      doc = nlp(clean_headline) # Run the pipeline on input text\n",
        "      headlines_processors_result.append(doc.sentences[0].words)\n",
        "      words = (obj.text for obj in doc.sentences[0].words)\n",
        "      headlines_tokenize.append(words)\n",
        "      self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)\n",
        "\n",
        "    self.tokens_claims , self.tokens_headlines = self.clean_tokens(target_list = claims_tokenize), self.clean_tokens(target_list = headlines_tokenize)   \n",
        "    if just_get_tokenized_words :                \n",
        "      return self.tokens_claims , self.tokens_headlines \n",
        "# ghaGH\n",
        "    return claims_processors_result , headlines_processors_result\n",
        "  # ------------------------------------------------\n",
        "  \n",
        "  \n",
        "  def clean_tokens(self, target_list):\n",
        "    assert isinstance(target_list, (list)) == True , \"Type of target_list is not correct. It has to be list.\"\n",
        "    normalizer = Normalizer()\n",
        "    #--------!!!! It was [] and the following was commented, I uncomment it\n",
        "            \n",
        "    clean_words = []\n",
        "\n",
        "    for item in target_list:\n",
        "      clean_words.append([i for i in item if normalizer.normalize(i) not in self.denied_words])\n",
        "\n",
        "    return clean_words\n",
        "  # --------------------------------------------------\n",
        "  def hazm_tokenize(self):\n",
        "    claims_result = []\n",
        "    headlines_result = []\n",
        "    \n",
        "    for claim,headline in zip(self.claims,self.headlines):\n",
        "      clean_claim = self.clean_sentence(claim)\n",
        "      self.clean_claims.append(clean_claim)\n",
        "      claims_result.append(word_tokenize(clean_claim))\n",
        "      # headline\n",
        "      clean_headline = self.clean_sentence(headline)\n",
        "      self.clean_headlines.append(clean_headline)\n",
        "      headlines_result.append(word_tokenize(clean_headline))\n",
        "\n",
        "      self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)\n",
        "\n",
        "    self.tokens_claims , self.tokens_headlines = self.clean_tokens(target_list = claims_result), self.clean_tokens(target_list = headlines_result)\n",
        "    return self.tokens_claims , self.tokens_headlines\n",
        "\n",
        "  # --------------------------------------------------\n",
        "  def nltk_tokenize(self):\n",
        "    claims_result = []\n",
        "    headlines_result = []\n",
        "    \n",
        "    for claim,headline in zip(self.claims,self.headlines):\n",
        "      clean_claim = self.clean_sentence(claim)\n",
        "      self.clean_claims.append(clean_claim)\n",
        "      claims_result.append(nltk_word_tokenize(clean_claim))\n",
        "      \n",
        "      # headline\n",
        "      clean_headline = self.clean_sentence(headline)\n",
        "      self.clean_headlines.append(clean_headline)\n",
        "      headlines_result.append(nltk_word_tokenize(clean_headline))\n",
        "\n",
        "      self.clean_claims_headlines.append(clean_claim + ' ' + clean_headline)\n",
        "    self.tokens_claims , self.tokens_headlines = self.clean_tokens(target_list = claims_result), self.clean_tokens(target_list = headlines_result)\n",
        "    return self.tokens_claims , self.tokens_headlines\n",
        "  # --------------------------------------------------\n",
        "  def clean_sentences(self):\n",
        "    claims_result = []\n",
        "    headlines_result = []\n",
        "    for claim,headline in zip(self.claims,self.headlines):\n",
        "      tokens = parsbert_tokenizer.tokenize(claim)\n",
        "      clean_words = []\n",
        "      for token in tokens:\n",
        "          if token not in self.denied_words:\n",
        "              clean_words.append(token)\n",
        "      new_sentence_c = parsbert_tokenizer.convert_tokens_to_string(clean_words)\n",
        "      self.clean_claims.append(new_sentence_c)\n",
        "      \n",
        "      # headline\n",
        "      tokens = parsbert_tokenizer.tokenize(headline)\n",
        "      clean_words = []\n",
        "      for token in tokens:\n",
        "          if token not in self.denied_words:\n",
        "              clean_words.append(token)\n",
        "      new_sentence_h = parsbert_tokenizer.convert_tokens_to_string(clean_words)\n",
        "      self.clean_headlines.append(new_sentence_h)\n",
        "\n",
        "      self.clean_claims_headlines.append(new_sentence_c + ' ' + new_sentence_h)\n",
        "  # --------------------------------------------------\n",
        "  def tf_idf(self):\n",
        "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, norm='l2', ngram_range=(1, 2))\n",
        "    features = tfidf.fit_transform(self.clean_claims_headlines).toarray() \n",
        "    return features\n",
        "  # --------------------------------------------------\n",
        "  def tf_idf(self):\n",
        "    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=10, norm='l2', ngram_range=(1, 2))\n",
        "    features = tfidf.fit_transform(self.clean_claims_headlines).toarray() \n",
        "    return features\n",
        "  # --------------------------------------------------\n",
        "  def similarity(self):\n",
        "    feature = []\n",
        "    for i, (claim,headline) in enumerate(zip(self.clean_claims,self.clean_headlines)):\n",
        "      ratio = SequenceMatcher(None, claim, headline).ratio()\n",
        "      quick_ratio = SequenceMatcher(None, claim, headline).quick_ratio()\n",
        "      real_quick_ratio = SequenceMatcher(None, claim, headline).real_quick_ratio()\n",
        "      feature.append([ratio,quick_ratio,real_quick_ratio])\n",
        "    return feature    \n",
        "  # --------------------------------------------------\n",
        "  def calc_important_words(self):\n",
        "    assert (self.important_words != None), 'For calculating important words you should pass important words in initializer.'\n",
        "    features = np.zeros((len(self.clean_claims_headlines), len(self.important_words)))\n",
        "    for i in range(len(self.clean_claims_headlines)):\n",
        "      for j in range(len(self.important_words)):\n",
        "        if self.important_words[j] in self.clean_claims_headlines[i]:\n",
        "            features[i][j] = 1\n",
        "    return features\n",
        "  # --------------------------------------------------\n",
        "  def calculate_root_distance(self ,target_sentences = None): # target_sentences = clean_headlines\n",
        "    \n",
        "    if target_sentences == None:\n",
        "      target_sentences = self.clean_headlines\n",
        "    \n",
        "    nlp = stanfordnlp.Pipeline(lang='fa', models_dir= self.stanford_models_path, treebank=None, use_gpu=True) \n",
        "    root_distance_feature = np.zeros((len(target_sentences),1))\n",
        "    for index,headline in enumerate(target_sentences):\n",
        "      root_distance_feature[index] = -1\n",
        "      doc = nlp(headline)\n",
        "      root = [(i,doc.sentences[0].words[i].text) for i in range(len(doc.sentences[0].words)) if  doc.sentences[0].words[i].dependency_relation == 'root' ]\n",
        "      if(len(root) == 0):\n",
        "        continue\n",
        "\n",
        "      root_index,root_word = root[0]\n",
        "\n",
        "      for word_index,word in enumerate(headline.split()) :\n",
        "        target = [(i,refute_hedge_reporte_words[i]) for i in range(len(refute_hedge_reporte_words)) if  refute_hedge_reporte_words[i] == word]\n",
        "        if(len(target) > 0):\n",
        "          target_index, target_word =target[0]\n",
        "          root_distance_feature[index] = abs(word_index - root_index)\n",
        "          break\n",
        "    return root_distance_feature\n",
        "  # --------------------------------------------------\n",
        "  def load_polarity_dataset(self):\n",
        "    excel = load_workbook(filename = self.polarity_dataset_path)\n",
        "    sheet = excel.active\n",
        "    words_polarity_fa={}\n",
        "    for row in sheet.iter_rows():\n",
        "      if row[2].value == \"Polarity\" or row[2].value == None:\n",
        "        continue\n",
        "      words_polarity_fa[row[0].value] = row[2].value\n",
        "    return words_polarity_fa  \n",
        "  # --------------------------------------------------\n",
        "  def calculate_polarity(self, target_sentences = None):\n",
        "    words_polarity_fa = self.load_polarity_dataset()\n",
        "\n",
        "    if target_sentences == None:\n",
        "      target_sentences = zip(self.tokens_claims,self.tokens_headlines)\n",
        "\n",
        "    # unzipping values \n",
        "    mapped = list(target_sentences)   \n",
        "    claims,headlines = zip(*mapped) \n",
        "    claims_array = np.asarray(claims)\n",
        "    polarity_vector = np.zeros((len(claims_array), 30))\n",
        "\n",
        "    for i,(claim,headline) in enumerate(zip(claims,headlines)):\n",
        "      j = 0\n",
        "      while j < len(claim) and j< 15:\n",
        "        if claim[j] in words_polarity_fa:\n",
        "          polarity_vector[i][j] = words_polarity_fa[claim[j]]\n",
        "        j += 1\n",
        "      j = 0\n",
        "      while j < len(headline) and j< 15:\n",
        "        if headline[j] in words_polarity_fa:\n",
        "          polarity_vector[i][j+15] = words_polarity_fa[headline[j]]          \n",
        "        j += 1\n",
        "    return polarity_vector\n",
        "  # --------------------------------------------------\n",
        "  # Function to average all word vectors in a paragraph\n",
        "  def __feature_vector_method(self, words, model, num_features):\n",
        "    # Pre-initialising empty numpy array for speed\n",
        "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
        "    nwords = 0\n",
        "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
        "    index2word_set = set(model.wv.index2word)\n",
        "    \n",
        "    for nwords, word in enumerate(words):\n",
        "      if word in index2word_set:\n",
        "        featureVec = np.add(featureVec,model[word])\n",
        "    \n",
        "    # Dividing the result by number of words to get average\n",
        "    featureVec = np.divide(featureVec, nwords)\n",
        "    return featureVec    \n",
        "  # --------------------------------------------------\n",
        "  def get_w2v_feature(self, model, num_features, target_sentences = None):\n",
        "    \n",
        "    if target_sentences == None:\n",
        "      target_sentences = self.clean_claims_headlines\n",
        "\n",
        "    reviewFeatureVecs = np.zeros((len(target_sentences),num_features),dtype=\"float32\")\n",
        "    for counter, sentence in enumerate(target_sentences):\n",
        "      # Printing a status message every 10000th review\n",
        "      if counter%1000 == 0:\n",
        "        print(\"data %d of %d\"%(counter,len(target_sentences)))\n",
        "          \n",
        "      reviewFeatureVecs[counter] = self.__feature_vector_method(sentence, model, num_features)\n",
        "        \n",
        "    return reviewFeatureVecs\n",
        "  # --------------------------------------------------\n",
        "  def get_bow(self, target_sentences = None):\n",
        "    if target_sentences == None:\n",
        "      target_sentences = self.clean_claims_headlines\n",
        "    vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "    X = vectorizer.fit_transform(target_sentences)\n",
        "    return X.toarray()\n",
        "  # --------------------------------------------------\n",
        "  def generate_Features(self, w2v_model_path, save_path, load_path, save_feature = False, load_if_exist = True, similarity = True, important_words = True, is_question = True, more_than2_parts = True, root_distance = True, polarity = True , w2v = True , bow = True ,tfidf = True):\n",
        "    features = self.isQuestion\n",
        "    features = np.reshape(features,(len(features),1))\n",
        "    file_name = ''\n",
        "\n",
        "    if load_if_exist == True or save_feature == True:\n",
        "      if tfidf:\n",
        "        file_name += 'tfidf_'\n",
        "      if similarity:\n",
        "        file_name += 'similarity_'\n",
        "      if important_words:\n",
        "        file_name += 'important_words_'        \n",
        "      if is_question:\n",
        "        file_name += 'is_question_'\n",
        "      if more_than2_parts:\n",
        "        file_name += 'more_than2_parts_'\n",
        "      if root_distance:\n",
        "        file_name += 'root_distance_'\n",
        "      if polarity:\n",
        "        file_name += 'polarity_'    \n",
        "      if w2v:\n",
        "        file_name += 'w2v_'     \n",
        "      if bow:\n",
        "        file_name += 'bow_'    \n",
        "\n",
        "\n",
        "    if load_if_exist :\n",
        "      assert len(load_path) > 0, \"Please enter load_path.\"\n",
        "      load_file_name = load_path + '/' + file_name + '.pkl'\n",
        "      if os.path.isfile(load_file_name) == True :\n",
        "        features = joblib.load(load_file_name)\n",
        "        print('Features loaded successfully.')\n",
        "        return features, file_name\n",
        "      else:\n",
        "        print('Features vector file is not exist.')      \n",
        "    # -------------- tfidf ----------\n",
        "    if tfidf:\n",
        "      print('Start to generate tf_idf feature')\n",
        "      tf_idf_feature = self.tf_idf()\n",
        "      features = np.append(features, tf_idf_feature ,axis = 1)\n",
        "      print('End of tf_idf feature')\n",
        "    # -------------- similarity ----------\n",
        "    if similarity:\n",
        "      print('Start to generate similarity feature')\n",
        "      similarity_feature = self.similarity()\n",
        "      features = np.append(features, similarity_feature ,axis = 1)\n",
        "      print('End of similarity feature')\n",
        "    # -------------- important words ----------\n",
        "    if important_words:\n",
        "      print('Start to generate important words feature')\n",
        "      important_words_feature = self.calc_important_words()\n",
        "      features = np.append(features, important_words_feature ,axis = 1)\n",
        "      print('End of important words feature')\n",
        "    # -------------- is question ----------\n",
        "    if is_question == False:\n",
        "      features = features[:,1:]\n",
        "    else:\n",
        "      print('\"is question\" feature was added.')\n",
        "    # -------------- more than tow parts ----------\n",
        "    if more_than2_parts:\n",
        "      features = np.append(features, np.reshape(self.hasTowParts, (len(self.hasTowParts),1)) ,axis = 1)\n",
        "      print('\"more than tow parts\" feature was added.')\n",
        "    # -------------- root distance ----------\n",
        "    if root_distance:\n",
        "      print('Start to generate root distance feature')\n",
        "      root_distance_feature = self.calculate_root_distance()\n",
        "      features = np.append(features, root_distance_feature ,axis = 1)\n",
        "      print('End of root distance feature')\n",
        "    # -------------- root distance ----------\n",
        "    if polarity:\n",
        "      print('Start to generate polarity feature')\n",
        "      polarity_feature = self.calculate_polarity()\n",
        "      features = np.append(features, polarity_feature ,axis = 1)\n",
        "      print('End of polarity feature')    \n",
        "    # -------------- w2v ----------\n",
        "    if w2v:\n",
        "      print('Start to generate w2v feature')\n",
        "      assert len(w2v_model_path) > 0, \"Please enter w2v_model_path.\"\n",
        "      w2v_model = joblib.load(w2v_model_path)\n",
        "      w2v_feature = self.get_w2v_feature(w2v_model, num_features = 300)\n",
        "      w2v_feature = (w2v_feature - np.min(w2v_feature))/ (np.max(w2v_feature) - np.min(w2v_feature))\n",
        "      features = np.append(features, w2v_feature ,axis = 1)\n",
        "      print('End of w2v feature')   \n",
        "    # -------------- bow ----------\n",
        "    if bow:\n",
        "      print('Start to generate bow feature')\n",
        "      bow_feature = self.get_bow()\n",
        "      features = np.append(features, bow_feature ,axis = 1)\n",
        "      print('End of bow feature')          \n",
        "    \n",
        "    if save_feature:\n",
        "      joblib.dump(features, (save_path + '/' + file_name + '.pkl'))\n",
        "      print('Features saved successfully.')          \n",
        "    return features, file_name\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoJ_-nM-ZzZy"
      },
      "source": [
        "a = np.array(['dfmdk','dmiofvbioefbn?','dig?gsveiobvm'])\n",
        "b = np.char.find(a, '?')\n",
        "z = np.zeros(a.shape[0])\n",
        "z[np.where(b>0)] = 1\n",
        "z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uqrNtwUz4so"
      },
      "source": [
        "#Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxF5KWtM_3-d"
      },
      "source": [
        "stanford_models_path  = '/content/drive/My Drive/persian_stance_baseline_data' \n",
        "stopWord_path = '/content/drive/My Drive/persian_stance_baseline_data/dataset/stopWords.txt'\n",
        "polarity_dataset_path = '/content/drive/My Drive/persian_stance_baseline_data/dataset/PerSent.xlsx'\n",
        "save_load_path = \"/content/drive/My Drive/persian_stance_baseline_data/vectors\"\n",
        "w2v_model_path = \"/content/drive/My Drive/persian_stance_baseline_data/vectors/w2v_persian.pkl\"\n",
        "train_test_sets_save_path = \"/content/drive/My Drive/persian_stance_baseline_data/dataset\"\n",
        "train_test_sets_load_path = \"/content/drive/My Drive/persian_stance_baseline_data/dataset\"\n",
        "train_path = \"/content/drive/My Drive/FullDS_train.csv\"\n",
        "test_path = \"/content/drive/My Drive/FullDS_test.csv\"\n",
        "full_path = '/content/drive/My Drive/FullDS.csv'# this full dataset\n",
        "model_path = \"HooshvareLab/bert-base-parsbert-uncased\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS_LoGhkyMfa"
      },
      "source": [
        "MAX_LEN_a2c = 400  # max sequence length for the model\n",
        "MAX_LEN_h2c = 32  # max sequence length for the model\n",
        "GDRIVE_DIR = Path(\"/content/drive/My Drive\")  # path for google drive folder\n",
        "BEST_MODEL_PATH_a2c = '/content/drive/My Drive/multilingual_bert_dataset/a2c_parsbert_weights_newdataset.h5'\n",
        "# BEST_MODEL_PATH_h2c = '/content/drive/My Drive/h2c_parsbert_weights_WithExtraFeatures.h5'\n",
        "BEST_MODEL_PATH_h2c = '/content/drive/My Drive/multilingual_bert_dataset/h2c_parsbert_weights_newdataset.h5'\n",
        "FA_PUNCTUATIONS = ['،','«','»',':','؛','ْ','ٌ','ٍ','ُ','ِ','َ','ّ','ٓ','ٰ','-','*']\n",
        "CLEAN_PUNCTUATION = False\n",
        "CLEAN_STOP_WORDS = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMxhC2BqF4Nu"
      },
      "source": [
        "#Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7FfU300BTHF"
      },
      "source": [
        "parsbert_config = AutoConfig.from_pretrained(model_path, num_labels = 4)\n",
        "parsbert_tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "parsbert_model = TFAutoModel.from_pretrained(model_path, config = parsbert_config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koWXw-VyBKgF"
      },
      "source": [
        "labels = list(parsbert_config.label2id.keys())\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9K7_r30k1_B"
      },
      "source": [
        "###functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHE-3UoZ11vl"
      },
      "source": [
        "\"\"\"\n",
        "Define our custom loss function.\n",
        "\"\"\"\n",
        "\n",
        "#FL1\n",
        "def categorical_focal_loss(gamma=2., alpha=.25):\n",
        "    \"\"\"\n",
        "    Softmax version of focal loss.\n",
        "           m\n",
        "      FL = ∑  -alpha * (1 - p_o,c)^gamma * y_o,c * log(p_o,c)\n",
        "          c=1\n",
        "      where m = number of classes, c = class and o = observation\n",
        "    Parameters:\n",
        "      alpha -- the same as weighing factor in balanced cross entropy\n",
        "      gamma -- focusing parameter for modulating factor (1-p)\n",
        "    Default value:\n",
        "      gamma -- 2.0 as mentioned in the paper\n",
        "      alpha -- 0.25 as mentioned in the paper\n",
        "    References:\n",
        "        Official paper: https://arxiv.org/pdf/1708.02002.pdf\n",
        "        https://www.tensorflow.org/api_docs/python/tf/keras/backend/categorical_crossentropy\n",
        "    Usage:\n",
        "     model.compile(loss=[categorical_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
        "    \"\"\"\n",
        "    def categorical_focal_loss_fixed(y_true, y_pred):\n",
        "        \"\"\"\n",
        "        :param y_true: A tensor of the same shape as `y_pred`\n",
        "        :param y_pred: A tensor resulting from a softmax\n",
        "        :return: Output tensor.\n",
        "        \"\"\"\n",
        "\n",
        "        # Scale predictions so that the class probas of each sample sum to 1\n",
        "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
        "\n",
        "        # Clip the prediction value to prevent NaN's and Inf's\n",
        "        epsilon = K.epsilon()\n",
        "        y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
        "\n",
        "        y_true = float(y_true)\n",
        "        # Calculate Cross Entropy\n",
        "        cross_entropy = -y_true * K.log(y_pred)\n",
        "\n",
        "        # Calculate Focal Loss\n",
        "        loss = alpha * K.pow(1 - y_pred, gamma) * cross_entropy\n",
        "\n",
        "        # Compute mean loss in mini_batch\n",
        "        return K.mean(loss, axis=1)\n",
        "\n",
        "    return categorical_focal_loss_fixed\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Test serialization of nested functions\n",
        "    cat_inner = dill.loads(dill.dumps(categorical_focal_loss(gamma=2., alpha=.25)))\n",
        "    print(cat_inner)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSzdqzsD46HY"
      },
      "source": [
        "#Load Bert Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs9YdHZ34PJL"
      },
      "source": [
        "##Article to claim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuzl9sAMw-fN"
      },
      "source": [
        "###Create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY38PqP4w96h"
      },
      "source": [
        "# BIAS INITILIZATION \n",
        "pi = 0.01\n",
        "b = -math.log((1-pi)/pi)\n",
        "bias_initializer = keras.initializers.Constant(value=b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z3jajK_cxCDG"
      },
      "source": [
        "input_ids = layers.Input(shape=(MAX_LEN_a2c,), dtype=tf.int32, name = 'input_ids')\n",
        "token_type_ids = layers.Input(shape=(MAX_LEN_a2c,), dtype=tf.int32, name = 'token_type_ids')\n",
        "attention_mask = layers.Input(shape=(MAX_LEN_a2c,), dtype=tf.int32, name = 'attention_mask')\n",
        "\n",
        "bert = parsbert_model(\n",
        "    input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        ")[0]\n",
        "\n",
        "hidden_mean = tf.reduce_mean(bert, axis=1)\n",
        "x = keras.layers.Dense(units = MAX_LEN_a2c, activation='relu')(hidden_mean)\n",
        "classifier = keras.layers.Dense(units = 4, name = 'classifier', activation = 'softmax')(x)\n",
        "# classifier = keras.layers.Dense(units = 4, name = 'classifier', activation = 'softmax', use_bias = True, \n",
        "#                                 bias_initializer = bias_initializer)(x)\n",
        "\n",
        "model_a2c = keras.Model(\n",
        "        inputs = [input_ids, token_type_ids, attention_mask],\n",
        "        outputs = [classifier],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9h4RoBIuxK2y"
      },
      "source": [
        "lr = 0.000035"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHJ8oi_ucwpD"
      },
      "source": [
        "def get_f1(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0rAa7IcxNIe"
      },
      "source": [
        "loss = keras.losses.CategoricalCrossentropy()\n",
        "focal_loss = [categorical_focal_loss(alpha=0.25, gamma=2)]\n",
        "optimizer = keras.optimizers.Adam(learning_rate = lr)\n",
        "\n",
        "# callbacks_list = [\n",
        "#   EarlyStopping(monitor='val_loss', patience = 5),\n",
        "#   ModelCheckpoint(filepath=BEST_MODEL_PATH_a2c, save_best_only=True, save_weights_only = True, monitor='val_accuracy')\n",
        "# ]\n",
        "\n",
        "metrics_list = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), get_f1]\n",
        "\n",
        "model_a2c.compile(optimizer = optimizer, loss = loss, metrics = metrics_list)\n",
        "# model.compile(optimizer = optimizer, loss = focal_loss, metrics = ['accuracy'])\n",
        "model_a2c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqFwu9-C4q_t"
      },
      "source": [
        "###Load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBOKI7ScxRp9"
      },
      "source": [
        "model_a2c.load_weights(BEST_MODEL_PATH_a2c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7Az-uTC33YE"
      },
      "source": [
        "##Headline to claim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwftJFLA6TwA"
      },
      "source": [
        "###create"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYS9AMy-6WFo"
      },
      "source": [
        "# BIAS INITILIZATION \n",
        "pi = 0.01\n",
        "b = -math.log((1-pi)/pi)\n",
        "bias_initializer = keras.initializers.Constant(value=b)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unihv5dO6Yi9"
      },
      "source": [
        "input_ids = layers.Input(shape=(MAX_LEN_h2c,), dtype=tf.int32, name = 'input_ids')\n",
        "token_type_ids = layers.Input(shape=(MAX_LEN_h2c,), dtype=tf.int32, name = 'token_type_ids')\n",
        "attention_mask = layers.Input(shape=(MAX_LEN_h2c,), dtype=tf.int32, name = 'attention_mask')\n",
        "\n",
        "bert = parsbert_model(\n",
        "    input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask\n",
        ")[0]\n",
        "\n",
        "hidden_mean = tf.reduce_mean(bert, axis=1)\n",
        "x = keras.layers.Dense(units = MAX_LEN_h2c, activation='relu')(hidden_mean)\n",
        "classifier = keras.layers.Dense(units = 4, name = 'classifier', activation = 'softmax')(x)\n",
        "# classifier = keras.layers.Dense(units = 4, name = 'classifier', activation = 'softmax', use_bias = True, \n",
        "                                #bias_initializer = bias_initializer)(x)\n",
        "\n",
        "model_h2c = keras.Model(\n",
        "        inputs = [input_ids, token_type_ids, attention_mask],\n",
        "        outputs = [classifier],\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8-yWHY16lQ2"
      },
      "source": [
        "lr = 0.000035\n",
        "loss = keras.losses.CategoricalCrossentropy()\n",
        "focal_loss = [categorical_focal_loss(alpha=0.1, gamma=4)]\n",
        "optimizer = keras.optimizers.Adam(lr)\n",
        "\n",
        "callbacks_list = [\n",
        "                  EarlyStopping(monitor='val_accuracy', patience=5),\n",
        "                  ModelCheckpoint(filepath='/content/drive/My Drive/multilingual_bert_dataset/h2c_parsbert_weights_newdataset.h5', save_best_only=True, save_weights_only = True, monitor='val_accuracy')\n",
        "]\n",
        "\n",
        "# metrics_list = ['accuracy', f1_score_m, precision_m, recall_m]\n",
        "metrics_list = ['accuracy', keras.metrics.Precision(), keras.metrics.Recall(), get_f1]\n",
        "model_h2c.compile(optimizer = optimizer, loss = loss, metrics = metrics_list)\n",
        "# model.compile(optimizer = optimizer, loss = focal_loss, metrics = metrics_list)\n",
        "model_h2c.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SDiYjQH7FIR"
      },
      "source": [
        "###load"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE6fzKhY6spi"
      },
      "source": [
        "model_h2c.load_weights(BEST_MODEL_PATH_h2c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDFQcyPiytKI"
      },
      "source": [
        "#Load Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bv7G7ZjXO8L"
      },
      "source": [
        "##Load_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UREmPmH-DQ97"
      },
      "source": [
        "class Load_data():\n",
        "  def __init__(self):\n",
        "    self.load_data()\n",
        "    self.generate_features()\n",
        "\n",
        "\n",
        "  def convert_data_to_one_hot(self,y_train):\n",
        "    y_train_temp = np.zeros((y_train.shape[0], 4), dtype=np.int)\n",
        "    \n",
        "    for i,label in enumerate(y_train):\n",
        "      if(label == 'Agree'):\n",
        "        y_train_temp[i] = [0,0,0,1]\n",
        "      elif (label == 'Disagree'):\n",
        "        y_train_temp[i] = [0,0,1,0]\n",
        "      elif (label == 'Discuss'):\n",
        "        y_train_temp[i] = [0,1,0,0]\n",
        "      else :\n",
        "        y_train_temp[i] = [1,0,0,0]\n",
        "\n",
        "    return y_train_temp\n",
        "    \n",
        "  #load dataset and calculate yh2c and ya2c\n",
        "  def load_data(self):\n",
        "    self.psf_a2c = PSFeatureExtractor(dataset_path = full_path, stopWord_path = stopWord_path, polarity_dataset_path = polarity_dataset_path\n",
        "                                 , stanford_models_path =stanford_models_path , use_google_drive = True, important_words = ['؟','تکذیب','تکذیب شد',':',],claim_name=\"claim\"\n",
        "                                 ,headline_name=\"body\",label_name=\"body_stance\",question_name=\"question\" ,part_name=\"part\",uniq_claims={})\n",
        "    self.psf_h2c = PSFeatureExtractor(dataset_path = full_path, stopWord_path = stopWord_path, polarity_dataset_path = polarity_dataset_path\n",
        "                                    , stanford_models_path =stanford_models_path , use_google_drive = True, important_words = ['؟','تکذیب','تکذیب شد',':',],claim_name=\"claim\"\n",
        "                                    ,headline_name=\"headline\",label_name=\"headline_stance\",question_name=\"question\" ,part_name=\"part\",uniq_claims={})\n",
        "    labels_a2c = np.reshape(self.psf_a2c.labels,(len(self.psf_a2c.labels),1))\n",
        "    labels_h2c = np.reshape(self.psf_h2c.labels,(len(self.psf_h2c.labels),1))\n",
        "    self.y_a2c = self.convert_data_to_one_hot(labels_a2c)\n",
        "    self.y_h2c = self.convert_data_to_one_hot(labels_h2c)\n",
        "\n",
        "  #calculate concatinated x_a2c and x_h2c \n",
        "  def generate_features(self):\n",
        "    print(\"generate_features_______________\")\n",
        "    print(self.psf_a2c.claims.shape)\n",
        "    claims_array = self.psf_a2c.claims.reshape((self.psf_a2c.claims.shape[0], 1))\n",
        "    bodies_array = self.psf_a2c.headlines.reshape((self.psf_a2c.headlines.shape[0], 1))\n",
        "    headlines_array = self.psf_h2c.headlines.reshape((self.psf_a2c.headlines.shape[0], 1))\n",
        "    \n",
        "    self.psf_a2c.clean_sentences()\n",
        "    self.psf_h2c.clean_sentences()\n",
        "    features_a2c, features_name_a2c= self.psf_a2c.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path, save_feature= False \n",
        "                                                               , load_path= save_load_path , root_distance = False , load_if_exist = False, tfidf = True, bow = False, w2v = False, polarity= False)\n",
        "    features_h2c, features_name_h2c= self.psf_h2c.generate_Features(w2v_model_path = w2v_model_path,save_path = save_load_path , save_feature= False\n",
        "                                                              , load_path= save_load_path , root_distance = False, load_if_exist = False, tfidf = False, bow = False, w2v = False , polarity= False)\n",
        "    \n",
        "    self.x_a2c = np.concatenate((claims_array, bodies_array, features_a2c), axis=1)\n",
        "    self.x_h2c = np.concatenate((claims_array, headlines_array , features_h2c), axis=1)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCEeJbhoXSXP"
      },
      "source": [
        "##Web"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWNc6mqNO0C3"
      },
      "source": [
        "class Web():\n",
        "  def __init__(self):\n",
        "      self.total_num=1\n",
        "      self.correct_head=0\n",
        "      self.correct_body=0\n",
        "      self.false_head=0\n",
        "      self.false_body=0\n",
        "      self.cred_head=0\n",
        "      self.cred_body=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCWdm8IRXUlW"
      },
      "source": [
        "##Credbility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEmfs3SRLI3i"
      },
      "source": [
        "class Credipility():\n",
        "  #data: stance of Load_data\n",
        "  #df : dataframe include web info\n",
        "  def __init__(self,data,model_h2c,model_a2c,df):\n",
        "    self.data = data\n",
        "    self.df = df\n",
        "    self.model_a2c = model_a2c\n",
        "    self.model_h2c = model_h2c\n",
        "    self.cal_cred_dic()\n",
        "\n",
        "  #calculate credibility score for one instance (body-article) return True and False score\n",
        "  def cal_body(self,i,stance_b,domain):\n",
        "    if stance_b == 'Agree':\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].correct_body +=1\n",
        "        # self.web_dic[domain].cred_body +=1\n",
        "        return 0,1\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        # self.web_dic[domain].cred_body -=1\n",
        "        self.web_dic[domain].false_body +=1\n",
        "        return 1,0\n",
        "    elif stance_b == 'Disagree':\n",
        "      #مدیری رفت بیمارستان (درست)\n",
        "      #خبر :‌\n",
        "      #مدیری نرفت بیمارستان\n",
        "      #disagree, veracity= false \n",
        "      if self.df['veracity'][i] == 'True': \n",
        "        # self.web_dic[domain].cred_body -=1\n",
        "        self.web_dic[domain].false_body +=1\n",
        "        return 1,0\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].correct_body +=1 \n",
        "        # self.web_dic[domain].cred_body +=1\n",
        "        return 0,1\n",
        "    elif stance_b == 'Discuss':\n",
        "      a = self.create_inputs_targets(np.array([self.data.x_a2c[i]]),MAX_LEN=MAX_LEN_a2c)\n",
        "      stance_a2c = self.model_a2c.predict([a[0],a[1], a[2]])\n",
        "      #agree3, disagree2: (agree- disagree)/(sum)\n",
        "      # truth_a2c= stance_a2c[0][3] -stance_a2c[0][2] /stance_a2c[0][3]+stance_a2c[0][2]\n",
        "      agree_a2c = stance_a2c[0][3]/stance_a2c[0][3]+stance_a2c[0][2]\n",
        "      disagree_a2c = stance_a2c[0][2] /stance_a2c[0][3]+stance_a2c[0][2]\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        # self.web_dic[domain].cred_body += truth_a2c\n",
        "        self.web_dic[domain].correct_body += agree_a2c\n",
        "        self.web_dic[domain].false_body += disagree_a2c\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        # self.web_dic[domain].cred_body -= truth_a2c\n",
        "        self.web_dic[domain].correct_body += disagree_a2c\n",
        "        self.web_dic[domain].false_body += agree_a2c\n",
        "\n",
        "    return 0,0\n",
        "\n",
        "  #calculate credibility score for one instance (header-article) return True and False score\n",
        "  def cal_header(self,i,stance_h,domain):\n",
        "    if stance_h == 'Agree':\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        self.web_dic[domain].correct_head +=1\n",
        "        # self.web_dic[domain].cred_head +=1\n",
        "        return 0,1\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        # self.web_dic[domain].cred_head -=1\n",
        "        self.web_dic[domain].false_head +=1\n",
        "        return 1,0\n",
        "    elif stance_h == 'Disagree':\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        # self.web_dic[domain].cred_head -=1\n",
        "        self.web_dic[domain].false_head +=1\n",
        "        return 1,0\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        self.web_dic[domain].correct_head +=1 \n",
        "        # self.web_dic[domain].cred_head +=1\n",
        "        return 0,1\n",
        "    elif stance_h == 'Discuss':\n",
        "      a = self.create_inputs_targets(np.array([self.data.x_h2c[i]]),MAX_LEN=MAX_LEN_h2c)\n",
        "      stance_h2c = self.model_h2c.predict([a[0],a[1], a[2]])\n",
        "      # truth_h2c= stance_h2c[0][3] - stance_h2c[0][2] /stance_h2c[0][3]+stance_h2c[0][2]\n",
        "      agree_h2c = stance_h2c[0][3]/stance_h2c[0][3]+stance_h2c[0][2]\n",
        "      disagree_h2c = stance_h2c[0][2] /stance_h2c[0][3]+stance_h2c[0][2]\n",
        "      if self.df['veracity'][i] == 'True':\n",
        "        # self.web_dic[domain].cred_head += truth_h2c\n",
        "        self.web_dic[domain].correct_head += agree_h2c\n",
        "        self.web_dic[domain].false_head += disagree_h2c\n",
        "      elif  self.df['veracity'][i] == 'False':\n",
        "        # self.web_dic[domain].cred_head -= truth_h2c\n",
        "        self.web_dic[domain].correct_head += disagree_h2c\n",
        "        self.web_dic[domain].false_head += agree_h2c\n",
        "    return 0,0\n",
        "\n",
        "  def cal_credibility(self):\n",
        "    self.web_dic = {}\n",
        "    self.total_num_truth = 0\n",
        "    self.total_num_falseness = 0\n",
        "    for i in range(len(df)):\n",
        "      stance_h = self.df['headline_stance'][i]\n",
        "      stance_b = self.df['body_stance'][i]\n",
        "      domain = self.df['web_domain'][i]\n",
        "      if domain not in self.web_dic:\n",
        "        self.web_dic[domain]=Web()\n",
        "      else: \n",
        "        self.web_dic[domain].total_num +=1\n",
        "      fh,th = self.cal_header(i,stance_h,domain)\n",
        "      fb,tb = self.cal_body(i,stance_b,domain)\n",
        "      # self.web_dic[domain].false_head += fh\n",
        "      # self.web_dic[domain].correct_head += th\n",
        "      # self.web_dic[domain].false_body += fb\n",
        "      # self.web_dic[domain].correct_body += tb\n",
        "      self.total_num_truth += tb\n",
        "      self.total_num_falseness +=fb  \n",
        "    for web in self.web_dic.keys():\n",
        "      self.web_dic[web].cred_head = (self.web_dic[web].correct_body -  self.web_dic[web].false_body)/self.web_dic[web].total_num\n",
        "      self.web_dic[web].cred_body = (self.web_dic[web].correct_head -  self.web_dic[web].false_head)/self.web_dic[web].total_num\n",
        "      # print(web,'..',self.web_dic[web].total_num,'head',self.web_dic[web].cred_head,'body',self.web_dic[web].cred_body)\n",
        "\n",
        "  def create_inputs_targets(self,x_raw, padding = True,MAX_LEN = MAX_LEN_a2c):\n",
        "    dataset_dict = {\n",
        "          \"input_ids\": [],\n",
        "          \"token_type_ids\": [],\n",
        "          \"attention_mask\": []\n",
        "          }\n",
        "\n",
        "    LENGTH = len(x_raw)\n",
        "    for i in range(LENGTH):\n",
        "        input = x_raw[i] # an array has two strings inside: headline, calim\n",
        "\n",
        "        body = input[0]\n",
        "        claim = input[1]\n",
        "\n",
        "        sequence = parsbert_tokenizer(body, claim, padding = 'max_length', max_length = MAX_LEN, truncation=True)\n",
        "        input_ids = sequence['input_ids']  # encoded tokens\n",
        "        token_type_ids = sequence['token_type_ids']  # segment number for every token. 0 for headline. 1 for claim.\n",
        "        attention_mask = sequence['attention_mask']\n",
        "\n",
        "        dataset_dict[\"input_ids\"].append(input_ids)\n",
        "        dataset_dict[\"token_type_ids\"].append(token_type_ids)\n",
        "        dataset_dict[\"attention_mask\"].append(attention_mask)\n",
        "\n",
        "    for key in dataset_dict:\n",
        "        dataset_dict[key] = tf.convert_to_tensor(dataset_dict[key])\n",
        "\n",
        "    x = [\n",
        "          dataset_dict[\"input_ids\"],\n",
        "          dataset_dict[\"token_type_ids\"],\n",
        "          dataset_dict[\"attention_mask\"],\n",
        "        ]\n",
        "    x = tf.convert_to_tensor(x)\n",
        "    return x\n",
        "    \n",
        "  def cal_cred_dic(self): \n",
        "    # self.x_a2c = self.create_inputs_targets(self.data.x_a2c,MAX_LEN=MAX_LEN_a2c)\n",
        "    # self.x_h2c = self.create_inputs_targets(self.data.x_h2c,MAX_LEN=MAX_LEN_h2c)\n",
        "    # self.tf_MLP_input = tf.convert_to_tensor(self.data.x_h2c[:, 2:], dtype=tf.float32)?!\n",
        "    #Change seld.data.x_a2c --> self.x_a2c\n",
        "    self.cal_credibility()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlJXHKzRXYkF"
      },
      "source": [
        "##Extract_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyn_vFsWnwYm"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY9StCcGIrUL"
      },
      "source": [
        "class Extract_features():\n",
        "  def __init__(self,df,data):\n",
        "    self.data = data\n",
        "    self.df = df\n",
        "    self.df_len = len(self.df)\n",
        "    self.set_webinfo()\n",
        "    self.set_importantwords()\n",
        "    self.cred = None\n",
        "    self.stance_a2c = []\n",
        "    self.stance_h2c = []\n",
        "    \n",
        "    \n",
        "  def set_webinfo(self):\n",
        "    self.df['web_host'] = np.empty(self.df_len,dtype=np.str)\n",
        "    self.df['web_domain'] = np.empty(self.df_len,dtype=np.str)\n",
        "    self.df['transfer_protocol'] = np.ones(self.df_len)*0.5\n",
        "    for i in range(len(self.df)):\n",
        "      try:\n",
        "        domain = urlparse(self.df['url'][i]).netloc\n",
        "        domain = domain.split('.')\n",
        "        self.df['web_host'][i] = domain[-1]\n",
        "        self.df['web_domain'][i] = domain[-2]\n",
        "        if self.df['url'][i][4] == 's':\n",
        "          self.df['transfer_protocol'][i] = 1\n",
        "        elif not df['url'][i][4] == ':':\n",
        "          self.df['transfer_protocol'][i] = 0    \n",
        "      except: \n",
        "        self.df['web_host'][i] = '-'\n",
        "        self.df['web_domain'][i] = '-'\n",
        "        self.df['transfer_protocol'][i] = 0 \n",
        "  \n",
        "  def set_importantwords(self):\n",
        "    self.data.psf_a2c.important_words = refute_hedge_reporte_words\n",
        "    self.data.psf_h2c.important_words = refute_hedge_reporte_words\n",
        "    self.featrues_important_words_a2c = self.data.psf_a2c.calc_important_words()\n",
        "    self.featrues_important_words_h2c = self.data.psf_h2c.calc_important_words()\n",
        "    # self.df =pd.concat([self.df, pd.DataFrame(featrues_important_words_h2c+featrues_important_words_a2c)], axis=1)\n",
        "\n",
        "  def cal_domain_onehot(self):\n",
        "    ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "    arr = ohe.fit_transform(self.df['web_domain'].to_numpy()[:, np.newaxis])\n",
        "    self.web_domain = arr\n",
        "\n",
        "  def cal_host_onehot(self):\n",
        "    ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "    arr = ohe.fit_transform(self.df['web_host'].to_numpy()[:, np.newaxis])\n",
        "    self.web_host = arr\n",
        "\n",
        "\n",
        "  def cal_stance_features(self):\n",
        "    for i in range(len(df)):\n",
        "      a2c_i = self.cred.create_inputs_targets(np.array([self.data.x_a2c[i]]),MAX_LEN=MAX_LEN_a2c)\n",
        "      h2c_i = self.cred.create_inputs_targets(np.array([self.data.x_h2c[i]]),MAX_LEN=MAX_LEN_h2c)\n",
        "      self.stance_a2c.append(self.cred.model_a2c.predict([a2c_i[0],a2c_i[1], a2c_i[2]]))\n",
        "      self.stance_h2c.append(self.cred.model_h2c.predict([h2c_i[0],h2c_i[1], h2c_i[2]]))\n",
        "  \n",
        "  def calculate_claims_features(self):\n",
        "    self.claims = {}\n",
        "    self.len_features = 0\n",
        "    for i in range(len(self.df)):\n",
        "      claim = self.df['claim'][i]\n",
        "      # print(claim)\n",
        "      if claim in self.claims:\n",
        "        self.claims[claim] = self.append_features(i,self.claims[claim])\n",
        "      else:\n",
        "        self.claims[claim] = self.append_features(i,(0,[],''))\n",
        "        self.len_features = len(self.claims[claim][1])\n",
        "\n",
        "    for claim in self.claims.keys():\n",
        "      num, features,label = self.claims[claim] \n",
        "      if num < 4:\n",
        "        features = features + [0]*self.len_features*(4-num)\n",
        "        self.claims[claim] = (features,label)\n",
        "      else: \n",
        "        self.claims[claim] = (features,label)\n",
        "\n",
        "        \n",
        "  def append_features(self,i,prev_value):\n",
        "    # print(prev_value)\n",
        "    num , prev_features,label = prev_value\n",
        "    if num >= 4 : \n",
        "      return prev_value\n",
        "\n",
        "    s_a2c = self.stance_a2c[i][0]\n",
        "    s_h2c = self.stance_h2c[i][0]\n",
        "    host = self.web_host[i]\n",
        "    # domain = self.web_domain[i]\n",
        "    transfer_protocol = [self.df['transfer_protocol'][i]]\n",
        "    cred_head = [self.cred.web_dic[self.df['web_domain'][i]].cred_head]\n",
        "    cred_body = [self.cred.web_dic[self.df['web_domain'][i]].cred_body]\n",
        "    features = list(s_a2c)  + list(s_h2c) + list(host) + transfer_protocol + cred_head + cred_body\n",
        "    return (num+1 , prev_features + features, self.df['veracity'][i])\n",
        "\n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYXHMdByomzq",
        "outputId": "d2be3c87-3a70-4061-f9ea-31ff0aee817e"
      },
      "source": [
        "a = np.array([1,2])\n",
        "b = np.array([1,2])\n",
        "list(a) + list(b)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 2, 1, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj8CRxAeXbVK"
      },
      "source": [
        "##Fakenews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdz1vc44q9cI"
      },
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5gGEkJAAVFg"
      },
      "source": [
        "class Fakenews():\n",
        "  def __init__(self,model,df):\n",
        "    self.model = model\n",
        "    self.df = df\n",
        "    self.data_loader = Load_data()\n",
        "    self.feature_extractor = Extract_features(df,self.data_loader)\n",
        "    self.feature_extractor.cred = Credipility(self.data_loader,model_h2c,model_a2c,self.df)\n",
        "    self.feature_extractor.cal_domain_onehot()\n",
        "    self.feature_extractor.cal_host_onehot()\n",
        "    self.feature_extractor.cal_stance_features()\n",
        "    self.feature_extractor.calculate_claims_features()\n",
        "    self.label_onehot()\n",
        "\n",
        "  def label_onehot(self):\n",
        "    labels = []\n",
        "    self.X = []\n",
        "    for claim in self.feature_extractor.claims.keys():\n",
        "      labels.append(self.feature_extractor.claims[claim][1])\n",
        "      self.X.append(self.feature_extractor.claims[claim][0])\n",
        "    ohe = OneHotEncoder(categories='auto', sparse=False)\n",
        "    arr = ohe.fit_transform(np.array(labels)[:, np.newaxis])\n",
        "    self.Y = arr\n",
        "\n",
        "  def train(self):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(1024, input_shape=(self.feature_extractor.len_features*4,), activation='relu'))\n",
        "    model.add(Dense(512, activation='relu'))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.fit(np.array(self.X), np.array(self.Y), epochs=50, batch_size=250, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3cRktDrgxKa",
        "outputId": "fab9440b-acd3-401b-c4e8-e93f7e52a630"
      },
      "source": [
        "df = pd.read_csv(full_path)\n",
        "fn = Fakenews(None,df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/persian_stance_baseline_data/dataset/stopWords.txt\n",
            "before read\n",
            "before\n",
            "after\n",
            "[0 0 0 ... 0 0 0]\n",
            "(1407,) (1407,) (1407,) (1407,) (1407,)\n",
            "after read\n",
            "/content/drive/My Drive/persian_stance_baseline_data/dataset/stopWords.txt\n",
            "before read\n",
            "before\n",
            "after\n",
            "[0 0 0 ... 0 0 0]\n",
            "(1407,) (1407,) (1407,) (1407,) (1407,)\n",
            "after read\n",
            "generate_features_______________\n",
            "(1407,)\n",
            "Start to generate tf_idf feature\n",
            "End of tf_idf feature\n",
            "Start to generate similarity feature\n",
            "End of similarity feature\n",
            "Start to generate important words feature\n",
            "End of important words feature\n",
            "\"is question\" feature was added.\n",
            "\"more than tow parts\" feature was added.\n",
            "Start to generate similarity feature\n",
            "End of similarity feature\n",
            "Start to generate important words feature\n",
            "End of important words feature\n",
            "\"is question\" feature was added.\n",
            "\"more than tow parts\" feature was added.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbFrkjjrn47U",
        "outputId": "7e7a58f4-c6be-48d2-aff1-733157ac6454"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(30, input_shape=(fn.feature_extractor.len_features*4,), activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(10, activation='relu'))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(np.array(fn.X), np.array(fn.Y), epochs=100, batch_size=250, verbose=1, validation_split=0.2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 [==============================] - 1s 200ms/step - loss: 1.3374 - accuracy: 0.0931 - val_loss: 1.2297 - val_accuracy: 0.0959\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.2709 - accuracy: 0.1207 - val_loss: 1.1765 - val_accuracy: 0.1507\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.1897 - accuracy: 0.2207 - val_loss: 1.1323 - val_accuracy: 0.2466\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.1591 - accuracy: 0.2448 - val_loss: 1.0941 - val_accuracy: 0.3836\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 1.0946 - accuracy: 0.3414 - val_loss: 1.0574 - val_accuracy: 0.5068\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.0555 - accuracy: 0.4379 - val_loss: 1.0231 - val_accuracy: 0.5753\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.0288 - accuracy: 0.4966 - val_loss: 0.9903 - val_accuracy: 0.6438\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.9619 - accuracy: 0.6207 - val_loss: 0.9582 - val_accuracy: 0.6849\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.9325 - accuracy: 0.6759 - val_loss: 0.9253 - val_accuracy: 0.7808\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.8842 - accuracy: 0.7379 - val_loss: 0.8902 - val_accuracy: 0.8082\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.8650 - accuracy: 0.7793 - val_loss: 0.8525 - val_accuracy: 0.8630\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.8037 - accuracy: 0.8138 - val_loss: 0.8132 - val_accuracy: 0.8904\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.7672 - accuracy: 0.8448 - val_loss: 0.7737 - val_accuracy: 0.8904\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 42ms/step - loss: 0.7433 - accuracy: 0.8414 - val_loss: 0.7346 - val_accuracy: 0.8904\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.7077 - accuracy: 0.8724 - val_loss: 0.6973 - val_accuracy: 0.8904\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.6795 - accuracy: 0.8621 - val_loss: 0.6620 - val_accuracy: 0.8904\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.6630 - accuracy: 0.8793 - val_loss: 0.6292 - val_accuracy: 0.8904\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.6224 - accuracy: 0.8759 - val_loss: 0.5992 - val_accuracy: 0.8904\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.5959 - accuracy: 0.8759 - val_loss: 0.5718 - val_accuracy: 0.9041\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5564 - accuracy: 0.8793 - val_loss: 0.5476 - val_accuracy: 0.9041\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.5354 - accuracy: 0.8793 - val_loss: 0.5263 - val_accuracy: 0.9041\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.5377 - accuracy: 0.8862 - val_loss: 0.5074 - val_accuracy: 0.9041\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.5266 - accuracy: 0.8862 - val_loss: 0.4909 - val_accuracy: 0.9041\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.4974 - accuracy: 0.8862 - val_loss: 0.4769 - val_accuracy: 0.9041\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4824 - accuracy: 0.8862 - val_loss: 0.4651 - val_accuracy: 0.9041\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.4827 - accuracy: 0.8862 - val_loss: 0.4549 - val_accuracy: 0.9041\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.4674 - accuracy: 0.8828 - val_loss: 0.4460 - val_accuracy: 0.9041\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4640 - accuracy: 0.8862 - val_loss: 0.4386 - val_accuracy: 0.9041\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4610 - accuracy: 0.8862 - val_loss: 0.4323 - val_accuracy: 0.9041\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.4458 - accuracy: 0.8862 - val_loss: 0.4271 - val_accuracy: 0.9041\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4403 - accuracy: 0.8862 - val_loss: 0.4230 - val_accuracy: 0.9041\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4655 - accuracy: 0.8862 - val_loss: 0.4196 - val_accuracy: 0.9041\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4339 - accuracy: 0.8862 - val_loss: 0.4165 - val_accuracy: 0.9041\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4321 - accuracy: 0.8862 - val_loss: 0.4138 - val_accuracy: 0.9041\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4431 - accuracy: 0.8862 - val_loss: 0.4114 - val_accuracy: 0.9041\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4223 - accuracy: 0.8862 - val_loss: 0.4093 - val_accuracy: 0.9041\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.4277 - accuracy: 0.8862 - val_loss: 0.4073 - val_accuracy: 0.9041\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4233 - accuracy: 0.8862 - val_loss: 0.4055 - val_accuracy: 0.9041\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4220 - accuracy: 0.8862 - val_loss: 0.4038 - val_accuracy: 0.9041\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.4137 - accuracy: 0.8862 - val_loss: 0.4023 - val_accuracy: 0.9041\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4273 - accuracy: 0.8862 - val_loss: 0.4010 - val_accuracy: 0.9041\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.4177 - accuracy: 0.8862 - val_loss: 0.3997 - val_accuracy: 0.9041\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4116 - accuracy: 0.8862 - val_loss: 0.3983 - val_accuracy: 0.9041\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4109 - accuracy: 0.8862 - val_loss: 0.3969 - val_accuracy: 0.9041\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.4037 - accuracy: 0.8793 - val_loss: 0.3957 - val_accuracy: 0.9041\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3950 - accuracy: 0.8862 - val_loss: 0.3946 - val_accuracy: 0.9041\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4237 - accuracy: 0.8862 - val_loss: 0.3936 - val_accuracy: 0.9041\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.3964 - accuracy: 0.8862 - val_loss: 0.3926 - val_accuracy: 0.9041\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.4099 - accuracy: 0.8862 - val_loss: 0.3916 - val_accuracy: 0.9041\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4132 - accuracy: 0.8862 - val_loss: 0.3907 - val_accuracy: 0.9041\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.3975 - accuracy: 0.8862 - val_loss: 0.3897 - val_accuracy: 0.9041\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4182 - accuracy: 0.8862 - val_loss: 0.3886 - val_accuracy: 0.9041\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.3975 - accuracy: 0.8862 - val_loss: 0.3874 - val_accuracy: 0.9041\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3984 - accuracy: 0.8897 - val_loss: 0.3864 - val_accuracy: 0.9041\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3887 - accuracy: 0.8862 - val_loss: 0.3855 - val_accuracy: 0.9041\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3829 - accuracy: 0.8897 - val_loss: 0.3848 - val_accuracy: 0.9041\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.3893 - accuracy: 0.8862 - val_loss: 0.3841 - val_accuracy: 0.9041\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3896 - accuracy: 0.8862 - val_loss: 0.3831 - val_accuracy: 0.9041\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3832 - accuracy: 0.8862 - val_loss: 0.3819 - val_accuracy: 0.9041\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4099 - accuracy: 0.8862 - val_loss: 0.3808 - val_accuracy: 0.9041\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3892 - accuracy: 0.8828 - val_loss: 0.3795 - val_accuracy: 0.9041\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3633 - accuracy: 0.8862 - val_loss: 0.3784 - val_accuracy: 0.9041\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3701 - accuracy: 0.8897 - val_loss: 0.3774 - val_accuracy: 0.9041\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3703 - accuracy: 0.8897 - val_loss: 0.3764 - val_accuracy: 0.9041\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.3685 - accuracy: 0.8828 - val_loss: 0.3754 - val_accuracy: 0.9041\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.3881 - accuracy: 0.8862 - val_loss: 0.3744 - val_accuracy: 0.9041\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3859 - accuracy: 0.8897 - val_loss: 0.3735 - val_accuracy: 0.9041\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.3726 - accuracy: 0.8828 - val_loss: 0.3728 - val_accuracy: 0.9041\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3650 - accuracy: 0.8897 - val_loss: 0.3722 - val_accuracy: 0.9041\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.3713 - accuracy: 0.8793 - val_loss: 0.3714 - val_accuracy: 0.9041\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3861 - accuracy: 0.8828 - val_loss: 0.3705 - val_accuracy: 0.9041\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3685 - accuracy: 0.8897 - val_loss: 0.3696 - val_accuracy: 0.9041\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.3700 - accuracy: 0.8931 - val_loss: 0.3686 - val_accuracy: 0.9041\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3573 - accuracy: 0.8862 - val_loss: 0.3676 - val_accuracy: 0.9041\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3618 - accuracy: 0.8828 - val_loss: 0.3667 - val_accuracy: 0.9041\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.3514 - accuracy: 0.8931 - val_loss: 0.3658 - val_accuracy: 0.9041\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3284 - accuracy: 0.8897 - val_loss: 0.3650 - val_accuracy: 0.9041\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.3445 - accuracy: 0.8931 - val_loss: 0.3644 - val_accuracy: 0.9041\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3790 - accuracy: 0.8862 - val_loss: 0.3641 - val_accuracy: 0.9041\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3586 - accuracy: 0.8897 - val_loss: 0.3639 - val_accuracy: 0.9041\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.3518 - accuracy: 0.8931 - val_loss: 0.3639 - val_accuracy: 0.9041\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.3570 - accuracy: 0.8966 - val_loss: 0.3638 - val_accuracy: 0.9041\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.3555 - accuracy: 0.8897 - val_loss: 0.3635 - val_accuracy: 0.9041\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3437 - accuracy: 0.9034 - val_loss: 0.3629 - val_accuracy: 0.9041\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3493 - accuracy: 0.8897 - val_loss: 0.3620 - val_accuracy: 0.9041\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3459 - accuracy: 0.8897 - val_loss: 0.3612 - val_accuracy: 0.9041\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3415 - accuracy: 0.8931 - val_loss: 0.3605 - val_accuracy: 0.9041\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3543 - accuracy: 0.8931 - val_loss: 0.3597 - val_accuracy: 0.9041\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.3342 - accuracy: 0.9000 - val_loss: 0.3588 - val_accuracy: 0.9041\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.3282 - accuracy: 0.9034 - val_loss: 0.3580 - val_accuracy: 0.9041\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.3197 - accuracy: 0.8931 - val_loss: 0.3575 - val_accuracy: 0.9041\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3613 - accuracy: 0.8966 - val_loss: 0.3570 - val_accuracy: 0.9041\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3432 - accuracy: 0.8966 - val_loss: 0.3564 - val_accuracy: 0.9041\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3142 - accuracy: 0.9034 - val_loss: 0.3558 - val_accuracy: 0.9041\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.3247 - accuracy: 0.9000 - val_loss: 0.3553 - val_accuracy: 0.9041\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3382 - accuracy: 0.9000 - val_loss: 0.3549 - val_accuracy: 0.9041\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.3269 - accuracy: 0.9069 - val_loss: 0.3545 - val_accuracy: 0.9041\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.3309 - accuracy: 0.8931 - val_loss: 0.3540 - val_accuracy: 0.9041\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.3282 - accuracy: 0.8897 - val_loss: 0.3535 - val_accuracy: 0.9041\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.3157 - accuracy: 0.9069 - val_loss: 0.3531 - val_accuracy: 0.9041\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2U8I9kGVw3Nw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e113131b-5845-4fac-d9de-95c4799c78e3"
      },
      "source": [
        "plot.plot(history.history['accuracy'], label='train')\n",
        "plot.plot(history.history['val_accuracy'], label='test')\n",
        "plot.legend()\n",
        "plot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpE5AHheehdQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "3a72b1fe-21e6-49a0-8b12-60ae4bf822a7"
      },
      "source": [
        "plot.plot(history.history['loss'], label='train')\n",
        "plot.plot(history.history['val_loss'], label='test')\n",
        "plot.legend()\n",
        "plot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm00L_0Iw4gt"
      },
      "source": [
        "#OverSampler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0vRV5eww4Dc"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.over_sampling import SVMSMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.over_sampling import (SMOTE, BorderlineSMOTE, SVMSMOTE, SMOTENC)\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDi39sVFDLUs"
      },
      "source": [
        "def train_model(x,y,epochs):\n",
        "  model = Sequential()\n",
        "  model.add(Dense(30, input_shape=(fn.feature_extractor.len_features*4,), activation='relu'))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(Dense(10, activation='relu'))\n",
        "  model.add(Dense(3, activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy','mse'])\n",
        "  history = model.fit(np.array(x), np.array(y), epochs=epochs, batch_size=250, verbose=1, validation_split=0.2)\n",
        "  return model,history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOJHz06cGmsD"
      },
      "source": [
        "def over_sample(X, Y, oversampling = 'SVMSMOTE'):\n",
        "  if oversampling == 'BorderlineSMOTE':\n",
        "    sampling_strategy = \"auto\"\n",
        "    ada = BorderlineSMOTE(sampling_strategy=sampling_strategy, random_state=0)\n",
        "    Xo, Yo = ada.fit_resample(X, Y)\n",
        "\n",
        "  elif oversampling == 'SVMSMOTE':\n",
        "    sm = SVMSMOTE(random_state=0)\n",
        "    Xo, Yo =sm.fit_resample(X, Y)\n",
        "\n",
        "  elif oversampling == 'RandomOverSampler':\n",
        "    rndsampler = RandomOverSampler(random_state=0)\n",
        "    Xo, Yo =rndsampler.fit_resample(X, Y)\n",
        "  elif oversampling == 'SMOTE':\n",
        "    sm = SMOTE(random_state=0)\n",
        "    Xo, Yo =sm.fit_resample(X, Y)\n",
        "  elif oversampling == 'ADASYN':\n",
        "    adsn = ADASYN(sampling_strategy='minority')\n",
        "    Xo, Yo = adsn.fit_resample(X, Y) \n",
        "  return (Xo, Yo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9fbr9tNBno9",
        "outputId": "6b64642d-f2fa-4681-a448-6855a43e6cae"
      },
      "source": [
        "Xo, Yo = over_sample(fn.X, fn.Y,'BorderlineSMOTE')\n",
        "borderline_model = train_model(Xo, Yo,200)\n",
        "output = borderline_model.predict_classes(np.array(fn.X))\n",
        "f1_score(np.argmax(fn.Y,1), output,labels=[0,1,2,3],average='weighted')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4/4 [==============================] - 1s 56ms/step - loss: 1.3841 - accuracy: 0.3187 - mse: 0.2676 - val_loss: 0.5847 - val_accuracy: 0.9227 - val_mse: 0.1125\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.2808 - accuracy: 0.3239 - mse: 0.2543 - val_loss: 0.6397 - val_accuracy: 0.8969 - val_mse: 0.1236\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.1741 - accuracy: 0.3523 - mse: 0.2367 - val_loss: 0.6898 - val_accuracy: 0.8660 - val_mse: 0.1329\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0967 - accuracy: 0.3613 - mse: 0.2220 - val_loss: 0.7529 - val_accuracy: 0.8454 - val_mse: 0.1462\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 1.0520 - accuracy: 0.4052 - mse: 0.2129 - val_loss: 0.8177 - val_accuracy: 0.7938 - val_mse: 0.1601\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.0232 - accuracy: 0.4090 - mse: 0.2066 - val_loss: 0.8877 - val_accuracy: 0.7062 - val_mse: 0.1758\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9769 - accuracy: 0.4645 - mse: 0.1962 - val_loss: 0.9362 - val_accuracy: 0.5670 - val_mse: 0.1866\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9675 - accuracy: 0.4968 - mse: 0.1941 - val_loss: 0.9491 - val_accuracy: 0.5567 - val_mse: 0.1891\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.9470 - accuracy: 0.5148 - mse: 0.1900 - val_loss: 0.9386 - val_accuracy: 0.5773 - val_mse: 0.1861\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9243 - accuracy: 0.5355 - mse: 0.1845 - val_loss: 0.9248 - val_accuracy: 0.5722 - val_mse: 0.1827\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9171 - accuracy: 0.5419 - mse: 0.1832 - val_loss: 0.8951 - val_accuracy: 0.5876 - val_mse: 0.1759\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9009 - accuracy: 0.5845 - mse: 0.1795 - val_loss: 0.8747 - val_accuracy: 0.5825 - val_mse: 0.1716\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8779 - accuracy: 0.5884 - mse: 0.1745 - val_loss: 0.8745 - val_accuracy: 0.5412 - val_mse: 0.1726\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.8626 - accuracy: 0.5935 - mse: 0.1720 - val_loss: 0.8725 - val_accuracy: 0.5309 - val_mse: 0.1730\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8478 - accuracy: 0.5910 - mse: 0.1693 - val_loss: 0.8625 - val_accuracy: 0.5412 - val_mse: 0.1710\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.8182 - accuracy: 0.6310 - mse: 0.1628 - val_loss: 0.8484 - val_accuracy: 0.5515 - val_mse: 0.1678\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8116 - accuracy: 0.6452 - mse: 0.1609 - val_loss: 0.8236 - val_accuracy: 0.5722 - val_mse: 0.1619\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7961 - accuracy: 0.6671 - mse: 0.1571 - val_loss: 0.7969 - val_accuracy: 0.6134 - val_mse: 0.1555\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7812 - accuracy: 0.6955 - mse: 0.1532 - val_loss: 0.7792 - val_accuracy: 0.6392 - val_mse: 0.1518\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7641 - accuracy: 0.6903 - mse: 0.1505 - val_loss: 0.7623 - val_accuracy: 0.6443 - val_mse: 0.1483\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7512 - accuracy: 0.7174 - mse: 0.1474 - val_loss: 0.7460 - val_accuracy: 0.6753 - val_mse: 0.1447\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7358 - accuracy: 0.7213 - mse: 0.1439 - val_loss: 0.7345 - val_accuracy: 0.7010 - val_mse: 0.1419\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.7115 - accuracy: 0.7419 - mse: 0.1371 - val_loss: 0.7321 - val_accuracy: 0.7010 - val_mse: 0.1413\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6804 - accuracy: 0.7948 - mse: 0.1311 - val_loss: 0.7241 - val_accuracy: 0.7113 - val_mse: 0.1395\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6907 - accuracy: 0.7703 - mse: 0.1342 - val_loss: 0.7143 - val_accuracy: 0.7165 - val_mse: 0.1372\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6635 - accuracy: 0.8245 - mse: 0.1262 - val_loss: 0.6999 - val_accuracy: 0.7268 - val_mse: 0.1340\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6258 - accuracy: 0.8142 - mse: 0.1186 - val_loss: 0.6857 - val_accuracy: 0.7320 - val_mse: 0.1312\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6220 - accuracy: 0.8065 - mse: 0.1181 - val_loss: 0.6639 - val_accuracy: 0.7423 - val_mse: 0.1265\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6134 - accuracy: 0.8065 - mse: 0.1163 - val_loss: 0.6544 - val_accuracy: 0.7371 - val_mse: 0.1252\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5969 - accuracy: 0.8065 - mse: 0.1129 - val_loss: 0.6369 - val_accuracy: 0.7474 - val_mse: 0.1214\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5666 - accuracy: 0.8219 - mse: 0.1058 - val_loss: 0.6270 - val_accuracy: 0.7423 - val_mse: 0.1200\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5823 - accuracy: 0.8065 - mse: 0.1100 - val_loss: 0.5896 - val_accuracy: 0.7629 - val_mse: 0.1118\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5498 - accuracy: 0.8284 - mse: 0.1025 - val_loss: 0.5331 - val_accuracy: 0.8041 - val_mse: 0.0979\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5578 - accuracy: 0.8348 - mse: 0.1041 - val_loss: 0.5122 - val_accuracy: 0.8196 - val_mse: 0.0929\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5313 - accuracy: 0.8606 - mse: 0.0979 - val_loss: 0.5096 - val_accuracy: 0.8144 - val_mse: 0.0929\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5255 - accuracy: 0.8323 - mse: 0.0976 - val_loss: 0.5273 - val_accuracy: 0.7835 - val_mse: 0.0982\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5155 - accuracy: 0.8465 - mse: 0.0942 - val_loss: 0.5413 - val_accuracy: 0.7732 - val_mse: 0.1021\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.5206 - accuracy: 0.8245 - mse: 0.0976 - val_loss: 0.5139 - val_accuracy: 0.7835 - val_mse: 0.0954\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4840 - accuracy: 0.8568 - mse: 0.0885 - val_loss: 0.4763 - val_accuracy: 0.8247 - val_mse: 0.0861\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4850 - accuracy: 0.8452 - mse: 0.0886 - val_loss: 0.4437 - val_accuracy: 0.8660 - val_mse: 0.0780\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4662 - accuracy: 0.8606 - mse: 0.0850 - val_loss: 0.4240 - val_accuracy: 0.8814 - val_mse: 0.0735\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4763 - accuracy: 0.8606 - mse: 0.0860 - val_loss: 0.4249 - val_accuracy: 0.8711 - val_mse: 0.0744\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4624 - accuracy: 0.8606 - mse: 0.0841 - val_loss: 0.4235 - val_accuracy: 0.8608 - val_mse: 0.0747\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4393 - accuracy: 0.8697 - mse: 0.0801 - val_loss: 0.4082 - val_accuracy: 0.8711 - val_mse: 0.0715\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4331 - accuracy: 0.8671 - mse: 0.0779 - val_loss: 0.3960 - val_accuracy: 0.8814 - val_mse: 0.0686\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4318 - accuracy: 0.8684 - mse: 0.0777 - val_loss: 0.3831 - val_accuracy: 0.8866 - val_mse: 0.0654\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4361 - accuracy: 0.8555 - mse: 0.0788 - val_loss: 0.3845 - val_accuracy: 0.8969 - val_mse: 0.0657\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4336 - accuracy: 0.8761 - mse: 0.0777 - val_loss: 0.3949 - val_accuracy: 0.8918 - val_mse: 0.0683\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4041 - accuracy: 0.8684 - mse: 0.0713 - val_loss: 0.4124 - val_accuracy: 0.8763 - val_mse: 0.0731\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3991 - accuracy: 0.8787 - mse: 0.0707 - val_loss: 0.4057 - val_accuracy: 0.8711 - val_mse: 0.0721\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3942 - accuracy: 0.8800 - mse: 0.0706 - val_loss: 0.3754 - val_accuracy: 0.8969 - val_mse: 0.0654\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3889 - accuracy: 0.8916 - mse: 0.0690 - val_loss: 0.3642 - val_accuracy: 0.9021 - val_mse: 0.0629\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3704 - accuracy: 0.8942 - mse: 0.0644 - val_loss: 0.3645 - val_accuracy: 0.8969 - val_mse: 0.0624\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3801 - accuracy: 0.8839 - mse: 0.0669 - val_loss: 0.3722 - val_accuracy: 0.8969 - val_mse: 0.0638\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3780 - accuracy: 0.8774 - mse: 0.0670 - val_loss: 0.3798 - val_accuracy: 0.8969 - val_mse: 0.0657\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3537 - accuracy: 0.8916 - mse: 0.0624 - val_loss: 0.3768 - val_accuracy: 0.8918 - val_mse: 0.0654\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3652 - accuracy: 0.8800 - mse: 0.0649 - val_loss: 0.3572 - val_accuracy: 0.9021 - val_mse: 0.0609\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3439 - accuracy: 0.8929 - mse: 0.0606 - val_loss: 0.3371 - val_accuracy: 0.9124 - val_mse: 0.0566\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3523 - accuracy: 0.8916 - mse: 0.0622 - val_loss: 0.3138 - val_accuracy: 0.9278 - val_mse: 0.0518\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3422 - accuracy: 0.8955 - mse: 0.0616 - val_loss: 0.2980 - val_accuracy: 0.9381 - val_mse: 0.0489\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3364 - accuracy: 0.8955 - mse: 0.0575 - val_loss: 0.2981 - val_accuracy: 0.9381 - val_mse: 0.0494\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3316 - accuracy: 0.9123 - mse: 0.0556 - val_loss: 0.2847 - val_accuracy: 0.9381 - val_mse: 0.0469\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3217 - accuracy: 0.9006 - mse: 0.0551 - val_loss: 0.2739 - val_accuracy: 0.9381 - val_mse: 0.0449\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3107 - accuracy: 0.9123 - mse: 0.0534 - val_loss: 0.2739 - val_accuracy: 0.9381 - val_mse: 0.0450\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3307 - accuracy: 0.9084 - mse: 0.0589 - val_loss: 0.2766 - val_accuracy: 0.9381 - val_mse: 0.0458\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3050 - accuracy: 0.9084 - mse: 0.0527 - val_loss: 0.2774 - val_accuracy: 0.9381 - val_mse: 0.0464\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3103 - accuracy: 0.9032 - mse: 0.0536 - val_loss: 0.2788 - val_accuracy: 0.9330 - val_mse: 0.0472\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3242 - accuracy: 0.8929 - mse: 0.0581 - val_loss: 0.2775 - val_accuracy: 0.9330 - val_mse: 0.0472\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3173 - accuracy: 0.8877 - mse: 0.0576 - val_loss: 0.2760 - val_accuracy: 0.9330 - val_mse: 0.0470\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2988 - accuracy: 0.9071 - mse: 0.0520 - val_loss: 0.2738 - val_accuracy: 0.9330 - val_mse: 0.0464\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3149 - accuracy: 0.9019 - mse: 0.0558 - val_loss: 0.2677 - val_accuracy: 0.9381 - val_mse: 0.0453\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2874 - accuracy: 0.9200 - mse: 0.0487 - val_loss: 0.2675 - val_accuracy: 0.9381 - val_mse: 0.0454\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2940 - accuracy: 0.9123 - mse: 0.0507 - val_loss: 0.2704 - val_accuracy: 0.9381 - val_mse: 0.0461\n",
            "Epoch 74/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2698 - accuracy: 0.9213 - mse: 0.0469 - val_loss: 0.2653 - val_accuracy: 0.9381 - val_mse: 0.0450\n",
            "Epoch 75/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2738 - accuracy: 0.9174 - mse: 0.0477 - val_loss: 0.2566 - val_accuracy: 0.9381 - val_mse: 0.0429\n",
            "Epoch 76/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2687 - accuracy: 0.9161 - mse: 0.0475 - val_loss: 0.2490 - val_accuracy: 0.9381 - val_mse: 0.0410\n",
            "Epoch 77/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2748 - accuracy: 0.9226 - mse: 0.0475 - val_loss: 0.2254 - val_accuracy: 0.9381 - val_mse: 0.0357\n",
            "Epoch 78/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2730 - accuracy: 0.9161 - mse: 0.0475 - val_loss: 0.2056 - val_accuracy: 0.9485 - val_mse: 0.0317\n",
            "Epoch 79/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2759 - accuracy: 0.9187 - mse: 0.0479 - val_loss: 0.2029 - val_accuracy: 0.9485 - val_mse: 0.0313\n",
            "Epoch 80/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2665 - accuracy: 0.9252 - mse: 0.0451 - val_loss: 0.2120 - val_accuracy: 0.9433 - val_mse: 0.0334\n",
            "Epoch 81/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.2551 - accuracy: 0.9303 - mse: 0.0430 - val_loss: 0.2260 - val_accuracy: 0.9381 - val_mse: 0.0365\n",
            "Epoch 82/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2603 - accuracy: 0.9226 - mse: 0.0443 - val_loss: 0.2303 - val_accuracy: 0.9381 - val_mse: 0.0375\n",
            "Epoch 83/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2578 - accuracy: 0.9290 - mse: 0.0445 - val_loss: 0.2326 - val_accuracy: 0.9381 - val_mse: 0.0377\n",
            "Epoch 84/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2331 - accuracy: 0.9265 - mse: 0.0394 - val_loss: 0.2311 - val_accuracy: 0.9381 - val_mse: 0.0371\n",
            "Epoch 85/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2607 - accuracy: 0.9252 - mse: 0.0446 - val_loss: 0.2193 - val_accuracy: 0.9381 - val_mse: 0.0342\n",
            "Epoch 86/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2354 - accuracy: 0.9381 - mse: 0.0392 - val_loss: 0.2091 - val_accuracy: 0.9485 - val_mse: 0.0321\n",
            "Epoch 87/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2329 - accuracy: 0.9316 - mse: 0.0395 - val_loss: 0.1976 - val_accuracy: 0.9485 - val_mse: 0.0301\n",
            "Epoch 88/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2419 - accuracy: 0.9277 - mse: 0.0413 - val_loss: 0.1913 - val_accuracy: 0.9485 - val_mse: 0.0293\n",
            "Epoch 89/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2417 - accuracy: 0.9277 - mse: 0.0419 - val_loss: 0.1891 - val_accuracy: 0.9485 - val_mse: 0.0293\n",
            "Epoch 90/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2254 - accuracy: 0.9419 - mse: 0.0383 - val_loss: 0.1818 - val_accuracy: 0.9485 - val_mse: 0.0281\n",
            "Epoch 91/200\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2378 - accuracy: 0.9277 - mse: 0.0413 - val_loss: 0.1664 - val_accuracy: 0.9588 - val_mse: 0.0251\n",
            "Epoch 92/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2259 - accuracy: 0.9394 - mse: 0.0378 - val_loss: 0.1498 - val_accuracy: 0.9639 - val_mse: 0.0216\n",
            "Epoch 93/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2439 - accuracy: 0.9368 - mse: 0.0416 - val_loss: 0.1380 - val_accuracy: 0.9639 - val_mse: 0.0190\n",
            "Epoch 94/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2269 - accuracy: 0.9303 - mse: 0.0391 - val_loss: 0.1363 - val_accuracy: 0.9742 - val_mse: 0.0186\n",
            "Epoch 95/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2307 - accuracy: 0.9329 - mse: 0.0397 - val_loss: 0.1528 - val_accuracy: 0.9588 - val_mse: 0.0221\n",
            "Epoch 96/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2188 - accuracy: 0.9355 - mse: 0.0369 - val_loss: 0.1801 - val_accuracy: 0.9485 - val_mse: 0.0281\n",
            "Epoch 97/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2443 - accuracy: 0.9226 - mse: 0.0425 - val_loss: 0.1870 - val_accuracy: 0.9433 - val_mse: 0.0297\n",
            "Epoch 98/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1957 - accuracy: 0.9471 - mse: 0.0327 - val_loss: 0.1779 - val_accuracy: 0.9433 - val_mse: 0.0276\n",
            "Epoch 99/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2350 - accuracy: 0.9445 - mse: 0.0389 - val_loss: 0.1675 - val_accuracy: 0.9536 - val_mse: 0.0254\n",
            "Epoch 100/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2087 - accuracy: 0.9419 - mse: 0.0352 - val_loss: 0.1617 - val_accuracy: 0.9536 - val_mse: 0.0242\n",
            "Epoch 101/200\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2186 - accuracy: 0.9316 - mse: 0.0384 - val_loss: 0.1640 - val_accuracy: 0.9639 - val_mse: 0.0247\n",
            "Epoch 102/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2160 - accuracy: 0.9381 - mse: 0.0360 - val_loss: 0.1679 - val_accuracy: 0.9639 - val_mse: 0.0256\n",
            "Epoch 103/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2034 - accuracy: 0.9432 - mse: 0.0334 - val_loss: 0.1601 - val_accuracy: 0.9639 - val_mse: 0.0239\n",
            "Epoch 104/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1927 - accuracy: 0.9445 - mse: 0.0323 - val_loss: 0.1478 - val_accuracy: 0.9742 - val_mse: 0.0213\n",
            "Epoch 105/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2079 - accuracy: 0.9458 - mse: 0.0353 - val_loss: 0.1411 - val_accuracy: 0.9742 - val_mse: 0.0200\n",
            "Epoch 106/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1905 - accuracy: 0.9406 - mse: 0.0322 - val_loss: 0.1382 - val_accuracy: 0.9794 - val_mse: 0.0195\n",
            "Epoch 107/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2028 - accuracy: 0.9355 - mse: 0.0348 - val_loss: 0.1373 - val_accuracy: 0.9794 - val_mse: 0.0194\n",
            "Epoch 108/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1739 - accuracy: 0.9574 - mse: 0.0286 - val_loss: 0.1402 - val_accuracy: 0.9691 - val_mse: 0.0202\n",
            "Epoch 109/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1857 - accuracy: 0.9484 - mse: 0.0314 - val_loss: 0.1452 - val_accuracy: 0.9588 - val_mse: 0.0213\n",
            "Epoch 110/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2022 - accuracy: 0.9381 - mse: 0.0341 - val_loss: 0.1458 - val_accuracy: 0.9588 - val_mse: 0.0215\n",
            "Epoch 111/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1744 - accuracy: 0.9523 - mse: 0.0285 - val_loss: 0.1444 - val_accuracy: 0.9588 - val_mse: 0.0213\n",
            "Epoch 112/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1815 - accuracy: 0.9523 - mse: 0.0300 - val_loss: 0.1335 - val_accuracy: 0.9742 - val_mse: 0.0189\n",
            "Epoch 113/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1969 - accuracy: 0.9355 - mse: 0.0334 - val_loss: 0.1246 - val_accuracy: 0.9794 - val_mse: 0.0171\n",
            "Epoch 114/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1684 - accuracy: 0.9445 - mse: 0.0283 - val_loss: 0.1154 - val_accuracy: 0.9845 - val_mse: 0.0154\n",
            "Epoch 115/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1875 - accuracy: 0.9445 - mse: 0.0314 - val_loss: 0.1091 - val_accuracy: 0.9845 - val_mse: 0.0144\n",
            "Epoch 116/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1899 - accuracy: 0.9419 - mse: 0.0325 - val_loss: 0.1097 - val_accuracy: 0.9794 - val_mse: 0.0148\n",
            "Epoch 117/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1610 - accuracy: 0.9510 - mse: 0.0265 - val_loss: 0.1112 - val_accuracy: 0.9794 - val_mse: 0.0151\n",
            "Epoch 118/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1735 - accuracy: 0.9445 - mse: 0.0291 - val_loss: 0.1131 - val_accuracy: 0.9794 - val_mse: 0.0154\n",
            "Epoch 119/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1751 - accuracy: 0.9497 - mse: 0.0292 - val_loss: 0.1311 - val_accuracy: 0.9742 - val_mse: 0.0187\n",
            "Epoch 120/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1564 - accuracy: 0.9535 - mse: 0.0248 - val_loss: 0.1442 - val_accuracy: 0.9639 - val_mse: 0.0211\n",
            "Epoch 121/200\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1798 - accuracy: 0.9535 - mse: 0.0297 - val_loss: 0.1420 - val_accuracy: 0.9691 - val_mse: 0.0205\n",
            "Epoch 122/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1585 - accuracy: 0.9523 - mse: 0.0263 - val_loss: 0.1282 - val_accuracy: 0.9794 - val_mse: 0.0177\n",
            "Epoch 123/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1721 - accuracy: 0.9510 - mse: 0.0291 - val_loss: 0.1157 - val_accuracy: 0.9897 - val_mse: 0.0152\n",
            "Epoch 124/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1832 - accuracy: 0.9432 - mse: 0.0311 - val_loss: 0.1031 - val_accuracy: 0.9897 - val_mse: 0.0127\n",
            "Epoch 125/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1656 - accuracy: 0.9523 - mse: 0.0277 - val_loss: 0.0928 - val_accuracy: 0.9897 - val_mse: 0.0110\n",
            "Epoch 126/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1575 - accuracy: 0.9510 - mse: 0.0257 - val_loss: 0.0940 - val_accuracy: 0.9897 - val_mse: 0.0114\n",
            "Epoch 127/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1555 - accuracy: 0.9548 - mse: 0.0255 - val_loss: 0.1054 - val_accuracy: 0.9845 - val_mse: 0.0137\n",
            "Epoch 128/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1526 - accuracy: 0.9652 - mse: 0.0240 - val_loss: 0.1087 - val_accuracy: 0.9794 - val_mse: 0.0144\n",
            "Epoch 129/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1582 - accuracy: 0.9510 - mse: 0.0263 - val_loss: 0.1060 - val_accuracy: 0.9794 - val_mse: 0.0140\n",
            "Epoch 130/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1517 - accuracy: 0.9587 - mse: 0.0246 - val_loss: 0.1018 - val_accuracy: 0.9794 - val_mse: 0.0133\n",
            "Epoch 131/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1517 - accuracy: 0.9587 - mse: 0.0246 - val_loss: 0.0983 - val_accuracy: 0.9794 - val_mse: 0.0128\n",
            "Epoch 132/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1661 - accuracy: 0.9458 - mse: 0.0286 - val_loss: 0.0995 - val_accuracy: 0.9794 - val_mse: 0.0131\n",
            "Epoch 133/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1544 - accuracy: 0.9639 - mse: 0.0240 - val_loss: 0.1011 - val_accuracy: 0.9794 - val_mse: 0.0134\n",
            "Epoch 134/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1514 - accuracy: 0.9548 - mse: 0.0253 - val_loss: 0.0941 - val_accuracy: 0.9897 - val_mse: 0.0120\n",
            "Epoch 135/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1491 - accuracy: 0.9548 - mse: 0.0245 - val_loss: 0.0764 - val_accuracy: 0.9897 - val_mse: 0.0085\n",
            "Epoch 136/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1417 - accuracy: 0.9613 - mse: 0.0222 - val_loss: 0.0658 - val_accuracy: 1.0000 - val_mse: 0.0066\n",
            "Epoch 137/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1497 - accuracy: 0.9535 - mse: 0.0255 - val_loss: 0.0657 - val_accuracy: 1.0000 - val_mse: 0.0065\n",
            "Epoch 138/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1491 - accuracy: 0.9600 - mse: 0.0235 - val_loss: 0.0783 - val_accuracy: 0.9948 - val_mse: 0.0086\n",
            "Epoch 139/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1574 - accuracy: 0.9548 - mse: 0.0255 - val_loss: 0.0959 - val_accuracy: 0.9897 - val_mse: 0.0116\n",
            "Epoch 140/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1424 - accuracy: 0.9613 - mse: 0.0218 - val_loss: 0.1033 - val_accuracy: 0.9897 - val_mse: 0.0130\n",
            "Epoch 141/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1477 - accuracy: 0.9561 - mse: 0.0243 - val_loss: 0.1004 - val_accuracy: 0.9897 - val_mse: 0.0124\n",
            "Epoch 142/200\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1406 - accuracy: 0.9548 - mse: 0.0236 - val_loss: 0.0924 - val_accuracy: 0.9897 - val_mse: 0.0109\n",
            "Epoch 143/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1545 - accuracy: 0.9574 - mse: 0.0246 - val_loss: 0.0798 - val_accuracy: 1.0000 - val_mse: 0.0085\n",
            "Epoch 144/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1276 - accuracy: 0.9626 - mse: 0.0197 - val_loss: 0.0727 - val_accuracy: 1.0000 - val_mse: 0.0073\n",
            "Epoch 145/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1284 - accuracy: 0.9626 - mse: 0.0201 - val_loss: 0.0706 - val_accuracy: 1.0000 - val_mse: 0.0069\n",
            "Epoch 146/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1210 - accuracy: 0.9665 - mse: 0.0195 - val_loss: 0.0676 - val_accuracy: 1.0000 - val_mse: 0.0065\n",
            "Epoch 147/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1327 - accuracy: 0.9600 - mse: 0.0220 - val_loss: 0.0674 - val_accuracy: 1.0000 - val_mse: 0.0066\n",
            "Epoch 148/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1348 - accuracy: 0.9626 - mse: 0.0219 - val_loss: 0.0702 - val_accuracy: 1.0000 - val_mse: 0.0071\n",
            "Epoch 149/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1210 - accuracy: 0.9781 - mse: 0.0179 - val_loss: 0.0721 - val_accuracy: 1.0000 - val_mse: 0.0075\n",
            "Epoch 150/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1359 - accuracy: 0.9561 - mse: 0.0229 - val_loss: 0.0693 - val_accuracy: 1.0000 - val_mse: 0.0071\n",
            "Epoch 151/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1320 - accuracy: 0.9600 - mse: 0.0224 - val_loss: 0.0636 - val_accuracy: 1.0000 - val_mse: 0.0062\n",
            "Epoch 152/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1369 - accuracy: 0.9677 - mse: 0.0212 - val_loss: 0.0608 - val_accuracy: 1.0000 - val_mse: 0.0057\n",
            "Epoch 153/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1293 - accuracy: 0.9574 - mse: 0.0211 - val_loss: 0.0677 - val_accuracy: 1.0000 - val_mse: 0.0066\n",
            "Epoch 154/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1338 - accuracy: 0.9665 - mse: 0.0208 - val_loss: 0.0722 - val_accuracy: 1.0000 - val_mse: 0.0073\n",
            "Epoch 155/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1371 - accuracy: 0.9548 - mse: 0.0228 - val_loss: 0.0780 - val_accuracy: 1.0000 - val_mse: 0.0082\n",
            "Epoch 156/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1241 - accuracy: 0.9639 - mse: 0.0203 - val_loss: 0.0721 - val_accuracy: 1.0000 - val_mse: 0.0072\n",
            "Epoch 157/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1183 - accuracy: 0.9677 - mse: 0.0187 - val_loss: 0.0664 - val_accuracy: 1.0000 - val_mse: 0.0063\n",
            "Epoch 158/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1282 - accuracy: 0.9561 - mse: 0.0216 - val_loss: 0.0691 - val_accuracy: 1.0000 - val_mse: 0.0067\n",
            "Epoch 159/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1214 - accuracy: 0.9574 - mse: 0.0203 - val_loss: 0.0710 - val_accuracy: 1.0000 - val_mse: 0.0071\n",
            "Epoch 160/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1287 - accuracy: 0.9626 - mse: 0.0203 - val_loss: 0.0690 - val_accuracy: 1.0000 - val_mse: 0.0068\n",
            "Epoch 161/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1083 - accuracy: 0.9729 - mse: 0.0164 - val_loss: 0.0657 - val_accuracy: 1.0000 - val_mse: 0.0063\n",
            "Epoch 162/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1062 - accuracy: 0.9742 - mse: 0.0161 - val_loss: 0.0650 - val_accuracy: 1.0000 - val_mse: 0.0062\n",
            "Epoch 163/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1290 - accuracy: 0.9665 - mse: 0.0191 - val_loss: 0.0604 - val_accuracy: 1.0000 - val_mse: 0.0055\n",
            "Epoch 164/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1244 - accuracy: 0.9639 - mse: 0.0196 - val_loss: 0.0583 - val_accuracy: 1.0000 - val_mse: 0.0052\n",
            "Epoch 165/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1061 - accuracy: 0.9690 - mse: 0.0172 - val_loss: 0.0599 - val_accuracy: 1.0000 - val_mse: 0.0054\n",
            "Epoch 166/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0991 - accuracy: 0.9729 - mse: 0.0155 - val_loss: 0.0551 - val_accuracy: 1.0000 - val_mse: 0.0047\n",
            "Epoch 167/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1014 - accuracy: 0.9716 - mse: 0.0160 - val_loss: 0.0494 - val_accuracy: 1.0000 - val_mse: 0.0039\n",
            "Epoch 168/200\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1155 - accuracy: 0.9755 - mse: 0.0177 - val_loss: 0.0457 - val_accuracy: 1.0000 - val_mse: 0.0034\n",
            "Epoch 169/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1263 - accuracy: 0.9626 - mse: 0.0199 - val_loss: 0.0430 - val_accuracy: 1.0000 - val_mse: 0.0031\n",
            "Epoch 170/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1069 - accuracy: 0.9703 - mse: 0.0163 - val_loss: 0.0408 - val_accuracy: 1.0000 - val_mse: 0.0029\n",
            "Epoch 171/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1000 - accuracy: 0.9755 - mse: 0.0148 - val_loss: 0.0403 - val_accuracy: 1.0000 - val_mse: 0.0028\n",
            "Epoch 172/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1244 - accuracy: 0.9626 - mse: 0.0201 - val_loss: 0.0429 - val_accuracy: 1.0000 - val_mse: 0.0031\n",
            "Epoch 173/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1154 - accuracy: 0.9665 - mse: 0.0188 - val_loss: 0.0449 - val_accuracy: 1.0000 - val_mse: 0.0033\n",
            "Epoch 174/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0993 - accuracy: 0.9819 - mse: 0.0143 - val_loss: 0.0468 - val_accuracy: 1.0000 - val_mse: 0.0035\n",
            "Epoch 175/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1028 - accuracy: 0.9677 - mse: 0.0163 - val_loss: 0.0480 - val_accuracy: 1.0000 - val_mse: 0.0037\n",
            "Epoch 176/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0846 - accuracy: 0.9858 - mse: 0.0116 - val_loss: 0.0471 - val_accuracy: 1.0000 - val_mse: 0.0036\n",
            "Epoch 177/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1160 - accuracy: 0.9639 - mse: 0.0197 - val_loss: 0.0470 - val_accuracy: 1.0000 - val_mse: 0.0036\n",
            "Epoch 178/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1045 - accuracy: 0.9755 - mse: 0.0158 - val_loss: 0.0426 - val_accuracy: 1.0000 - val_mse: 0.0031\n",
            "Epoch 179/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1204 - accuracy: 0.9652 - mse: 0.0192 - val_loss: 0.0399 - val_accuracy: 1.0000 - val_mse: 0.0028\n",
            "Epoch 180/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0932 - accuracy: 0.9729 - mse: 0.0150 - val_loss: 0.0388 - val_accuracy: 1.0000 - val_mse: 0.0026\n",
            "Epoch 181/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0910 - accuracy: 0.9806 - mse: 0.0131 - val_loss: 0.0424 - val_accuracy: 1.0000 - val_mse: 0.0030\n",
            "Epoch 182/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0893 - accuracy: 0.9806 - mse: 0.0136 - val_loss: 0.0427 - val_accuracy: 1.0000 - val_mse: 0.0031\n",
            "Epoch 183/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1019 - accuracy: 0.9755 - mse: 0.0157 - val_loss: 0.0389 - val_accuracy: 1.0000 - val_mse: 0.0026\n",
            "Epoch 184/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0968 - accuracy: 0.9729 - mse: 0.0154 - val_loss: 0.0402 - val_accuracy: 1.0000 - val_mse: 0.0028\n",
            "Epoch 185/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0982 - accuracy: 0.9716 - mse: 0.0158 - val_loss: 0.0438 - val_accuracy: 1.0000 - val_mse: 0.0032\n",
            "Epoch 186/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1113 - accuracy: 0.9703 - mse: 0.0172 - val_loss: 0.0457 - val_accuracy: 1.0000 - val_mse: 0.0034\n",
            "Epoch 187/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1068 - accuracy: 0.9716 - mse: 0.0163 - val_loss: 0.0452 - val_accuracy: 1.0000 - val_mse: 0.0033\n",
            "Epoch 188/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1086 - accuracy: 0.9677 - mse: 0.0173 - val_loss: 0.0463 - val_accuracy: 1.0000 - val_mse: 0.0033\n",
            "Epoch 189/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0902 - accuracy: 0.9806 - mse: 0.0134 - val_loss: 0.0437 - val_accuracy: 1.0000 - val_mse: 0.0030\n",
            "Epoch 190/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0999 - accuracy: 0.9742 - mse: 0.0152 - val_loss: 0.0408 - val_accuracy: 1.0000 - val_mse: 0.0027\n",
            "Epoch 191/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0939 - accuracy: 0.9716 - mse: 0.0147 - val_loss: 0.0397 - val_accuracy: 1.0000 - val_mse: 0.0025\n",
            "Epoch 192/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1018 - accuracy: 0.9742 - mse: 0.0151 - val_loss: 0.0391 - val_accuracy: 1.0000 - val_mse: 0.0025\n",
            "Epoch 193/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0950 - accuracy: 0.9652 - mse: 0.0148 - val_loss: 0.0375 - val_accuracy: 1.0000 - val_mse: 0.0023\n",
            "Epoch 194/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0820 - accuracy: 0.9703 - mse: 0.0132 - val_loss: 0.0362 - val_accuracy: 1.0000 - val_mse: 0.0022\n",
            "Epoch 195/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0887 - accuracy: 0.9806 - mse: 0.0124 - val_loss: 0.0376 - val_accuracy: 1.0000 - val_mse: 0.0024\n",
            "Epoch 196/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0832 - accuracy: 0.9781 - mse: 0.0119 - val_loss: 0.0385 - val_accuracy: 1.0000 - val_mse: 0.0024\n",
            "Epoch 197/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0935 - accuracy: 0.9768 - mse: 0.0146 - val_loss: 0.0357 - val_accuracy: 1.0000 - val_mse: 0.0021\n",
            "Epoch 198/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0820 - accuracy: 0.9781 - mse: 0.0122 - val_loss: 0.0323 - val_accuracy: 1.0000 - val_mse: 0.0017\n",
            "Epoch 199/200\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1059 - accuracy: 0.9677 - mse: 0.0170 - val_loss: 0.0321 - val_accuracy: 1.0000 - val_mse: 0.0017\n",
            "Epoch 200/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0952 - accuracy: 0.9652 - mse: 0.0161 - val_loss: 0.0330 - val_accuracy: 1.0000 - val_mse: 0.0018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9805724034822184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kjfugu8_NJrM",
        "outputId": "ed27437b-590d-44af-906d-8bc5e23e1a20"
      },
      "source": [
        "Xo, Yo = over_sample(fn.X, fn.Y,'SVMSMOTE')\n",
        "borderline_model = train_model(Xo, Yo,200)\n",
        "output = borderline_model.predict_classes(np.array(fn.X))\n",
        "f1_score(np.argmax(fn.Y,1), output,labels=[0,1,2,3],average='weighted')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 82ms/step - loss: 1.1057 - accuracy: 0.3547 - mse: 0.2248 - val_loss: 2.1165 - val_accuracy: 0.0000e+00 - val_mse: 0.3621\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 1.0564 - accuracy: 0.4074 - mse: 0.2155 - val_loss: 2.1084 - val_accuracy: 0.0000e+00 - val_mse: 0.3632\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 1.0421 - accuracy: 0.4444 - mse: 0.2098 - val_loss: 2.1087 - val_accuracy: 0.0000e+00 - val_mse: 0.3654\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 1.0112 - accuracy: 0.4701 - mse: 0.2050 - val_loss: 2.1016 - val_accuracy: 0.0000e+00 - val_mse: 0.3673\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.9883 - accuracy: 0.4815 - mse: 0.2005 - val_loss: 2.0935 - val_accuracy: 0.0000e+00 - val_mse: 0.3692\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.9633 - accuracy: 0.5256 - mse: 0.1941 - val_loss: 2.0839 - val_accuracy: 0.0000e+00 - val_mse: 0.3713\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.9405 - accuracy: 0.5570 - mse: 0.1884 - val_loss: 2.0639 - val_accuracy: 0.0000e+00 - val_mse: 0.3726\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.9118 - accuracy: 0.5399 - mse: 0.1854 - val_loss: 2.0413 - val_accuracy: 0.0000e+00 - val_mse: 0.3738\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.8769 - accuracy: 0.5798 - mse: 0.1786 - val_loss: 2.0288 - val_accuracy: 0.0000e+00 - val_mse: 0.3768\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.8608 - accuracy: 0.6026 - mse: 0.1751 - val_loss: 2.0159 - val_accuracy: 0.0000e+00 - val_mse: 0.3798\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.8413 - accuracy: 0.6339 - mse: 0.1703 - val_loss: 1.9974 - val_accuracy: 0.0000e+00 - val_mse: 0.3815\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.8301 - accuracy: 0.6111 - mse: 0.1684 - val_loss: 1.9756 - val_accuracy: 0.0000e+00 - val_mse: 0.3823\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.8125 - accuracy: 0.6553 - mse: 0.1651 - val_loss: 1.9546 - val_accuracy: 0.0000e+00 - val_mse: 0.3823\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.7826 - accuracy: 0.6667 - mse: 0.1583 - val_loss: 1.9282 - val_accuracy: 0.0000e+00 - val_mse: 0.3810\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.7789 - accuracy: 0.6766 - mse: 0.1574 - val_loss: 1.8937 - val_accuracy: 0.0000e+00 - val_mse: 0.3787\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.7484 - accuracy: 0.6795 - mse: 0.1517 - val_loss: 1.8658 - val_accuracy: 0.0000e+00 - val_mse: 0.3764\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.7438 - accuracy: 0.6838 - mse: 0.1501 - val_loss: 1.8400 - val_accuracy: 0.0000e+00 - val_mse: 0.3743\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.7114 - accuracy: 0.7066 - mse: 0.1435 - val_loss: 1.8203 - val_accuracy: 0.0000e+00 - val_mse: 0.3727\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.6873 - accuracy: 0.7265 - mse: 0.1381 - val_loss: 1.8059 - val_accuracy: 0.0000e+00 - val_mse: 0.3723\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.6704 - accuracy: 0.7308 - mse: 0.1343 - val_loss: 1.7923 - val_accuracy: 0.0000e+00 - val_mse: 0.3727\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.6460 - accuracy: 0.7336 - mse: 0.1292 - val_loss: 1.7763 - val_accuracy: 0.0000e+00 - val_mse: 0.3727\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.6492 - accuracy: 0.7422 - mse: 0.1294 - val_loss: 1.7556 - val_accuracy: 0.0000e+00 - val_mse: 0.3711\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.6281 - accuracy: 0.7393 - mse: 0.1249 - val_loss: 1.7265 - val_accuracy: 0.0000e+00 - val_mse: 0.3677\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.5838 - accuracy: 0.7949 - mse: 0.1132 - val_loss: 1.6988 - val_accuracy: 0.0000e+00 - val_mse: 0.3648\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.6125 - accuracy: 0.7450 - mse: 0.1226 - val_loss: 1.6678 - val_accuracy: 0.0000e+00 - val_mse: 0.3612\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.5628 - accuracy: 0.7877 - mse: 0.1100 - val_loss: 1.6356 - val_accuracy: 0.0000e+00 - val_mse: 0.3573\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.5479 - accuracy: 0.8077 - mse: 0.1057 - val_loss: 1.6136 - val_accuracy: 0.0000e+00 - val_mse: 0.3554\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.5265 - accuracy: 0.8120 - mse: 0.1009 - val_loss: 1.5937 - val_accuracy: 0.0000e+00 - val_mse: 0.3528\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.5140 - accuracy: 0.8006 - mse: 0.1009 - val_loss: 1.5761 - val_accuracy: 0.0000e+00 - val_mse: 0.3501\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.5119 - accuracy: 0.8105 - mse: 0.0991 - val_loss: 1.5555 - val_accuracy: 0.0000e+00 - val_mse: 0.3468\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.4955 - accuracy: 0.7963 - mse: 0.0973 - val_loss: 1.5332 - val_accuracy: 0.0000e+00 - val_mse: 0.3432\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.4695 - accuracy: 0.8205 - mse: 0.0906 - val_loss: 1.5088 - val_accuracy: 0.0000e+00 - val_mse: 0.3382\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.4550 - accuracy: 0.8191 - mse: 0.0871 - val_loss: 1.4831 - val_accuracy: 0.0000e+00 - val_mse: 0.3331\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.4588 - accuracy: 0.8134 - mse: 0.0890 - val_loss: 1.4609 - val_accuracy: 0.0114 - val_mse: 0.3292\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.4407 - accuracy: 0.8376 - mse: 0.0841 - val_loss: 1.4376 - val_accuracy: 0.0341 - val_mse: 0.3251\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.4443 - accuracy: 0.8305 - mse: 0.0866 - val_loss: 1.4080 - val_accuracy: 0.0511 - val_mse: 0.3192\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.4203 - accuracy: 0.8405 - mse: 0.0804 - val_loss: 1.3748 - val_accuracy: 0.0852 - val_mse: 0.3133\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.4124 - accuracy: 0.8476 - mse: 0.0782 - val_loss: 1.3454 - val_accuracy: 0.0966 - val_mse: 0.3074\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.3951 - accuracy: 0.8575 - mse: 0.0740 - val_loss: 1.3186 - val_accuracy: 0.1080 - val_mse: 0.3018\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3995 - accuracy: 0.8490 - mse: 0.0767 - val_loss: 1.2843 - val_accuracy: 0.1250 - val_mse: 0.2938\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.3884 - accuracy: 0.8519 - mse: 0.0738 - val_loss: 1.2583 - val_accuracy: 0.1420 - val_mse: 0.2872\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.3845 - accuracy: 0.8476 - mse: 0.0735 - val_loss: 1.2376 - val_accuracy: 0.1477 - val_mse: 0.2818\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3785 - accuracy: 0.8504 - mse: 0.0721 - val_loss: 1.2224 - val_accuracy: 0.1477 - val_mse: 0.2794\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.3570 - accuracy: 0.8632 - mse: 0.0670 - val_loss: 1.2026 - val_accuracy: 0.1534 - val_mse: 0.2754\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3462 - accuracy: 0.8661 - mse: 0.0639 - val_loss: 1.1903 - val_accuracy: 0.1818 - val_mse: 0.2732\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3588 - accuracy: 0.8604 - mse: 0.0675 - val_loss: 1.1719 - val_accuracy: 0.1989 - val_mse: 0.2698\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.3334 - accuracy: 0.8732 - mse: 0.0622 - val_loss: 1.1563 - val_accuracy: 0.2102 - val_mse: 0.2662\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.3265 - accuracy: 0.8832 - mse: 0.0607 - val_loss: 1.1391 - val_accuracy: 0.2159 - val_mse: 0.2613\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3298 - accuracy: 0.8718 - mse: 0.0612 - val_loss: 1.1258 - val_accuracy: 0.2273 - val_mse: 0.2572\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.3133 - accuracy: 0.8875 - mse: 0.0565 - val_loss: 1.1165 - val_accuracy: 0.2386 - val_mse: 0.2539\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.3184 - accuracy: 0.8803 - mse: 0.0597 - val_loss: 1.1028 - val_accuracy: 0.2386 - val_mse: 0.2498\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3102 - accuracy: 0.8704 - mse: 0.0589 - val_loss: 1.0844 - val_accuracy: 0.2443 - val_mse: 0.2459\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3145 - accuracy: 0.8974 - mse: 0.0573 - val_loss: 1.0657 - val_accuracy: 0.2443 - val_mse: 0.2424\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.3209 - accuracy: 0.8775 - mse: 0.0595 - val_loss: 1.0446 - val_accuracy: 0.2670 - val_mse: 0.2379\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.3171 - accuracy: 0.8761 - mse: 0.0593 - val_loss: 1.0207 - val_accuracy: 0.2727 - val_mse: 0.2319\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2951 - accuracy: 0.8775 - mse: 0.0556 - val_loss: 0.9982 - val_accuracy: 0.3011 - val_mse: 0.2264\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.2904 - accuracy: 0.9003 - mse: 0.0535 - val_loss: 0.9778 - val_accuracy: 0.3011 - val_mse: 0.2220\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2885 - accuracy: 0.9017 - mse: 0.0519 - val_loss: 0.9487 - val_accuracy: 0.3920 - val_mse: 0.2147\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.3046 - accuracy: 0.8903 - mse: 0.0563 - val_loss: 0.9229 - val_accuracy: 0.4318 - val_mse: 0.2084\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 0.2856 - accuracy: 0.8989 - mse: 0.0522 - val_loss: 0.9123 - val_accuracy: 0.4318 - val_mse: 0.2055\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2769 - accuracy: 0.9145 - mse: 0.0489 - val_loss: 0.8988 - val_accuracy: 0.4602 - val_mse: 0.2006\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2734 - accuracy: 0.9117 - mse: 0.0489 - val_loss: 0.9035 - val_accuracy: 0.4659 - val_mse: 0.2020\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.2834 - accuracy: 0.9003 - mse: 0.0516 - val_loss: 0.9097 - val_accuracy: 0.4659 - val_mse: 0.2045\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.2646 - accuracy: 0.8960 - mse: 0.0488 - val_loss: 0.9084 - val_accuracy: 0.4659 - val_mse: 0.2048\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2632 - accuracy: 0.9031 - mse: 0.0484 - val_loss: 0.8928 - val_accuracy: 0.4773 - val_mse: 0.2009\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2546 - accuracy: 0.9131 - mse: 0.0458 - val_loss: 0.8812 - val_accuracy: 0.4886 - val_mse: 0.1980\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2504 - accuracy: 0.9074 - mse: 0.0452 - val_loss: 0.8631 - val_accuracy: 0.5170 - val_mse: 0.1931\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2587 - accuracy: 0.9174 - mse: 0.0463 - val_loss: 0.8452 - val_accuracy: 0.5284 - val_mse: 0.1878\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2442 - accuracy: 0.9188 - mse: 0.0448 - val_loss: 0.8356 - val_accuracy: 0.5398 - val_mse: 0.1851\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.2553 - accuracy: 0.9074 - mse: 0.0460 - val_loss: 0.8314 - val_accuracy: 0.5398 - val_mse: 0.1842\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2346 - accuracy: 0.9217 - mse: 0.0422 - val_loss: 0.8216 - val_accuracy: 0.5568 - val_mse: 0.1818\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2391 - accuracy: 0.9302 - mse: 0.0420 - val_loss: 0.8139 - val_accuracy: 0.5682 - val_mse: 0.1798\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.2332 - accuracy: 0.9145 - mse: 0.0424 - val_loss: 0.8061 - val_accuracy: 0.5682 - val_mse: 0.1777\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2322 - accuracy: 0.9288 - mse: 0.0417 - val_loss: 0.7926 - val_accuracy: 0.5909 - val_mse: 0.1735\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2287 - accuracy: 0.9288 - mse: 0.0412 - val_loss: 0.7949 - val_accuracy: 0.5909 - val_mse: 0.1739\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2372 - accuracy: 0.9217 - mse: 0.0409 - val_loss: 0.7963 - val_accuracy: 0.5966 - val_mse: 0.1747\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2176 - accuracy: 0.9217 - mse: 0.0387 - val_loss: 0.7905 - val_accuracy: 0.6023 - val_mse: 0.1737\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2329 - accuracy: 0.9259 - mse: 0.0420 - val_loss: 0.7779 - val_accuracy: 0.6080 - val_mse: 0.1709\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2255 - accuracy: 0.9231 - mse: 0.0406 - val_loss: 0.7580 - val_accuracy: 0.6193 - val_mse: 0.1660\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2229 - accuracy: 0.9274 - mse: 0.0384 - val_loss: 0.7445 - val_accuracy: 0.6364 - val_mse: 0.1623\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2303 - accuracy: 0.9330 - mse: 0.0401 - val_loss: 0.7259 - val_accuracy: 0.6477 - val_mse: 0.1572\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2143 - accuracy: 0.9145 - mse: 0.0389 - val_loss: 0.7141 - val_accuracy: 0.6591 - val_mse: 0.1543\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2017 - accuracy: 0.9373 - mse: 0.0353 - val_loss: 0.7041 - val_accuracy: 0.6648 - val_mse: 0.1518\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1964 - accuracy: 0.9345 - mse: 0.0337 - val_loss: 0.6888 - val_accuracy: 0.6875 - val_mse: 0.1477\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2037 - accuracy: 0.9373 - mse: 0.0340 - val_loss: 0.6784 - val_accuracy: 0.6875 - val_mse: 0.1446\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1968 - accuracy: 0.9302 - mse: 0.0349 - val_loss: 0.6720 - val_accuracy: 0.6932 - val_mse: 0.1429\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2094 - accuracy: 0.9345 - mse: 0.0370 - val_loss: 0.6744 - val_accuracy: 0.6932 - val_mse: 0.1440\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1987 - accuracy: 0.9359 - mse: 0.0348 - val_loss: 0.6823 - val_accuracy: 0.6875 - val_mse: 0.1462\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2188 - accuracy: 0.9202 - mse: 0.0392 - val_loss: 0.6744 - val_accuracy: 0.6932 - val_mse: 0.1443\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.2038 - accuracy: 0.9274 - mse: 0.0369 - val_loss: 0.6617 - val_accuracy: 0.6989 - val_mse: 0.1411\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1995 - accuracy: 0.9330 - mse: 0.0345 - val_loss: 0.6582 - val_accuracy: 0.7045 - val_mse: 0.1407\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1819 - accuracy: 0.9473 - mse: 0.0306 - val_loss: 0.6440 - val_accuracy: 0.7045 - val_mse: 0.1369\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.2012 - accuracy: 0.9373 - mse: 0.0354 - val_loss: 0.6337 - val_accuracy: 0.7159 - val_mse: 0.1339\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1830 - accuracy: 0.9444 - mse: 0.0311 - val_loss: 0.6283 - val_accuracy: 0.7273 - val_mse: 0.1323\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1916 - accuracy: 0.9345 - mse: 0.0331 - val_loss: 0.6207 - val_accuracy: 0.7273 - val_mse: 0.1306\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1812 - accuracy: 0.9416 - mse: 0.0303 - val_loss: 0.6123 - val_accuracy: 0.7330 - val_mse: 0.1285\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1604 - accuracy: 0.9530 - mse: 0.0264 - val_loss: 0.6077 - val_accuracy: 0.7273 - val_mse: 0.1273\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1706 - accuracy: 0.9459 - mse: 0.0292 - val_loss: 0.6095 - val_accuracy: 0.7273 - val_mse: 0.1276\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1931 - accuracy: 0.9402 - mse: 0.0331 - val_loss: 0.6075 - val_accuracy: 0.7330 - val_mse: 0.1274\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1751 - accuracy: 0.9430 - mse: 0.0304 - val_loss: 0.6016 - val_accuracy: 0.7386 - val_mse: 0.1259\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1726 - accuracy: 0.9544 - mse: 0.0291 - val_loss: 0.5999 - val_accuracy: 0.7500 - val_mse: 0.1253\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1673 - accuracy: 0.9444 - mse: 0.0286 - val_loss: 0.6030 - val_accuracy: 0.7330 - val_mse: 0.1264\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1611 - accuracy: 0.9444 - mse: 0.0288 - val_loss: 0.5962 - val_accuracy: 0.7443 - val_mse: 0.1246\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1681 - accuracy: 0.9487 - mse: 0.0290 - val_loss: 0.5893 - val_accuracy: 0.7443 - val_mse: 0.1229\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1746 - accuracy: 0.9416 - mse: 0.0312 - val_loss: 0.5892 - val_accuracy: 0.7443 - val_mse: 0.1233\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1664 - accuracy: 0.9473 - mse: 0.0281 - val_loss: 0.5743 - val_accuracy: 0.7557 - val_mse: 0.1198\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1597 - accuracy: 0.9544 - mse: 0.0262 - val_loss: 0.5564 - val_accuracy: 0.7614 - val_mse: 0.1152\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1563 - accuracy: 0.9430 - mse: 0.0269 - val_loss: 0.5359 - val_accuracy: 0.7841 - val_mse: 0.1096\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1598 - accuracy: 0.9558 - mse: 0.0264 - val_loss: 0.5280 - val_accuracy: 0.7898 - val_mse: 0.1075\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1783 - accuracy: 0.9473 - mse: 0.0296 - val_loss: 0.5272 - val_accuracy: 0.7841 - val_mse: 0.1075\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1521 - accuracy: 0.9573 - mse: 0.0257 - val_loss: 0.5298 - val_accuracy: 0.7841 - val_mse: 0.1084\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1551 - accuracy: 0.9573 - mse: 0.0259 - val_loss: 0.5297 - val_accuracy: 0.7841 - val_mse: 0.1088\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1702 - accuracy: 0.9430 - mse: 0.0286 - val_loss: 0.5220 - val_accuracy: 0.7841 - val_mse: 0.1071\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1564 - accuracy: 0.9501 - mse: 0.0271 - val_loss: 0.5153 - val_accuracy: 0.7841 - val_mse: 0.1056\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1784 - accuracy: 0.9473 - mse: 0.0294 - val_loss: 0.5042 - val_accuracy: 0.7898 - val_mse: 0.1030\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1600 - accuracy: 0.9501 - mse: 0.0276 - val_loss: 0.4981 - val_accuracy: 0.7955 - val_mse: 0.1016\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1574 - accuracy: 0.9573 - mse: 0.0258 - val_loss: 0.4966 - val_accuracy: 0.7955 - val_mse: 0.1012\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1505 - accuracy: 0.9530 - mse: 0.0262 - val_loss: 0.4884 - val_accuracy: 0.8011 - val_mse: 0.0991\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1553 - accuracy: 0.9573 - mse: 0.0257 - val_loss: 0.4797 - val_accuracy: 0.8125 - val_mse: 0.0966\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1529 - accuracy: 0.9544 - mse: 0.0265 - val_loss: 0.4695 - val_accuracy: 0.8125 - val_mse: 0.0940\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1409 - accuracy: 0.9644 - mse: 0.0232 - val_loss: 0.4644 - val_accuracy: 0.8125 - val_mse: 0.0927\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1543 - accuracy: 0.9558 - mse: 0.0264 - val_loss: 0.4717 - val_accuracy: 0.8125 - val_mse: 0.0947\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1393 - accuracy: 0.9644 - mse: 0.0223 - val_loss: 0.4769 - val_accuracy: 0.8125 - val_mse: 0.0963\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1451 - accuracy: 0.9501 - mse: 0.0250 - val_loss: 0.4700 - val_accuracy: 0.8182 - val_mse: 0.0943\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1290 - accuracy: 0.9658 - mse: 0.0205 - val_loss: 0.4644 - val_accuracy: 0.8182 - val_mse: 0.0928\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1548 - accuracy: 0.9501 - mse: 0.0275 - val_loss: 0.4588 - val_accuracy: 0.8182 - val_mse: 0.0914\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1216 - accuracy: 0.9658 - mse: 0.0205 - val_loss: 0.4581 - val_accuracy: 0.8125 - val_mse: 0.0916\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1317 - accuracy: 0.9558 - mse: 0.0227 - val_loss: 0.4483 - val_accuracy: 0.8239 - val_mse: 0.0894\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1295 - accuracy: 0.9630 - mse: 0.0215 - val_loss: 0.4343 - val_accuracy: 0.8352 - val_mse: 0.0858\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1380 - accuracy: 0.9601 - mse: 0.0228 - val_loss: 0.4316 - val_accuracy: 0.8409 - val_mse: 0.0851\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1506 - accuracy: 0.9459 - mse: 0.0263 - val_loss: 0.4298 - val_accuracy: 0.8466 - val_mse: 0.0849\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1358 - accuracy: 0.9587 - mse: 0.0231 - val_loss: 0.4285 - val_accuracy: 0.8466 - val_mse: 0.0847\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1094 - accuracy: 0.9758 - mse: 0.0170 - val_loss: 0.4246 - val_accuracy: 0.8466 - val_mse: 0.0837\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1359 - accuracy: 0.9573 - mse: 0.0232 - val_loss: 0.4268 - val_accuracy: 0.8466 - val_mse: 0.0844\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 23ms/step - loss: 0.1374 - accuracy: 0.9587 - mse: 0.0241 - val_loss: 0.4290 - val_accuracy: 0.8409 - val_mse: 0.0854\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1312 - accuracy: 0.9573 - mse: 0.0211 - val_loss: 0.4296 - val_accuracy: 0.8409 - val_mse: 0.0857\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1263 - accuracy: 0.9630 - mse: 0.0211 - val_loss: 0.4227 - val_accuracy: 0.8409 - val_mse: 0.0837\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1278 - accuracy: 0.9644 - mse: 0.0215 - val_loss: 0.4129 - val_accuracy: 0.8523 - val_mse: 0.0807\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1271 - accuracy: 0.9615 - mse: 0.0222 - val_loss: 0.4075 - val_accuracy: 0.8523 - val_mse: 0.0792\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1263 - accuracy: 0.9630 - mse: 0.0209 - val_loss: 0.4020 - val_accuracy: 0.8466 - val_mse: 0.0781\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1346 - accuracy: 0.9615 - mse: 0.0232 - val_loss: 0.3935 - val_accuracy: 0.8523 - val_mse: 0.0763\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1328 - accuracy: 0.9601 - mse: 0.0230 - val_loss: 0.3836 - val_accuracy: 0.8636 - val_mse: 0.0740\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1222 - accuracy: 0.9672 - mse: 0.0204 - val_loss: 0.3832 - val_accuracy: 0.8580 - val_mse: 0.0740\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1283 - accuracy: 0.9573 - mse: 0.0224 - val_loss: 0.3760 - val_accuracy: 0.8580 - val_mse: 0.0721\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1248 - accuracy: 0.9658 - mse: 0.0203 - val_loss: 0.3721 - val_accuracy: 0.8580 - val_mse: 0.0711\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1205 - accuracy: 0.9644 - mse: 0.0215 - val_loss: 0.3766 - val_accuracy: 0.8523 - val_mse: 0.0724\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1090 - accuracy: 0.9758 - mse: 0.0169 - val_loss: 0.3792 - val_accuracy: 0.8523 - val_mse: 0.0734\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1093 - accuracy: 0.9744 - mse: 0.0173 - val_loss: 0.3802 - val_accuracy: 0.8523 - val_mse: 0.0737\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1323 - accuracy: 0.9530 - mse: 0.0222 - val_loss: 0.3742 - val_accuracy: 0.8523 - val_mse: 0.0722\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1176 - accuracy: 0.9630 - mse: 0.0194 - val_loss: 0.3627 - val_accuracy: 0.8580 - val_mse: 0.0692\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1158 - accuracy: 0.9644 - mse: 0.0190 - val_loss: 0.3515 - val_accuracy: 0.8693 - val_mse: 0.0663\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1174 - accuracy: 0.9658 - mse: 0.0193 - val_loss: 0.3443 - val_accuracy: 0.8750 - val_mse: 0.0645\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1265 - accuracy: 0.9658 - mse: 0.0210 - val_loss: 0.3433 - val_accuracy: 0.8750 - val_mse: 0.0643\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.0988 - accuracy: 0.9715 - mse: 0.0163 - val_loss: 0.3463 - val_accuracy: 0.8693 - val_mse: 0.0654\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 12ms/step - loss: 0.1138 - accuracy: 0.9630 - mse: 0.0185 - val_loss: 0.3470 - val_accuracy: 0.8636 - val_mse: 0.0657\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1053 - accuracy: 0.9644 - mse: 0.0186 - val_loss: 0.3476 - val_accuracy: 0.8636 - val_mse: 0.0660\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1215 - accuracy: 0.9587 - mse: 0.0198 - val_loss: 0.3475 - val_accuracy: 0.8636 - val_mse: 0.0661\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1087 - accuracy: 0.9630 - mse: 0.0187 - val_loss: 0.3460 - val_accuracy: 0.8693 - val_mse: 0.0659\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1100 - accuracy: 0.9658 - mse: 0.0183 - val_loss: 0.3447 - val_accuracy: 0.8636 - val_mse: 0.0659\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.1043 - accuracy: 0.9729 - mse: 0.0173 - val_loss: 0.3461 - val_accuracy: 0.8636 - val_mse: 0.0663\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1129 - accuracy: 0.9701 - mse: 0.0182 - val_loss: 0.3437 - val_accuracy: 0.8693 - val_mse: 0.0656\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1012 - accuracy: 0.9715 - mse: 0.0167 - val_loss: 0.3409 - val_accuracy: 0.8693 - val_mse: 0.0649\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1010 - accuracy: 0.9729 - mse: 0.0162 - val_loss: 0.3259 - val_accuracy: 0.8693 - val_mse: 0.0609\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0992 - accuracy: 0.9801 - mse: 0.0148 - val_loss: 0.3134 - val_accuracy: 0.8807 - val_mse: 0.0578\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1234 - accuracy: 0.9530 - mse: 0.0222 - val_loss: 0.3063 - val_accuracy: 0.8864 - val_mse: 0.0559\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1124 - accuracy: 0.9672 - mse: 0.0195 - val_loss: 0.3038 - val_accuracy: 0.8864 - val_mse: 0.0553\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1024 - accuracy: 0.9744 - mse: 0.0157 - val_loss: 0.3038 - val_accuracy: 0.8864 - val_mse: 0.0554\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0944 - accuracy: 0.9729 - mse: 0.0159 - val_loss: 0.3008 - val_accuracy: 0.8920 - val_mse: 0.0549\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0997 - accuracy: 0.9744 - mse: 0.0156 - val_loss: 0.2953 - val_accuracy: 0.8977 - val_mse: 0.0536\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.1039 - accuracy: 0.9687 - mse: 0.0168 - val_loss: 0.2937 - val_accuracy: 0.8977 - val_mse: 0.0533\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0967 - accuracy: 0.9715 - mse: 0.0150 - val_loss: 0.2903 - val_accuracy: 0.8977 - val_mse: 0.0525\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0992 - accuracy: 0.9701 - mse: 0.0171 - val_loss: 0.2907 - val_accuracy: 0.8977 - val_mse: 0.0525\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0908 - accuracy: 0.9744 - mse: 0.0154 - val_loss: 0.2975 - val_accuracy: 0.8864 - val_mse: 0.0541\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1045 - accuracy: 0.9672 - mse: 0.0174 - val_loss: 0.3042 - val_accuracy: 0.8807 - val_mse: 0.0558\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0844 - accuracy: 0.9758 - mse: 0.0138 - val_loss: 0.3025 - val_accuracy: 0.8807 - val_mse: 0.0554\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0957 - accuracy: 0.9744 - mse: 0.0151 - val_loss: 0.2936 - val_accuracy: 0.8920 - val_mse: 0.0532\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0913 - accuracy: 0.9715 - mse: 0.0151 - val_loss: 0.2887 - val_accuracy: 0.8977 - val_mse: 0.0520\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.1015 - accuracy: 0.9672 - mse: 0.0174 - val_loss: 0.2820 - val_accuracy: 0.8977 - val_mse: 0.0506\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0873 - accuracy: 0.9729 - mse: 0.0141 - val_loss: 0.2754 - val_accuracy: 0.8977 - val_mse: 0.0492\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1003 - accuracy: 0.9701 - mse: 0.0173 - val_loss: 0.2762 - val_accuracy: 0.8920 - val_mse: 0.0496\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1106 - accuracy: 0.9658 - mse: 0.0185 - val_loss: 0.2752 - val_accuracy: 0.8977 - val_mse: 0.0495\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0902 - accuracy: 0.9772 - mse: 0.0144 - val_loss: 0.2821 - val_accuracy: 0.8920 - val_mse: 0.0514\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0876 - accuracy: 0.9729 - mse: 0.0147 - val_loss: 0.2801 - val_accuracy: 0.8977 - val_mse: 0.0510\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.0964 - accuracy: 0.9644 - mse: 0.0172 - val_loss: 0.2763 - val_accuracy: 0.8977 - val_mse: 0.0499\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.1081 - accuracy: 0.9672 - mse: 0.0173 - val_loss: 0.2709 - val_accuracy: 0.9091 - val_mse: 0.0484\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0843 - accuracy: 0.9772 - mse: 0.0142 - val_loss: 0.2716 - val_accuracy: 0.9091 - val_mse: 0.0485\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0776 - accuracy: 0.9872 - mse: 0.0116 - val_loss: 0.2686 - val_accuracy: 0.9148 - val_mse: 0.0477\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0859 - accuracy: 0.9772 - mse: 0.0137 - val_loss: 0.2570 - val_accuracy: 0.9148 - val_mse: 0.0448\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0756 - accuracy: 0.9829 - mse: 0.0114 - val_loss: 0.2493 - val_accuracy: 0.9148 - val_mse: 0.0431\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0802 - accuracy: 0.9758 - mse: 0.0132 - val_loss: 0.2383 - val_accuracy: 0.9261 - val_mse: 0.0407\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.0821 - accuracy: 0.9786 - mse: 0.0139 - val_loss: 0.2334 - val_accuracy: 0.9318 - val_mse: 0.0397\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.0954 - accuracy: 0.9615 - mse: 0.0172 - val_loss: 0.2326 - val_accuracy: 0.9261 - val_mse: 0.0397\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.0895 - accuracy: 0.9729 - mse: 0.0149 - val_loss: 0.2350 - val_accuracy: 0.9205 - val_mse: 0.0403\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0907 - accuracy: 0.9729 - mse: 0.0152 - val_loss: 0.2423 - val_accuracy: 0.9205 - val_mse: 0.0420\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.0887 - accuracy: 0.9715 - mse: 0.0149 - val_loss: 0.2499 - val_accuracy: 0.9205 - val_mse: 0.0437\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0822 - accuracy: 0.9715 - mse: 0.0143 - val_loss: 0.2480 - val_accuracy: 0.9205 - val_mse: 0.0433\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0886 - accuracy: 0.9729 - mse: 0.0143 - val_loss: 0.2393 - val_accuracy: 0.9318 - val_mse: 0.0409\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 14ms/step - loss: 0.0821 - accuracy: 0.9758 - mse: 0.0133 - val_loss: 0.2432 - val_accuracy: 0.9205 - val_mse: 0.0419\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0824 - accuracy: 0.9786 - mse: 0.0133 - val_loss: 0.2406 - val_accuracy: 0.9205 - val_mse: 0.0413\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 0.0821 - accuracy: 0.9744 - mse: 0.0137 - val_loss: 0.2445 - val_accuracy: 0.9205 - val_mse: 0.0422\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9787647041398981"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "90wNiZXcNKLZ",
        "outputId": "ebe38522-9faa-441f-bd21-11bc952eece1"
      },
      "source": [
        "Xo, Yo = over_sample(fn.X, fn.Y,'RandomOverSampler')\n",
        "borderline_model = train_model(Xo, Yo,200)\n",
        "output = borderline_model.predict_classes(np.array(fn.X))\n",
        "f1_score(np.argmax(fn.Y,1), output,labels=[0,1,2,3],average='weighted')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4/4 [==============================] - 1s 56ms/step - loss: 1.1460 - accuracy: 0.3935 - mse: 0.2310 - val_loss: 1.5492 - val_accuracy: 0.0000e+00 - val_mse: 0.3149\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 1.1003 - accuracy: 0.4077 - mse: 0.2227 - val_loss: 1.5107 - val_accuracy: 0.0000e+00 - val_mse: 0.3092\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.0880 - accuracy: 0.4194 - mse: 0.2200 - val_loss: 1.4560 - val_accuracy: 0.0000e+00 - val_mse: 0.3005\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0386 - accuracy: 0.4955 - mse: 0.2099 - val_loss: 1.4068 - val_accuracy: 0.0000e+00 - val_mse: 0.2924\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.0133 - accuracy: 0.5123 - mse: 0.2041 - val_loss: 1.3513 - val_accuracy: 0.0000e+00 - val_mse: 0.2822\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9868 - accuracy: 0.4929 - mse: 0.1993 - val_loss: 1.3085 - val_accuracy: 0.0000e+00 - val_mse: 0.2736\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9710 - accuracy: 0.5303 - mse: 0.1957 - val_loss: 1.2640 - val_accuracy: 0.0670 - val_mse: 0.2641\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9517 - accuracy: 0.5406 - mse: 0.1920 - val_loss: 1.2210 - val_accuracy: 0.1289 - val_mse: 0.2548\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9278 - accuracy: 0.5587 - mse: 0.1871 - val_loss: 1.1965 - val_accuracy: 0.1289 - val_mse: 0.2488\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.9124 - accuracy: 0.5729 - mse: 0.1839 - val_loss: 1.1764 - val_accuracy: 0.1289 - val_mse: 0.2438\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8948 - accuracy: 0.5845 - mse: 0.1807 - val_loss: 1.1582 - val_accuracy: 0.2268 - val_mse: 0.2390\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8871 - accuracy: 0.5819 - mse: 0.1793 - val_loss: 1.1402 - val_accuracy: 0.2268 - val_mse: 0.2341\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8593 - accuracy: 0.6039 - mse: 0.1728 - val_loss: 1.1411 - val_accuracy: 0.2268 - val_mse: 0.2338\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8502 - accuracy: 0.6129 - mse: 0.1719 - val_loss: 1.1469 - val_accuracy: 0.2268 - val_mse: 0.2352\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8498 - accuracy: 0.6013 - mse: 0.1725 - val_loss: 1.1369 - val_accuracy: 0.2268 - val_mse: 0.2332\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8276 - accuracy: 0.6103 - mse: 0.1675 - val_loss: 1.1344 - val_accuracy: 0.2268 - val_mse: 0.2325\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8343 - accuracy: 0.6116 - mse: 0.1693 - val_loss: 1.1246 - val_accuracy: 0.2268 - val_mse: 0.2305\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7944 - accuracy: 0.6529 - mse: 0.1599 - val_loss: 1.1011 - val_accuracy: 0.2268 - val_mse: 0.2254\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7856 - accuracy: 0.6542 - mse: 0.1586 - val_loss: 1.0695 - val_accuracy: 0.2268 - val_mse: 0.2182\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7772 - accuracy: 0.6465 - mse: 0.1576 - val_loss: 1.0295 - val_accuracy: 0.3196 - val_mse: 0.2092\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.7660 - accuracy: 0.6348 - mse: 0.1551 - val_loss: 0.9983 - val_accuracy: 0.3196 - val_mse: 0.2030\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7527 - accuracy: 0.6581 - mse: 0.1522 - val_loss: 0.9703 - val_accuracy: 0.4227 - val_mse: 0.1975\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7542 - accuracy: 0.6645 - mse: 0.1533 - val_loss: 0.9572 - val_accuracy: 0.4227 - val_mse: 0.1955\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7244 - accuracy: 0.6774 - mse: 0.1470 - val_loss: 0.9624 - val_accuracy: 0.4227 - val_mse: 0.1976\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7238 - accuracy: 0.6748 - mse: 0.1471 - val_loss: 0.9613 - val_accuracy: 0.4227 - val_mse: 0.1976\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6970 - accuracy: 0.6890 - mse: 0.1410 - val_loss: 0.9443 - val_accuracy: 0.4227 - val_mse: 0.1941\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6967 - accuracy: 0.6929 - mse: 0.1396 - val_loss: 0.9280 - val_accuracy: 0.4227 - val_mse: 0.1914\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.6971 - accuracy: 0.6774 - mse: 0.1411 - val_loss: 0.9188 - val_accuracy: 0.4227 - val_mse: 0.1904\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6746 - accuracy: 0.6955 - mse: 0.1375 - val_loss: 0.8888 - val_accuracy: 0.4227 - val_mse: 0.1844\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6663 - accuracy: 0.7265 - mse: 0.1344 - val_loss: 0.8421 - val_accuracy: 0.5052 - val_mse: 0.1737\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6618 - accuracy: 0.7226 - mse: 0.1340 - val_loss: 0.8115 - val_accuracy: 0.5052 - val_mse: 0.1671\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6470 - accuracy: 0.7187 - mse: 0.1309 - val_loss: 0.7919 - val_accuracy: 0.5052 - val_mse: 0.1626\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.6457 - accuracy: 0.7239 - mse: 0.1299 - val_loss: 0.7739 - val_accuracy: 0.5052 - val_mse: 0.1587\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6326 - accuracy: 0.7290 - mse: 0.1274 - val_loss: 0.7506 - val_accuracy: 0.5876 - val_mse: 0.1541\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6232 - accuracy: 0.7355 - mse: 0.1254 - val_loss: 0.7184 - val_accuracy: 0.6856 - val_mse: 0.1474\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6141 - accuracy: 0.7303 - mse: 0.1247 - val_loss: 0.6943 - val_accuracy: 0.6856 - val_mse: 0.1429\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6092 - accuracy: 0.7484 - mse: 0.1233 - val_loss: 0.6935 - val_accuracy: 0.6856 - val_mse: 0.1432\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5975 - accuracy: 0.7290 - mse: 0.1215 - val_loss: 0.7132 - val_accuracy: 0.6856 - val_mse: 0.1489\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5901 - accuracy: 0.7445 - mse: 0.1194 - val_loss: 0.7067 - val_accuracy: 0.6856 - val_mse: 0.1475\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6129 - accuracy: 0.7277 - mse: 0.1252 - val_loss: 0.6717 - val_accuracy: 0.6856 - val_mse: 0.1391\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5931 - accuracy: 0.7406 - mse: 0.1202 - val_loss: 0.6424 - val_accuracy: 0.6856 - val_mse: 0.1324\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5837 - accuracy: 0.7652 - mse: 0.1176 - val_loss: 0.6349 - val_accuracy: 0.6856 - val_mse: 0.1309\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5700 - accuracy: 0.7690 - mse: 0.1151 - val_loss: 0.6338 - val_accuracy: 0.6856 - val_mse: 0.1307\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.5453 - accuracy: 0.7768 - mse: 0.1088 - val_loss: 0.6370 - val_accuracy: 0.6856 - val_mse: 0.1317\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5357 - accuracy: 0.7935 - mse: 0.1065 - val_loss: 0.6313 - val_accuracy: 0.6856 - val_mse: 0.1300\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5379 - accuracy: 0.7845 - mse: 0.1073 - val_loss: 0.6220 - val_accuracy: 0.6856 - val_mse: 0.1282\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5350 - accuracy: 0.7587 - mse: 0.1083 - val_loss: 0.5938 - val_accuracy: 0.6856 - val_mse: 0.1215\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5238 - accuracy: 0.7948 - mse: 0.1048 - val_loss: 0.5832 - val_accuracy: 0.6856 - val_mse: 0.1198\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5213 - accuracy: 0.7897 - mse: 0.1053 - val_loss: 0.5715 - val_accuracy: 0.7835 - val_mse: 0.1172\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5107 - accuracy: 0.7935 - mse: 0.1021 - val_loss: 0.5522 - val_accuracy: 0.7835 - val_mse: 0.1124\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5338 - accuracy: 0.7652 - mse: 0.1082 - val_loss: 0.5471 - val_accuracy: 0.7835 - val_mse: 0.1111\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5217 - accuracy: 0.7665 - mse: 0.1046 - val_loss: 0.5566 - val_accuracy: 0.7835 - val_mse: 0.1138\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4822 - accuracy: 0.8194 - mse: 0.0959 - val_loss: 0.5727 - val_accuracy: 0.7835 - val_mse: 0.1182\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4843 - accuracy: 0.8052 - mse: 0.0959 - val_loss: 0.5822 - val_accuracy: 0.7835 - val_mse: 0.1216\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.4737 - accuracy: 0.8232 - mse: 0.0935 - val_loss: 0.5563 - val_accuracy: 0.7835 - val_mse: 0.1147\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4758 - accuracy: 0.8129 - mse: 0.0951 - val_loss: 0.5389 - val_accuracy: 0.7835 - val_mse: 0.1103\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4510 - accuracy: 0.8335 - mse: 0.0890 - val_loss: 0.5196 - val_accuracy: 0.7835 - val_mse: 0.1047\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4577 - accuracy: 0.8206 - mse: 0.0903 - val_loss: 0.5062 - val_accuracy: 0.7835 - val_mse: 0.1008\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4580 - accuracy: 0.8348 - mse: 0.0900 - val_loss: 0.4952 - val_accuracy: 0.7835 - val_mse: 0.0980\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4626 - accuracy: 0.8271 - mse: 0.0919 - val_loss: 0.4766 - val_accuracy: 0.9072 - val_mse: 0.0933\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4624 - accuracy: 0.8297 - mse: 0.0925 - val_loss: 0.4585 - val_accuracy: 0.9072 - val_mse: 0.0884\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4525 - accuracy: 0.8400 - mse: 0.0899 - val_loss: 0.4451 - val_accuracy: 1.0000 - val_mse: 0.0845\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4526 - accuracy: 0.8310 - mse: 0.0890 - val_loss: 0.4309 - val_accuracy: 1.0000 - val_mse: 0.0801\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4398 - accuracy: 0.8361 - mse: 0.0860 - val_loss: 0.4387 - val_accuracy: 1.0000 - val_mse: 0.0823\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4410 - accuracy: 0.8374 - mse: 0.0868 - val_loss: 0.4529 - val_accuracy: 1.0000 - val_mse: 0.0865\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4354 - accuracy: 0.8219 - mse: 0.0865 - val_loss: 0.4627 - val_accuracy: 1.0000 - val_mse: 0.0896\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4338 - accuracy: 0.8361 - mse: 0.0852 - val_loss: 0.4634 - val_accuracy: 1.0000 - val_mse: 0.0899\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4278 - accuracy: 0.8297 - mse: 0.0847 - val_loss: 0.4547 - val_accuracy: 1.0000 - val_mse: 0.0877\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.4151 - accuracy: 0.8568 - mse: 0.0806 - val_loss: 0.4384 - val_accuracy: 1.0000 - val_mse: 0.0836\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4218 - accuracy: 0.8310 - mse: 0.0829 - val_loss: 0.4266 - val_accuracy: 1.0000 - val_mse: 0.0806\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4173 - accuracy: 0.8400 - mse: 0.0815 - val_loss: 0.4148 - val_accuracy: 1.0000 - val_mse: 0.0774\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4041 - accuracy: 0.8361 - mse: 0.0793 - val_loss: 0.3907 - val_accuracy: 1.0000 - val_mse: 0.0705\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3912 - accuracy: 0.8594 - mse: 0.0765 - val_loss: 0.3787 - val_accuracy: 1.0000 - val_mse: 0.0670\n",
            "Epoch 74/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.4007 - accuracy: 0.8555 - mse: 0.0787 - val_loss: 0.3787 - val_accuracy: 1.0000 - val_mse: 0.0670\n",
            "Epoch 75/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3726 - accuracy: 0.8606 - mse: 0.0711 - val_loss: 0.3833 - val_accuracy: 1.0000 - val_mse: 0.0684\n",
            "Epoch 76/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3929 - accuracy: 0.8645 - mse: 0.0755 - val_loss: 0.3899 - val_accuracy: 1.0000 - val_mse: 0.0709\n",
            "Epoch 77/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3833 - accuracy: 0.8606 - mse: 0.0746 - val_loss: 0.4016 - val_accuracy: 0.9072 - val_mse: 0.0744\n",
            "Epoch 78/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4043 - accuracy: 0.8348 - mse: 0.0806 - val_loss: 0.4102 - val_accuracy: 0.9072 - val_mse: 0.0766\n",
            "Epoch 79/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3757 - accuracy: 0.8581 - mse: 0.0732 - val_loss: 0.4068 - val_accuracy: 0.9072 - val_mse: 0.0757\n",
            "Epoch 80/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3646 - accuracy: 0.8748 - mse: 0.0698 - val_loss: 0.3892 - val_accuracy: 0.9072 - val_mse: 0.0713\n",
            "Epoch 81/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3670 - accuracy: 0.8723 - mse: 0.0709 - val_loss: 0.3576 - val_accuracy: 1.0000 - val_mse: 0.0627\n",
            "Epoch 82/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3734 - accuracy: 0.8632 - mse: 0.0713 - val_loss: 0.3339 - val_accuracy: 1.0000 - val_mse: 0.0560\n",
            "Epoch 83/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3765 - accuracy: 0.8697 - mse: 0.0724 - val_loss: 0.3287 - val_accuracy: 1.0000 - val_mse: 0.0545\n",
            "Epoch 84/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3603 - accuracy: 0.8684 - mse: 0.0687 - val_loss: 0.3190 - val_accuracy: 1.0000 - val_mse: 0.0523\n",
            "Epoch 85/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3474 - accuracy: 0.8813 - mse: 0.0653 - val_loss: 0.2990 - val_accuracy: 1.0000 - val_mse: 0.0475\n",
            "Epoch 86/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3414 - accuracy: 0.8774 - mse: 0.0650 - val_loss: 0.2931 - val_accuracy: 1.0000 - val_mse: 0.0463\n",
            "Epoch 87/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3423 - accuracy: 0.8903 - mse: 0.0642 - val_loss: 0.2961 - val_accuracy: 1.0000 - val_mse: 0.0473\n",
            "Epoch 88/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3367 - accuracy: 0.8787 - mse: 0.0637 - val_loss: 0.2883 - val_accuracy: 1.0000 - val_mse: 0.0455\n",
            "Epoch 89/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3538 - accuracy: 0.8735 - mse: 0.0668 - val_loss: 0.2850 - val_accuracy: 1.0000 - val_mse: 0.0447\n",
            "Epoch 90/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3289 - accuracy: 0.8839 - mse: 0.0619 - val_loss: 0.2858 - val_accuracy: 1.0000 - val_mse: 0.0450\n",
            "Epoch 91/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3293 - accuracy: 0.8890 - mse: 0.0622 - val_loss: 0.2821 - val_accuracy: 1.0000 - val_mse: 0.0441\n",
            "Epoch 92/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3066 - accuracy: 0.9148 - mse: 0.0562 - val_loss: 0.2753 - val_accuracy: 1.0000 - val_mse: 0.0426\n",
            "Epoch 93/200\n",
            "4/4 [==============================] - 0s 13ms/step - loss: 0.3070 - accuracy: 0.8968 - mse: 0.0572 - val_loss: 0.2726 - val_accuracy: 1.0000 - val_mse: 0.0422\n",
            "Epoch 94/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3213 - accuracy: 0.8942 - mse: 0.0607 - val_loss: 0.2778 - val_accuracy: 1.0000 - val_mse: 0.0437\n",
            "Epoch 95/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3442 - accuracy: 0.8839 - mse: 0.0659 - val_loss: 0.2942 - val_accuracy: 1.0000 - val_mse: 0.0482\n",
            "Epoch 96/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3068 - accuracy: 0.9045 - mse: 0.0570 - val_loss: 0.3165 - val_accuracy: 1.0000 - val_mse: 0.0540\n",
            "Epoch 97/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2870 - accuracy: 0.9187 - mse: 0.0524 - val_loss: 0.3178 - val_accuracy: 1.0000 - val_mse: 0.0544\n",
            "Epoch 98/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3103 - accuracy: 0.9045 - mse: 0.0573 - val_loss: 0.3078 - val_accuracy: 1.0000 - val_mse: 0.0518\n",
            "Epoch 99/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2914 - accuracy: 0.9097 - mse: 0.0540 - val_loss: 0.3020 - val_accuracy: 1.0000 - val_mse: 0.0505\n",
            "Epoch 100/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2842 - accuracy: 0.9148 - mse: 0.0519 - val_loss: 0.2947 - val_accuracy: 1.0000 - val_mse: 0.0490\n",
            "Epoch 101/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2847 - accuracy: 0.9019 - mse: 0.0527 - val_loss: 0.2826 - val_accuracy: 1.0000 - val_mse: 0.0461\n",
            "Epoch 102/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3070 - accuracy: 0.9006 - mse: 0.0582 - val_loss: 0.2683 - val_accuracy: 1.0000 - val_mse: 0.0426\n",
            "Epoch 103/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2803 - accuracy: 0.9200 - mse: 0.0517 - val_loss: 0.2569 - val_accuracy: 1.0000 - val_mse: 0.0398\n",
            "Epoch 104/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2801 - accuracy: 0.9097 - mse: 0.0510 - val_loss: 0.2397 - val_accuracy: 1.0000 - val_mse: 0.0355\n",
            "Epoch 105/200\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2794 - accuracy: 0.9097 - mse: 0.0516 - val_loss: 0.2328 - val_accuracy: 1.0000 - val_mse: 0.0338\n",
            "Epoch 106/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2693 - accuracy: 0.9200 - mse: 0.0482 - val_loss: 0.2389 - val_accuracy: 1.0000 - val_mse: 0.0353\n",
            "Epoch 107/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2642 - accuracy: 0.9277 - mse: 0.0474 - val_loss: 0.2425 - val_accuracy: 1.0000 - val_mse: 0.0364\n",
            "Epoch 108/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2755 - accuracy: 0.9097 - mse: 0.0502 - val_loss: 0.2441 - val_accuracy: 1.0000 - val_mse: 0.0370\n",
            "Epoch 109/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2580 - accuracy: 0.9252 - mse: 0.0462 - val_loss: 0.2418 - val_accuracy: 1.0000 - val_mse: 0.0365\n",
            "Epoch 110/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2739 - accuracy: 0.9213 - mse: 0.0495 - val_loss: 0.2400 - val_accuracy: 1.0000 - val_mse: 0.0362\n",
            "Epoch 111/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2562 - accuracy: 0.9303 - mse: 0.0468 - val_loss: 0.2366 - val_accuracy: 1.0000 - val_mse: 0.0356\n",
            "Epoch 112/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2645 - accuracy: 0.9110 - mse: 0.0486 - val_loss: 0.2365 - val_accuracy: 1.0000 - val_mse: 0.0359\n",
            "Epoch 113/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2571 - accuracy: 0.9148 - mse: 0.0466 - val_loss: 0.2451 - val_accuracy: 1.0000 - val_mse: 0.0381\n",
            "Epoch 114/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2651 - accuracy: 0.9135 - mse: 0.0487 - val_loss: 0.2391 - val_accuracy: 1.0000 - val_mse: 0.0366\n",
            "Epoch 115/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2477 - accuracy: 0.9239 - mse: 0.0444 - val_loss: 0.2230 - val_accuracy: 1.0000 - val_mse: 0.0327\n",
            "Epoch 116/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2637 - accuracy: 0.9277 - mse: 0.0472 - val_loss: 0.2138 - val_accuracy: 1.0000 - val_mse: 0.0304\n",
            "Epoch 117/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2394 - accuracy: 0.9329 - mse: 0.0430 - val_loss: 0.2071 - val_accuracy: 1.0000 - val_mse: 0.0289\n",
            "Epoch 118/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2347 - accuracy: 0.9252 - mse: 0.0414 - val_loss: 0.2050 - val_accuracy: 1.0000 - val_mse: 0.0285\n",
            "Epoch 119/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2464 - accuracy: 0.9226 - mse: 0.0443 - val_loss: 0.1989 - val_accuracy: 1.0000 - val_mse: 0.0271\n",
            "Epoch 120/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2371 - accuracy: 0.9252 - mse: 0.0425 - val_loss: 0.1924 - val_accuracy: 1.0000 - val_mse: 0.0255\n",
            "Epoch 121/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2356 - accuracy: 0.9355 - mse: 0.0415 - val_loss: 0.1930 - val_accuracy: 1.0000 - val_mse: 0.0256\n",
            "Epoch 122/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2311 - accuracy: 0.9303 - mse: 0.0406 - val_loss: 0.1909 - val_accuracy: 1.0000 - val_mse: 0.0250\n",
            "Epoch 123/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2192 - accuracy: 0.9394 - mse: 0.0386 - val_loss: 0.1889 - val_accuracy: 1.0000 - val_mse: 0.0242\n",
            "Epoch 124/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2230 - accuracy: 0.9316 - mse: 0.0392 - val_loss: 0.1840 - val_accuracy: 1.0000 - val_mse: 0.0230\n",
            "Epoch 125/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2188 - accuracy: 0.9355 - mse: 0.0378 - val_loss: 0.1789 - val_accuracy: 1.0000 - val_mse: 0.0221\n",
            "Epoch 126/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2176 - accuracy: 0.9406 - mse: 0.0376 - val_loss: 0.1769 - val_accuracy: 1.0000 - val_mse: 0.0219\n",
            "Epoch 127/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2078 - accuracy: 0.9471 - mse: 0.0359 - val_loss: 0.1761 - val_accuracy: 1.0000 - val_mse: 0.0220\n",
            "Epoch 128/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2138 - accuracy: 0.9484 - mse: 0.0362 - val_loss: 0.1719 - val_accuracy: 1.0000 - val_mse: 0.0212\n",
            "Epoch 129/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2116 - accuracy: 0.9394 - mse: 0.0364 - val_loss: 0.1706 - val_accuracy: 1.0000 - val_mse: 0.0208\n",
            "Epoch 130/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1981 - accuracy: 0.9458 - mse: 0.0343 - val_loss: 0.1679 - val_accuracy: 1.0000 - val_mse: 0.0203\n",
            "Epoch 131/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2121 - accuracy: 0.9445 - mse: 0.0361 - val_loss: 0.1609 - val_accuracy: 1.0000 - val_mse: 0.0187\n",
            "Epoch 132/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2182 - accuracy: 0.9381 - mse: 0.0372 - val_loss: 0.1571 - val_accuracy: 1.0000 - val_mse: 0.0178\n",
            "Epoch 133/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2130 - accuracy: 0.9406 - mse: 0.0364 - val_loss: 0.1545 - val_accuracy: 1.0000 - val_mse: 0.0172\n",
            "Epoch 134/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2046 - accuracy: 0.9355 - mse: 0.0357 - val_loss: 0.1479 - val_accuracy: 1.0000 - val_mse: 0.0159\n",
            "Epoch 135/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2006 - accuracy: 0.9432 - mse: 0.0345 - val_loss: 0.1471 - val_accuracy: 1.0000 - val_mse: 0.0159\n",
            "Epoch 136/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1864 - accuracy: 0.9471 - mse: 0.0318 - val_loss: 0.1515 - val_accuracy: 1.0000 - val_mse: 0.0168\n",
            "Epoch 137/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1998 - accuracy: 0.9523 - mse: 0.0343 - val_loss: 0.1517 - val_accuracy: 1.0000 - val_mse: 0.0168\n",
            "Epoch 138/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1831 - accuracy: 0.9510 - mse: 0.0310 - val_loss: 0.1475 - val_accuracy: 1.0000 - val_mse: 0.0159\n",
            "Epoch 139/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1909 - accuracy: 0.9523 - mse: 0.0321 - val_loss: 0.1441 - val_accuracy: 1.0000 - val_mse: 0.0153\n",
            "Epoch 140/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1846 - accuracy: 0.9626 - mse: 0.0307 - val_loss: 0.1335 - val_accuracy: 1.0000 - val_mse: 0.0133\n",
            "Epoch 141/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1937 - accuracy: 0.9445 - mse: 0.0336 - val_loss: 0.1295 - val_accuracy: 1.0000 - val_mse: 0.0127\n",
            "Epoch 142/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1991 - accuracy: 0.9342 - mse: 0.0352 - val_loss: 0.1301 - val_accuracy: 1.0000 - val_mse: 0.0130\n",
            "Epoch 143/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1910 - accuracy: 0.9445 - mse: 0.0330 - val_loss: 0.1377 - val_accuracy: 1.0000 - val_mse: 0.0144\n",
            "Epoch 144/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1825 - accuracy: 0.9497 - mse: 0.0308 - val_loss: 0.1458 - val_accuracy: 1.0000 - val_mse: 0.0158\n",
            "Epoch 145/200\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1760 - accuracy: 0.9548 - mse: 0.0285 - val_loss: 0.1439 - val_accuracy: 1.0000 - val_mse: 0.0153\n",
            "Epoch 146/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1794 - accuracy: 0.9523 - mse: 0.0306 - val_loss: 0.1356 - val_accuracy: 1.0000 - val_mse: 0.0137\n",
            "Epoch 147/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1818 - accuracy: 0.9523 - mse: 0.0305 - val_loss: 0.1243 - val_accuracy: 1.0000 - val_mse: 0.0117\n",
            "Epoch 148/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1806 - accuracy: 0.9484 - mse: 0.0309 - val_loss: 0.1187 - val_accuracy: 1.0000 - val_mse: 0.0109\n",
            "Epoch 149/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1660 - accuracy: 0.9574 - mse: 0.0271 - val_loss: 0.1247 - val_accuracy: 1.0000 - val_mse: 0.0119\n",
            "Epoch 150/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1786 - accuracy: 0.9484 - mse: 0.0303 - val_loss: 0.1285 - val_accuracy: 1.0000 - val_mse: 0.0128\n",
            "Epoch 151/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1858 - accuracy: 0.9497 - mse: 0.0322 - val_loss: 0.1260 - val_accuracy: 1.0000 - val_mse: 0.0125\n",
            "Epoch 152/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1628 - accuracy: 0.9561 - mse: 0.0265 - val_loss: 0.1193 - val_accuracy: 1.0000 - val_mse: 0.0115\n",
            "Epoch 153/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1735 - accuracy: 0.9574 - mse: 0.0284 - val_loss: 0.1147 - val_accuracy: 1.0000 - val_mse: 0.0108\n",
            "Epoch 154/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1700 - accuracy: 0.9458 - mse: 0.0287 - val_loss: 0.1176 - val_accuracy: 1.0000 - val_mse: 0.0111\n",
            "Epoch 155/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1765 - accuracy: 0.9587 - mse: 0.0297 - val_loss: 0.1290 - val_accuracy: 1.0000 - val_mse: 0.0131\n",
            "Epoch 156/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1613 - accuracy: 0.9561 - mse: 0.0270 - val_loss: 0.1396 - val_accuracy: 1.0000 - val_mse: 0.0152\n",
            "Epoch 157/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1480 - accuracy: 0.9613 - mse: 0.0240 - val_loss: 0.1414 - val_accuracy: 1.0000 - val_mse: 0.0158\n",
            "Epoch 158/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1486 - accuracy: 0.9639 - mse: 0.0235 - val_loss: 0.1316 - val_accuracy: 1.0000 - val_mse: 0.0142\n",
            "Epoch 159/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1671 - accuracy: 0.9497 - mse: 0.0285 - val_loss: 0.1196 - val_accuracy: 1.0000 - val_mse: 0.0122\n",
            "Epoch 160/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1548 - accuracy: 0.9535 - mse: 0.0259 - val_loss: 0.1109 - val_accuracy: 1.0000 - val_mse: 0.0107\n",
            "Epoch 161/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1657 - accuracy: 0.9445 - mse: 0.0286 - val_loss: 0.1035 - val_accuracy: 1.0000 - val_mse: 0.0095\n",
            "Epoch 162/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1439 - accuracy: 0.9716 - mse: 0.0229 - val_loss: 0.1026 - val_accuracy: 1.0000 - val_mse: 0.0093\n",
            "Epoch 163/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1518 - accuracy: 0.9548 - mse: 0.0256 - val_loss: 0.1044 - val_accuracy: 1.0000 - val_mse: 0.0094\n",
            "Epoch 164/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1450 - accuracy: 0.9677 - mse: 0.0231 - val_loss: 0.1043 - val_accuracy: 1.0000 - val_mse: 0.0092\n",
            "Epoch 165/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1475 - accuracy: 0.9587 - mse: 0.0240 - val_loss: 0.1018 - val_accuracy: 1.0000 - val_mse: 0.0088\n",
            "Epoch 166/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1604 - accuracy: 0.9535 - mse: 0.0273 - val_loss: 0.0973 - val_accuracy: 1.0000 - val_mse: 0.0083\n",
            "Epoch 167/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1356 - accuracy: 0.9665 - mse: 0.0215 - val_loss: 0.0982 - val_accuracy: 1.0000 - val_mse: 0.0087\n",
            "Epoch 168/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1410 - accuracy: 0.9652 - mse: 0.0224 - val_loss: 0.0985 - val_accuracy: 1.0000 - val_mse: 0.0088\n",
            "Epoch 169/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1563 - accuracy: 0.9548 - mse: 0.0257 - val_loss: 0.0994 - val_accuracy: 1.0000 - val_mse: 0.0090\n",
            "Epoch 170/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1435 - accuracy: 0.9574 - mse: 0.0236 - val_loss: 0.0962 - val_accuracy: 1.0000 - val_mse: 0.0086\n",
            "Epoch 171/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1524 - accuracy: 0.9535 - mse: 0.0250 - val_loss: 0.0944 - val_accuracy: 1.0000 - val_mse: 0.0083\n",
            "Epoch 172/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1290 - accuracy: 0.9613 - mse: 0.0210 - val_loss: 0.0943 - val_accuracy: 1.0000 - val_mse: 0.0083\n",
            "Epoch 173/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1348 - accuracy: 0.9677 - mse: 0.0213 - val_loss: 0.0955 - val_accuracy: 1.0000 - val_mse: 0.0084\n",
            "Epoch 174/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1373 - accuracy: 0.9652 - mse: 0.0218 - val_loss: 0.0914 - val_accuracy: 1.0000 - val_mse: 0.0077\n",
            "Epoch 175/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1389 - accuracy: 0.9600 - mse: 0.0232 - val_loss: 0.0855 - val_accuracy: 1.0000 - val_mse: 0.0068\n",
            "Epoch 176/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1484 - accuracy: 0.9600 - mse: 0.0248 - val_loss: 0.0802 - val_accuracy: 1.0000 - val_mse: 0.0060\n",
            "Epoch 177/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1316 - accuracy: 0.9652 - mse: 0.0207 - val_loss: 0.0800 - val_accuracy: 1.0000 - val_mse: 0.0059\n",
            "Epoch 178/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1500 - accuracy: 0.9548 - mse: 0.0253 - val_loss: 0.0875 - val_accuracy: 1.0000 - val_mse: 0.0069\n",
            "Epoch 179/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1444 - accuracy: 0.9613 - mse: 0.0236 - val_loss: 0.0958 - val_accuracy: 1.0000 - val_mse: 0.0080\n",
            "Epoch 180/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1236 - accuracy: 0.9716 - mse: 0.0192 - val_loss: 0.0997 - val_accuracy: 1.0000 - val_mse: 0.0086\n",
            "Epoch 181/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1201 - accuracy: 0.9690 - mse: 0.0185 - val_loss: 0.0981 - val_accuracy: 1.0000 - val_mse: 0.0083\n",
            "Epoch 182/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1377 - accuracy: 0.9626 - mse: 0.0226 - val_loss: 0.0919 - val_accuracy: 1.0000 - val_mse: 0.0074\n",
            "Epoch 183/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1370 - accuracy: 0.9652 - mse: 0.0222 - val_loss: 0.0864 - val_accuracy: 1.0000 - val_mse: 0.0066\n",
            "Epoch 184/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1276 - accuracy: 0.9690 - mse: 0.0211 - val_loss: 0.0835 - val_accuracy: 1.0000 - val_mse: 0.0062\n",
            "Epoch 185/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1386 - accuracy: 0.9574 - mse: 0.0230 - val_loss: 0.0787 - val_accuracy: 1.0000 - val_mse: 0.0056\n",
            "Epoch 186/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1238 - accuracy: 0.9690 - mse: 0.0200 - val_loss: 0.0733 - val_accuracy: 1.0000 - val_mse: 0.0050\n",
            "Epoch 187/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1087 - accuracy: 0.9768 - mse: 0.0162 - val_loss: 0.0738 - val_accuracy: 1.0000 - val_mse: 0.0051\n",
            "Epoch 188/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1251 - accuracy: 0.9690 - mse: 0.0194 - val_loss: 0.0780 - val_accuracy: 1.0000 - val_mse: 0.0056\n",
            "Epoch 189/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1312 - accuracy: 0.9755 - mse: 0.0201 - val_loss: 0.0829 - val_accuracy: 1.0000 - val_mse: 0.0064\n",
            "Epoch 190/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1198 - accuracy: 0.9677 - mse: 0.0190 - val_loss: 0.0848 - val_accuracy: 1.0000 - val_mse: 0.0065\n",
            "Epoch 191/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1138 - accuracy: 0.9832 - mse: 0.0167 - val_loss: 0.0895 - val_accuracy: 1.0000 - val_mse: 0.0071\n",
            "Epoch 192/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1172 - accuracy: 0.9677 - mse: 0.0184 - val_loss: 0.0920 - val_accuracy: 1.0000 - val_mse: 0.0074\n",
            "Epoch 193/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1224 - accuracy: 0.9742 - mse: 0.0189 - val_loss: 0.0877 - val_accuracy: 1.0000 - val_mse: 0.0067\n",
            "Epoch 194/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1157 - accuracy: 0.9677 - mse: 0.0183 - val_loss: 0.0768 - val_accuracy: 1.0000 - val_mse: 0.0052\n",
            "Epoch 195/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1102 - accuracy: 0.9690 - mse: 0.0174 - val_loss: 0.0702 - val_accuracy: 1.0000 - val_mse: 0.0045\n",
            "Epoch 196/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1214 - accuracy: 0.9652 - mse: 0.0208 - val_loss: 0.0718 - val_accuracy: 1.0000 - val_mse: 0.0048\n",
            "Epoch 197/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1118 - accuracy: 0.9755 - mse: 0.0172 - val_loss: 0.0791 - val_accuracy: 1.0000 - val_mse: 0.0058\n",
            "Epoch 198/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1035 - accuracy: 0.9755 - mse: 0.0159 - val_loss: 0.0815 - val_accuracy: 1.0000 - val_mse: 0.0061\n",
            "Epoch 199/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1132 - accuracy: 0.9703 - mse: 0.0180 - val_loss: 0.0849 - val_accuracy: 1.0000 - val_mse: 0.0065\n",
            "Epoch 200/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1114 - accuracy: 0.9768 - mse: 0.0167 - val_loss: 0.0864 - val_accuracy: 1.0000 - val_mse: 0.0066\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9867187656153761"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ5hlFwYNV2u",
        "outputId": "e8b9bd6b-0c19-42ee-8bbe-d6c08e295269"
      },
      "source": [
        "Xo, Yo = over_sample(fn.X, fn.Y,'SMOTE')\n",
        "borderline_model = train_model(Xo, Yo,200)\n",
        "output = borderline_model.predict_classes(np.array(fn.X))\n",
        "f1_score(np.argmax(fn.Y,1), output,labels=[0,1,2,3],average='weighted')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "4/4 [==============================] - 1s 52ms/step - loss: 1.2212 - accuracy: 0.4116 - mse: 0.2360 - val_loss: 1.2011 - val_accuracy: 0.0206 - val_mse: 0.2512\n",
            "Epoch 2/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.1653 - accuracy: 0.4052 - mse: 0.2291 - val_loss: 1.2976 - val_accuracy: 0.0155 - val_mse: 0.2691\n",
            "Epoch 3/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.1443 - accuracy: 0.4026 - mse: 0.2268 - val_loss: 1.3758 - val_accuracy: 0.0000e+00 - val_mse: 0.2840\n",
            "Epoch 4/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0987 - accuracy: 0.4400 - mse: 0.2190 - val_loss: 1.4455 - val_accuracy: 0.0000e+00 - val_mse: 0.2970\n",
            "Epoch 5/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 1.0593 - accuracy: 0.4632 - mse: 0.2118 - val_loss: 1.4918 - val_accuracy: 0.0000e+00 - val_mse: 0.3052\n",
            "Epoch 6/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0204 - accuracy: 0.4865 - mse: 0.2039 - val_loss: 1.5287 - val_accuracy: 0.0000e+00 - val_mse: 0.3115\n",
            "Epoch 7/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 1.0063 - accuracy: 0.5032 - mse: 0.2014 - val_loss: 1.5595 - val_accuracy: 0.0000e+00 - val_mse: 0.3165\n",
            "Epoch 8/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9793 - accuracy: 0.5342 - mse: 0.1958 - val_loss: 1.5808 - val_accuracy: 0.0000e+00 - val_mse: 0.3197\n",
            "Epoch 9/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9749 - accuracy: 0.5600 - mse: 0.1932 - val_loss: 1.6027 - val_accuracy: 0.0000e+00 - val_mse: 0.3232\n",
            "Epoch 10/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9535 - accuracy: 0.5768 - mse: 0.1896 - val_loss: 1.6218 - val_accuracy: 0.0000e+00 - val_mse: 0.3260\n",
            "Epoch 11/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9372 - accuracy: 0.5961 - mse: 0.1860 - val_loss: 1.6332 - val_accuracy: 0.0000e+00 - val_mse: 0.3276\n",
            "Epoch 12/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9160 - accuracy: 0.5781 - mse: 0.1815 - val_loss: 1.6368 - val_accuracy: 0.0000e+00 - val_mse: 0.3281\n",
            "Epoch 13/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9179 - accuracy: 0.5961 - mse: 0.1817 - val_loss: 1.6253 - val_accuracy: 0.0000e+00 - val_mse: 0.3265\n",
            "Epoch 14/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.9136 - accuracy: 0.5961 - mse: 0.1814 - val_loss: 1.5949 - val_accuracy: 0.0000e+00 - val_mse: 0.3219\n",
            "Epoch 15/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.8769 - accuracy: 0.6142 - mse: 0.1729 - val_loss: 1.5635 - val_accuracy: 0.0000e+00 - val_mse: 0.3170\n",
            "Epoch 16/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8627 - accuracy: 0.6310 - mse: 0.1703 - val_loss: 1.5372 - val_accuracy: 0.0000e+00 - val_mse: 0.3131\n",
            "Epoch 17/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.8563 - accuracy: 0.6335 - mse: 0.1681 - val_loss: 1.5132 - val_accuracy: 0.0000e+00 - val_mse: 0.3094\n",
            "Epoch 18/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.8323 - accuracy: 0.6529 - mse: 0.1636 - val_loss: 1.4811 - val_accuracy: 0.0103 - val_mse: 0.3035\n",
            "Epoch 19/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.8284 - accuracy: 0.6516 - mse: 0.1630 - val_loss: 1.4487 - val_accuracy: 0.0258 - val_mse: 0.2974\n",
            "Epoch 20/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.8111 - accuracy: 0.6439 - mse: 0.1588 - val_loss: 1.4067 - val_accuracy: 0.0361 - val_mse: 0.2894\n",
            "Epoch 21/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.8005 - accuracy: 0.6568 - mse: 0.1565 - val_loss: 1.3697 - val_accuracy: 0.0567 - val_mse: 0.2823\n",
            "Epoch 22/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7768 - accuracy: 0.6839 - mse: 0.1502 - val_loss: 1.3379 - val_accuracy: 0.0619 - val_mse: 0.2763\n",
            "Epoch 23/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.7815 - accuracy: 0.6877 - mse: 0.1524 - val_loss: 1.3133 - val_accuracy: 0.0670 - val_mse: 0.2717\n",
            "Epoch 24/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.7680 - accuracy: 0.6865 - mse: 0.1499 - val_loss: 1.2871 - val_accuracy: 0.0773 - val_mse: 0.2667\n",
            "Epoch 25/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.7559 - accuracy: 0.7032 - mse: 0.1477 - val_loss: 1.2689 - val_accuracy: 0.0825 - val_mse: 0.2635\n",
            "Epoch 26/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.7420 - accuracy: 0.7006 - mse: 0.1439 - val_loss: 1.2368 - val_accuracy: 0.1031 - val_mse: 0.2571\n",
            "Epoch 27/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.7033 - accuracy: 0.7045 - mse: 0.1363 - val_loss: 1.1950 - val_accuracy: 0.1546 - val_mse: 0.2476\n",
            "Epoch 28/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.7144 - accuracy: 0.7045 - mse: 0.1391 - val_loss: 1.1584 - val_accuracy: 0.1804 - val_mse: 0.2391\n",
            "Epoch 29/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6886 - accuracy: 0.7381 - mse: 0.1330 - val_loss: 1.1323 - val_accuracy: 0.2577 - val_mse: 0.2334\n",
            "Epoch 30/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6827 - accuracy: 0.7239 - mse: 0.1319 - val_loss: 1.0888 - val_accuracy: 0.3557 - val_mse: 0.2238\n",
            "Epoch 31/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6606 - accuracy: 0.7303 - mse: 0.1268 - val_loss: 1.0220 - val_accuracy: 0.4175 - val_mse: 0.2086\n",
            "Epoch 32/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6506 - accuracy: 0.7600 - mse: 0.1250 - val_loss: 0.9912 - val_accuracy: 0.4433 - val_mse: 0.2021\n",
            "Epoch 33/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6265 - accuracy: 0.7639 - mse: 0.1196 - val_loss: 0.9554 - val_accuracy: 0.4588 - val_mse: 0.1940\n",
            "Epoch 34/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.6221 - accuracy: 0.7665 - mse: 0.1187 - val_loss: 0.9183 - val_accuracy: 0.4948 - val_mse: 0.1853\n",
            "Epoch 35/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6176 - accuracy: 0.7806 - mse: 0.1177 - val_loss: 0.8915 - val_accuracy: 0.5103 - val_mse: 0.1791\n",
            "Epoch 36/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.6205 - accuracy: 0.7497 - mse: 0.1184 - val_loss: 0.8538 - val_accuracy: 0.5773 - val_mse: 0.1696\n",
            "Epoch 37/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5842 - accuracy: 0.7871 - mse: 0.1102 - val_loss: 0.8248 - val_accuracy: 0.6289 - val_mse: 0.1617\n",
            "Epoch 38/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5703 - accuracy: 0.7910 - mse: 0.1079 - val_loss: 0.8168 - val_accuracy: 0.6495 - val_mse: 0.1600\n",
            "Epoch 39/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5612 - accuracy: 0.7923 - mse: 0.1063 - val_loss: 0.8091 - val_accuracy: 0.6082 - val_mse: 0.1600\n",
            "Epoch 40/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5466 - accuracy: 0.7987 - mse: 0.1038 - val_loss: 0.8086 - val_accuracy: 0.5979 - val_mse: 0.1624\n",
            "Epoch 41/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5454 - accuracy: 0.7948 - mse: 0.1038 - val_loss: 0.8120 - val_accuracy: 0.5722 - val_mse: 0.1652\n",
            "Epoch 42/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.5197 - accuracy: 0.8116 - mse: 0.0976 - val_loss: 0.8310 - val_accuracy: 0.5155 - val_mse: 0.1721\n",
            "Epoch 43/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5492 - accuracy: 0.7897 - mse: 0.1052 - val_loss: 0.8096 - val_accuracy: 0.5670 - val_mse: 0.1658\n",
            "Epoch 44/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5142 - accuracy: 0.8168 - mse: 0.0976 - val_loss: 0.8047 - val_accuracy: 0.5773 - val_mse: 0.1646\n",
            "Epoch 45/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4998 - accuracy: 0.8245 - mse: 0.0946 - val_loss: 0.7646 - val_accuracy: 0.5979 - val_mse: 0.1538\n",
            "Epoch 46/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.5027 - accuracy: 0.8181 - mse: 0.0947 - val_loss: 0.7246 - val_accuracy: 0.6443 - val_mse: 0.1436\n",
            "Epoch 47/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4973 - accuracy: 0.8129 - mse: 0.0944 - val_loss: 0.6883 - val_accuracy: 0.6753 - val_mse: 0.1352\n",
            "Epoch 48/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4847 - accuracy: 0.8387 - mse: 0.0904 - val_loss: 0.6624 - val_accuracy: 0.7010 - val_mse: 0.1289\n",
            "Epoch 49/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.4820 - accuracy: 0.8297 - mse: 0.0913 - val_loss: 0.6374 - val_accuracy: 0.7474 - val_mse: 0.1221\n",
            "Epoch 50/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4699 - accuracy: 0.8374 - mse: 0.0883 - val_loss: 0.6250 - val_accuracy: 0.7629 - val_mse: 0.1194\n",
            "Epoch 51/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4581 - accuracy: 0.8516 - mse: 0.0854 - val_loss: 0.6346 - val_accuracy: 0.7423 - val_mse: 0.1228\n",
            "Epoch 52/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4597 - accuracy: 0.8232 - mse: 0.0859 - val_loss: 0.6712 - val_accuracy: 0.7216 - val_mse: 0.1333\n",
            "Epoch 53/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4374 - accuracy: 0.8426 - mse: 0.0806 - val_loss: 0.6992 - val_accuracy: 0.6701 - val_mse: 0.1418\n",
            "Epoch 54/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4421 - accuracy: 0.8361 - mse: 0.0842 - val_loss: 0.6824 - val_accuracy: 0.6804 - val_mse: 0.1381\n",
            "Epoch 55/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4445 - accuracy: 0.8452 - mse: 0.0835 - val_loss: 0.6310 - val_accuracy: 0.7268 - val_mse: 0.1253\n",
            "Epoch 56/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4279 - accuracy: 0.8581 - mse: 0.0788 - val_loss: 0.5852 - val_accuracy: 0.7474 - val_mse: 0.1136\n",
            "Epoch 57/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.4158 - accuracy: 0.8490 - mse: 0.0777 - val_loss: 0.5458 - val_accuracy: 0.7887 - val_mse: 0.1026\n",
            "Epoch 58/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4150 - accuracy: 0.8452 - mse: 0.0774 - val_loss: 0.5180 - val_accuracy: 0.8402 - val_mse: 0.0944\n",
            "Epoch 59/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4041 - accuracy: 0.8529 - mse: 0.0750 - val_loss: 0.5085 - val_accuracy: 0.8660 - val_mse: 0.0918\n",
            "Epoch 60/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4144 - accuracy: 0.8465 - mse: 0.0775 - val_loss: 0.5239 - val_accuracy: 0.8247 - val_mse: 0.0963\n",
            "Epoch 61/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.4057 - accuracy: 0.8632 - mse: 0.0753 - val_loss: 0.5291 - val_accuracy: 0.8041 - val_mse: 0.0979\n",
            "Epoch 62/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3945 - accuracy: 0.8671 - mse: 0.0724 - val_loss: 0.5351 - val_accuracy: 0.7835 - val_mse: 0.1000\n",
            "Epoch 63/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3984 - accuracy: 0.8516 - mse: 0.0739 - val_loss: 0.5123 - val_accuracy: 0.8144 - val_mse: 0.0946\n",
            "Epoch 64/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3794 - accuracy: 0.8813 - mse: 0.0687 - val_loss: 0.4838 - val_accuracy: 0.8505 - val_mse: 0.0877\n",
            "Epoch 65/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3743 - accuracy: 0.8813 - mse: 0.0679 - val_loss: 0.4756 - val_accuracy: 0.8711 - val_mse: 0.0862\n",
            "Epoch 66/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.3500 - accuracy: 0.8865 - mse: 0.0632 - val_loss: 0.4585 - val_accuracy: 0.9330 - val_mse: 0.0818\n",
            "Epoch 67/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3697 - accuracy: 0.8658 - mse: 0.0684 - val_loss: 0.4193 - val_accuracy: 0.9742 - val_mse: 0.0714\n",
            "Epoch 68/200\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.3468 - accuracy: 0.8877 - mse: 0.0627 - val_loss: 0.3932 - val_accuracy: 0.9897 - val_mse: 0.0643\n",
            "Epoch 69/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3590 - accuracy: 0.8903 - mse: 0.0657 - val_loss: 0.3952 - val_accuracy: 0.9897 - val_mse: 0.0649\n",
            "Epoch 70/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3374 - accuracy: 0.8929 - mse: 0.0607 - val_loss: 0.4118 - val_accuracy: 0.9742 - val_mse: 0.0698\n",
            "Epoch 71/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3347 - accuracy: 0.8903 - mse: 0.0613 - val_loss: 0.4063 - val_accuracy: 0.9742 - val_mse: 0.0690\n",
            "Epoch 72/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3270 - accuracy: 0.9032 - mse: 0.0585 - val_loss: 0.4042 - val_accuracy: 0.9639 - val_mse: 0.0688\n",
            "Epoch 73/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3157 - accuracy: 0.8890 - mse: 0.0574 - val_loss: 0.3945 - val_accuracy: 0.9691 - val_mse: 0.0662\n",
            "Epoch 74/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3266 - accuracy: 0.8890 - mse: 0.0594 - val_loss: 0.3842 - val_accuracy: 0.9691 - val_mse: 0.0633\n",
            "Epoch 75/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3194 - accuracy: 0.9084 - mse: 0.0568 - val_loss: 0.3750 - val_accuracy: 0.9742 - val_mse: 0.0608\n",
            "Epoch 76/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3273 - accuracy: 0.8942 - mse: 0.0596 - val_loss: 0.3520 - val_accuracy: 0.9845 - val_mse: 0.0551\n",
            "Epoch 77/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.3322 - accuracy: 0.8865 - mse: 0.0603 - val_loss: 0.3560 - val_accuracy: 0.9845 - val_mse: 0.0564\n",
            "Epoch 78/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.3274 - accuracy: 0.8903 - mse: 0.0591 - val_loss: 0.3588 - val_accuracy: 0.9845 - val_mse: 0.0575\n",
            "Epoch 79/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2970 - accuracy: 0.9097 - mse: 0.0516 - val_loss: 0.3366 - val_accuracy: 0.9845 - val_mse: 0.0525\n",
            "Epoch 80/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.3032 - accuracy: 0.9097 - mse: 0.0551 - val_loss: 0.3289 - val_accuracy: 0.9845 - val_mse: 0.0511\n",
            "Epoch 81/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2901 - accuracy: 0.9135 - mse: 0.0520 - val_loss: 0.3242 - val_accuracy: 0.9845 - val_mse: 0.0500\n",
            "Epoch 82/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3076 - accuracy: 0.9097 - mse: 0.0548 - val_loss: 0.3122 - val_accuracy: 0.9897 - val_mse: 0.0472\n",
            "Epoch 83/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2769 - accuracy: 0.9213 - mse: 0.0479 - val_loss: 0.3161 - val_accuracy: 0.9897 - val_mse: 0.0483\n",
            "Epoch 84/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.3014 - accuracy: 0.8981 - mse: 0.0547 - val_loss: 0.3221 - val_accuracy: 0.9845 - val_mse: 0.0498\n",
            "Epoch 85/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2821 - accuracy: 0.9148 - mse: 0.0508 - val_loss: 0.3170 - val_accuracy: 0.9897 - val_mse: 0.0484\n",
            "Epoch 86/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2835 - accuracy: 0.9110 - mse: 0.0489 - val_loss: 0.3074 - val_accuracy: 1.0000 - val_mse: 0.0460\n",
            "Epoch 87/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2596 - accuracy: 0.9290 - mse: 0.0451 - val_loss: 0.3140 - val_accuracy: 0.9948 - val_mse: 0.0477\n",
            "Epoch 88/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2969 - accuracy: 0.9161 - mse: 0.0519 - val_loss: 0.3283 - val_accuracy: 0.9897 - val_mse: 0.0515\n",
            "Epoch 89/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2866 - accuracy: 0.9110 - mse: 0.0512 - val_loss: 0.3139 - val_accuracy: 1.0000 - val_mse: 0.0484\n",
            "Epoch 90/200\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.2704 - accuracy: 0.9200 - mse: 0.0477 - val_loss: 0.2774 - val_accuracy: 1.0000 - val_mse: 0.0399\n",
            "Epoch 91/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2684 - accuracy: 0.9174 - mse: 0.0475 - val_loss: 0.2543 - val_accuracy: 1.0000 - val_mse: 0.0347\n",
            "Epoch 92/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2832 - accuracy: 0.9110 - mse: 0.0514 - val_loss: 0.2411 - val_accuracy: 1.0000 - val_mse: 0.0317\n",
            "Epoch 93/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2545 - accuracy: 0.9226 - mse: 0.0443 - val_loss: 0.2311 - val_accuracy: 1.0000 - val_mse: 0.0297\n",
            "Epoch 94/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2523 - accuracy: 0.9174 - mse: 0.0443 - val_loss: 0.2330 - val_accuracy: 1.0000 - val_mse: 0.0303\n",
            "Epoch 95/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2708 - accuracy: 0.9123 - mse: 0.0476 - val_loss: 0.2463 - val_accuracy: 1.0000 - val_mse: 0.0335\n",
            "Epoch 96/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2632 - accuracy: 0.9148 - mse: 0.0468 - val_loss: 0.2560 - val_accuracy: 1.0000 - val_mse: 0.0360\n",
            "Epoch 97/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2483 - accuracy: 0.9290 - mse: 0.0436 - val_loss: 0.2445 - val_accuracy: 1.0000 - val_mse: 0.0335\n",
            "Epoch 98/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2635 - accuracy: 0.9135 - mse: 0.0474 - val_loss: 0.2223 - val_accuracy: 1.0000 - val_mse: 0.0285\n",
            "Epoch 99/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2459 - accuracy: 0.9226 - mse: 0.0427 - val_loss: 0.2238 - val_accuracy: 1.0000 - val_mse: 0.0286\n",
            "Epoch 100/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2449 - accuracy: 0.9303 - mse: 0.0419 - val_loss: 0.2286 - val_accuracy: 1.0000 - val_mse: 0.0296\n",
            "Epoch 101/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2436 - accuracy: 0.9239 - mse: 0.0435 - val_loss: 0.2331 - val_accuracy: 1.0000 - val_mse: 0.0304\n",
            "Epoch 102/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2378 - accuracy: 0.9252 - mse: 0.0418 - val_loss: 0.2227 - val_accuracy: 1.0000 - val_mse: 0.0284\n",
            "Epoch 103/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2342 - accuracy: 0.9290 - mse: 0.0414 - val_loss: 0.2245 - val_accuracy: 1.0000 - val_mse: 0.0291\n",
            "Epoch 104/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2218 - accuracy: 0.9406 - mse: 0.0379 - val_loss: 0.2379 - val_accuracy: 1.0000 - val_mse: 0.0324\n",
            "Epoch 105/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2259 - accuracy: 0.9316 - mse: 0.0392 - val_loss: 0.2457 - val_accuracy: 1.0000 - val_mse: 0.0341\n",
            "Epoch 106/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2221 - accuracy: 0.9368 - mse: 0.0390 - val_loss: 0.2281 - val_accuracy: 1.0000 - val_mse: 0.0298\n",
            "Epoch 107/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2136 - accuracy: 0.9303 - mse: 0.0374 - val_loss: 0.2075 - val_accuracy: 1.0000 - val_mse: 0.0253\n",
            "Epoch 108/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2231 - accuracy: 0.9406 - mse: 0.0386 - val_loss: 0.1982 - val_accuracy: 1.0000 - val_mse: 0.0233\n",
            "Epoch 109/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2071 - accuracy: 0.9432 - mse: 0.0354 - val_loss: 0.2009 - val_accuracy: 1.0000 - val_mse: 0.0237\n",
            "Epoch 110/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.2228 - accuracy: 0.9342 - mse: 0.0387 - val_loss: 0.2086 - val_accuracy: 1.0000 - val_mse: 0.0253\n",
            "Epoch 111/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1960 - accuracy: 0.9548 - mse: 0.0325 - val_loss: 0.2142 - val_accuracy: 1.0000 - val_mse: 0.0267\n",
            "Epoch 112/200\n",
            "4/4 [==============================] - 0s 8ms/step - loss: 0.2171 - accuracy: 0.9406 - mse: 0.0377 - val_loss: 0.2222 - val_accuracy: 1.0000 - val_mse: 0.0289\n",
            "Epoch 113/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2145 - accuracy: 0.9290 - mse: 0.0375 - val_loss: 0.2220 - val_accuracy: 1.0000 - val_mse: 0.0291\n",
            "Epoch 114/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2049 - accuracy: 0.9523 - mse: 0.0341 - val_loss: 0.2038 - val_accuracy: 1.0000 - val_mse: 0.0256\n",
            "Epoch 115/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2006 - accuracy: 0.9510 - mse: 0.0333 - val_loss: 0.1842 - val_accuracy: 1.0000 - val_mse: 0.0218\n",
            "Epoch 116/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2055 - accuracy: 0.9484 - mse: 0.0345 - val_loss: 0.1612 - val_accuracy: 1.0000 - val_mse: 0.0176\n",
            "Epoch 117/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2017 - accuracy: 0.9342 - mse: 0.0348 - val_loss: 0.1509 - val_accuracy: 1.0000 - val_mse: 0.0159\n",
            "Epoch 118/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1944 - accuracy: 0.9471 - mse: 0.0325 - val_loss: 0.1579 - val_accuracy: 1.0000 - val_mse: 0.0172\n",
            "Epoch 119/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1831 - accuracy: 0.9523 - mse: 0.0299 - val_loss: 0.1647 - val_accuracy: 1.0000 - val_mse: 0.0186\n",
            "Epoch 120/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.2033 - accuracy: 0.9303 - mse: 0.0358 - val_loss: 0.1786 - val_accuracy: 1.0000 - val_mse: 0.0213\n",
            "Epoch 121/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1975 - accuracy: 0.9432 - mse: 0.0335 - val_loss: 0.2087 - val_accuracy: 1.0000 - val_mse: 0.0274\n",
            "Epoch 122/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1961 - accuracy: 0.9368 - mse: 0.0338 - val_loss: 0.2052 - val_accuracy: 1.0000 - val_mse: 0.0264\n",
            "Epoch 123/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.2031 - accuracy: 0.9394 - mse: 0.0346 - val_loss: 0.1849 - val_accuracy: 1.0000 - val_mse: 0.0222\n",
            "Epoch 124/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1949 - accuracy: 0.9419 - mse: 0.0333 - val_loss: 0.1606 - val_accuracy: 1.0000 - val_mse: 0.0176\n",
            "Epoch 125/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1893 - accuracy: 0.9432 - mse: 0.0323 - val_loss: 0.1615 - val_accuracy: 1.0000 - val_mse: 0.0177\n",
            "Epoch 126/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1799 - accuracy: 0.9471 - mse: 0.0300 - val_loss: 0.1677 - val_accuracy: 1.0000 - val_mse: 0.0189\n",
            "Epoch 127/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1793 - accuracy: 0.9484 - mse: 0.0300 - val_loss: 0.1590 - val_accuracy: 1.0000 - val_mse: 0.0173\n",
            "Epoch 128/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1733 - accuracy: 0.9432 - mse: 0.0292 - val_loss: 0.1529 - val_accuracy: 1.0000 - val_mse: 0.0162\n",
            "Epoch 129/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1718 - accuracy: 0.9484 - mse: 0.0282 - val_loss: 0.1419 - val_accuracy: 1.0000 - val_mse: 0.0144\n",
            "Epoch 130/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1696 - accuracy: 0.9548 - mse: 0.0277 - val_loss: 0.1384 - val_accuracy: 1.0000 - val_mse: 0.0139\n",
            "Epoch 131/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1778 - accuracy: 0.9484 - mse: 0.0302 - val_loss: 0.1419 - val_accuracy: 1.0000 - val_mse: 0.0146\n",
            "Epoch 132/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1636 - accuracy: 0.9574 - mse: 0.0271 - val_loss: 0.1538 - val_accuracy: 1.0000 - val_mse: 0.0169\n",
            "Epoch 133/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1934 - accuracy: 0.9394 - mse: 0.0339 - val_loss: 0.1670 - val_accuracy: 1.0000 - val_mse: 0.0196\n",
            "Epoch 134/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1834 - accuracy: 0.9458 - mse: 0.0319 - val_loss: 0.1534 - val_accuracy: 1.0000 - val_mse: 0.0171\n",
            "Epoch 135/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1734 - accuracy: 0.9523 - mse: 0.0295 - val_loss: 0.1384 - val_accuracy: 1.0000 - val_mse: 0.0142\n",
            "Epoch 136/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1767 - accuracy: 0.9471 - mse: 0.0295 - val_loss: 0.1212 - val_accuracy: 1.0000 - val_mse: 0.0113\n",
            "Epoch 137/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1577 - accuracy: 0.9613 - mse: 0.0247 - val_loss: 0.1055 - val_accuracy: 1.0000 - val_mse: 0.0090\n",
            "Epoch 138/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1698 - accuracy: 0.9458 - mse: 0.0295 - val_loss: 0.1052 - val_accuracy: 1.0000 - val_mse: 0.0089\n",
            "Epoch 139/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1654 - accuracy: 0.9484 - mse: 0.0284 - val_loss: 0.1103 - val_accuracy: 1.0000 - val_mse: 0.0096\n",
            "Epoch 140/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1666 - accuracy: 0.9497 - mse: 0.0283 - val_loss: 0.1198 - val_accuracy: 1.0000 - val_mse: 0.0111\n",
            "Epoch 141/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1516 - accuracy: 0.9613 - mse: 0.0256 - val_loss: 0.1217 - val_accuracy: 1.0000 - val_mse: 0.0113\n",
            "Epoch 142/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1430 - accuracy: 0.9639 - mse: 0.0228 - val_loss: 0.1205 - val_accuracy: 1.0000 - val_mse: 0.0110\n",
            "Epoch 143/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1487 - accuracy: 0.9587 - mse: 0.0235 - val_loss: 0.1182 - val_accuracy: 1.0000 - val_mse: 0.0106\n",
            "Epoch 144/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1510 - accuracy: 0.9587 - mse: 0.0249 - val_loss: 0.1174 - val_accuracy: 1.0000 - val_mse: 0.0104\n",
            "Epoch 145/200\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.1363 - accuracy: 0.9716 - mse: 0.0206 - val_loss: 0.1305 - val_accuracy: 1.0000 - val_mse: 0.0122\n",
            "Epoch 146/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1624 - accuracy: 0.9574 - mse: 0.0274 - val_loss: 0.1328 - val_accuracy: 1.0000 - val_mse: 0.0126\n",
            "Epoch 147/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1389 - accuracy: 0.9665 - mse: 0.0218 - val_loss: 0.1221 - val_accuracy: 1.0000 - val_mse: 0.0108\n",
            "Epoch 148/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1536 - accuracy: 0.9613 - mse: 0.0246 - val_loss: 0.1159 - val_accuracy: 1.0000 - val_mse: 0.0099\n",
            "Epoch 149/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1302 - accuracy: 0.9677 - mse: 0.0202 - val_loss: 0.1188 - val_accuracy: 1.0000 - val_mse: 0.0103\n",
            "Epoch 150/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1428 - accuracy: 0.9548 - mse: 0.0237 - val_loss: 0.1133 - val_accuracy: 1.0000 - val_mse: 0.0095\n",
            "Epoch 151/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1477 - accuracy: 0.9600 - mse: 0.0245 - val_loss: 0.1133 - val_accuracy: 1.0000 - val_mse: 0.0094\n",
            "Epoch 152/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1387 - accuracy: 0.9613 - mse: 0.0223 - val_loss: 0.1084 - val_accuracy: 1.0000 - val_mse: 0.0088\n",
            "Epoch 153/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1372 - accuracy: 0.9677 - mse: 0.0209 - val_loss: 0.0931 - val_accuracy: 1.0000 - val_mse: 0.0068\n",
            "Epoch 154/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1377 - accuracy: 0.9639 - mse: 0.0220 - val_loss: 0.0859 - val_accuracy: 1.0000 - val_mse: 0.0059\n",
            "Epoch 155/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1397 - accuracy: 0.9535 - mse: 0.0233 - val_loss: 0.0928 - val_accuracy: 1.0000 - val_mse: 0.0068\n",
            "Epoch 156/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1291 - accuracy: 0.9703 - mse: 0.0198 - val_loss: 0.1105 - val_accuracy: 1.0000 - val_mse: 0.0093\n",
            "Epoch 157/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1284 - accuracy: 0.9665 - mse: 0.0204 - val_loss: 0.1153 - val_accuracy: 1.0000 - val_mse: 0.0101\n",
            "Epoch 158/200\n",
            "4/4 [==============================] - 0s 17ms/step - loss: 0.1311 - accuracy: 0.9613 - mse: 0.0217 - val_loss: 0.1047 - val_accuracy: 1.0000 - val_mse: 0.0086\n",
            "Epoch 159/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1390 - accuracy: 0.9639 - mse: 0.0228 - val_loss: 0.0950 - val_accuracy: 1.0000 - val_mse: 0.0073\n",
            "Epoch 160/200\n",
            "4/4 [==============================] - 0s 14ms/step - loss: 0.1180 - accuracy: 0.9677 - mse: 0.0191 - val_loss: 0.0887 - val_accuracy: 1.0000 - val_mse: 0.0065\n",
            "Epoch 161/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1374 - accuracy: 0.9690 - mse: 0.0218 - val_loss: 0.0860 - val_accuracy: 1.0000 - val_mse: 0.0062\n",
            "Epoch 162/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1411 - accuracy: 0.9587 - mse: 0.0235 - val_loss: 0.0859 - val_accuracy: 1.0000 - val_mse: 0.0062\n",
            "Epoch 163/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1191 - accuracy: 0.9677 - mse: 0.0190 - val_loss: 0.0984 - val_accuracy: 1.0000 - val_mse: 0.0079\n",
            "Epoch 164/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1167 - accuracy: 0.9677 - mse: 0.0184 - val_loss: 0.1047 - val_accuracy: 1.0000 - val_mse: 0.0088\n",
            "Epoch 165/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1284 - accuracy: 0.9703 - mse: 0.0210 - val_loss: 0.1033 - val_accuracy: 1.0000 - val_mse: 0.0086\n",
            "Epoch 166/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1146 - accuracy: 0.9742 - mse: 0.0178 - val_loss: 0.0893 - val_accuracy: 1.0000 - val_mse: 0.0067\n",
            "Epoch 167/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1335 - accuracy: 0.9639 - mse: 0.0224 - val_loss: 0.0831 - val_accuracy: 1.0000 - val_mse: 0.0059\n",
            "Epoch 168/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1255 - accuracy: 0.9639 - mse: 0.0206 - val_loss: 0.0902 - val_accuracy: 1.0000 - val_mse: 0.0068\n",
            "Epoch 169/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1347 - accuracy: 0.9561 - mse: 0.0226 - val_loss: 0.0967 - val_accuracy: 1.0000 - val_mse: 0.0078\n",
            "Epoch 170/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1202 - accuracy: 0.9677 - mse: 0.0187 - val_loss: 0.0929 - val_accuracy: 1.0000 - val_mse: 0.0073\n",
            "Epoch 171/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1011 - accuracy: 0.9781 - mse: 0.0146 - val_loss: 0.0874 - val_accuracy: 1.0000 - val_mse: 0.0065\n",
            "Epoch 172/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1129 - accuracy: 0.9729 - mse: 0.0177 - val_loss: 0.0952 - val_accuracy: 1.0000 - val_mse: 0.0076\n",
            "Epoch 173/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1107 - accuracy: 0.9729 - mse: 0.0172 - val_loss: 0.1007 - val_accuracy: 1.0000 - val_mse: 0.0084\n",
            "Epoch 174/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1153 - accuracy: 0.9729 - mse: 0.0178 - val_loss: 0.1024 - val_accuracy: 1.0000 - val_mse: 0.0086\n",
            "Epoch 175/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1149 - accuracy: 0.9677 - mse: 0.0184 - val_loss: 0.0870 - val_accuracy: 1.0000 - val_mse: 0.0063\n",
            "Epoch 176/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1174 - accuracy: 0.9677 - mse: 0.0190 - val_loss: 0.0687 - val_accuracy: 1.0000 - val_mse: 0.0040\n",
            "Epoch 177/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1192 - accuracy: 0.9639 - mse: 0.0199 - val_loss: 0.0637 - val_accuracy: 1.0000 - val_mse: 0.0035\n",
            "Epoch 178/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1170 - accuracy: 0.9665 - mse: 0.0191 - val_loss: 0.0668 - val_accuracy: 1.0000 - val_mse: 0.0038\n",
            "Epoch 179/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1181 - accuracy: 0.9626 - mse: 0.0197 - val_loss: 0.0702 - val_accuracy: 1.0000 - val_mse: 0.0042\n",
            "Epoch 180/200\n",
            "4/4 [==============================] - 0s 12ms/step - loss: 0.1102 - accuracy: 0.9729 - mse: 0.0175 - val_loss: 0.0718 - val_accuracy: 1.0000 - val_mse: 0.0044\n",
            "Epoch 181/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1089 - accuracy: 0.9703 - mse: 0.0168 - val_loss: 0.0696 - val_accuracy: 1.0000 - val_mse: 0.0042\n",
            "Epoch 182/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1117 - accuracy: 0.9716 - mse: 0.0176 - val_loss: 0.0703 - val_accuracy: 1.0000 - val_mse: 0.0042\n",
            "Epoch 183/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0986 - accuracy: 0.9768 - mse: 0.0150 - val_loss: 0.0802 - val_accuracy: 1.0000 - val_mse: 0.0054\n",
            "Epoch 184/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1084 - accuracy: 0.9755 - mse: 0.0165 - val_loss: 0.0845 - val_accuracy: 1.0000 - val_mse: 0.0060\n",
            "Epoch 185/200\n",
            "4/4 [==============================] - 0s 15ms/step - loss: 0.1040 - accuracy: 0.9729 - mse: 0.0164 - val_loss: 0.0784 - val_accuracy: 1.0000 - val_mse: 0.0052\n",
            "Epoch 186/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0978 - accuracy: 0.9755 - mse: 0.0152 - val_loss: 0.0739 - val_accuracy: 1.0000 - val_mse: 0.0047\n",
            "Epoch 187/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.1033 - accuracy: 0.9729 - mse: 0.0161 - val_loss: 0.0730 - val_accuracy: 1.0000 - val_mse: 0.0047\n",
            "Epoch 188/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1086 - accuracy: 0.9755 - mse: 0.0171 - val_loss: 0.0720 - val_accuracy: 1.0000 - val_mse: 0.0046\n",
            "Epoch 189/200\n",
            "4/4 [==============================] - 0s 16ms/step - loss: 0.0934 - accuracy: 0.9755 - mse: 0.0139 - val_loss: 0.0696 - val_accuracy: 1.0000 - val_mse: 0.0043\n",
            "Epoch 190/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0994 - accuracy: 0.9703 - mse: 0.0159 - val_loss: 0.0675 - val_accuracy: 1.0000 - val_mse: 0.0041\n",
            "Epoch 191/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0983 - accuracy: 0.9716 - mse: 0.0148 - val_loss: 0.0649 - val_accuracy: 1.0000 - val_mse: 0.0038\n",
            "Epoch 192/200\n",
            "4/4 [==============================] - 0s 9ms/step - loss: 0.0937 - accuracy: 0.9781 - mse: 0.0137 - val_loss: 0.0666 - val_accuracy: 1.0000 - val_mse: 0.0040\n",
            "Epoch 193/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0963 - accuracy: 0.9768 - mse: 0.0153 - val_loss: 0.0751 - val_accuracy: 1.0000 - val_mse: 0.0049\n",
            "Epoch 194/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1034 - accuracy: 0.9677 - mse: 0.0173 - val_loss: 0.0781 - val_accuracy: 1.0000 - val_mse: 0.0053\n",
            "Epoch 195/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.1047 - accuracy: 0.9690 - mse: 0.0169 - val_loss: 0.0794 - val_accuracy: 1.0000 - val_mse: 0.0056\n",
            "Epoch 196/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0879 - accuracy: 0.9794 - mse: 0.0124 - val_loss: 0.0774 - val_accuracy: 1.0000 - val_mse: 0.0054\n",
            "Epoch 197/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.1035 - accuracy: 0.9716 - mse: 0.0167 - val_loss: 0.0771 - val_accuracy: 1.0000 - val_mse: 0.0055\n",
            "Epoch 198/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0970 - accuracy: 0.9703 - mse: 0.0152 - val_loss: 0.0728 - val_accuracy: 1.0000 - val_mse: 0.0050\n",
            "Epoch 199/200\n",
            "4/4 [==============================] - 0s 11ms/step - loss: 0.0918 - accuracy: 0.9781 - mse: 0.0140 - val_loss: 0.0683 - val_accuracy: 1.0000 - val_mse: 0.0044\n",
            "Epoch 200/200\n",
            "4/4 [==============================] - 0s 10ms/step - loss: 0.0940 - accuracy: 0.9768 - mse: 0.0140 - val_loss: 0.0716 - val_accuracy: 1.0000 - val_mse: 0.0046\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9972663894833959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGTTum8SNY9E",
        "outputId": "8b8e39af-e7e0-42e3-e004-7f7531be533a"
      },
      "source": [
        "Xo, Yo = over_sample(fn.X, fn.Y,'ADASYN')\n",
        "borderline_model, history = train_model(Xo, Yo,200)\n",
        "# output = borderline_model.predict_classes(np.array(fn.X))\n",
        "# f1_score(np.argmax(fn.Y,1), output,labels=[0,1,2,3],average='weighted')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 114ms/step - loss: 1.1970 - accuracy: 0.3035 - mse: 0.2414 - val_loss: 0.9892 - val_accuracy: 0.7037 - val_mse: 0.1980\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.1515 - accuracy: 0.3445 - mse: 0.2327 - val_loss: 1.0271 - val_accuracy: 0.3556 - val_mse: 0.2077\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.1027 - accuracy: 0.3855 - mse: 0.2230 - val_loss: 1.0616 - val_accuracy: 0.1704 - val_mse: 0.2169\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 1.0544 - accuracy: 0.4413 - mse: 0.2128 - val_loss: 1.0872 - val_accuracy: 0.0815 - val_mse: 0.2242\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 1.0328 - accuracy: 0.4432 - mse: 0.2081 - val_loss: 1.1085 - val_accuracy: 0.0667 - val_mse: 0.2308\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 1.0050 - accuracy: 0.4991 - mse: 0.2026 - val_loss: 1.1252 - val_accuracy: 0.0370 - val_mse: 0.2364\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.9812 - accuracy: 0.5196 - mse: 0.1971 - val_loss: 1.1388 - val_accuracy: 0.0296 - val_mse: 0.2415\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.9685 - accuracy: 0.5419 - mse: 0.1947 - val_loss: 1.1521 - val_accuracy: 0.0000e+00 - val_mse: 0.2467\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.9462 - accuracy: 0.5810 - mse: 0.1900 - val_loss: 1.1618 - val_accuracy: 0.0000e+00 - val_mse: 0.2513\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.9332 - accuracy: 0.5698 - mse: 0.1877 - val_loss: 1.1702 - val_accuracy: 0.0000e+00 - val_mse: 0.2556\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.9088 - accuracy: 0.6015 - mse: 0.1826 - val_loss: 1.1701 - val_accuracy: 0.0000e+00 - val_mse: 0.2578\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.8805 - accuracy: 0.6071 - mse: 0.1766 - val_loss: 1.1609 - val_accuracy: 0.0000e+00 - val_mse: 0.2578\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.8833 - accuracy: 0.5978 - mse: 0.1776 - val_loss: 1.1532 - val_accuracy: 0.0000e+00 - val_mse: 0.2582\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.8824 - accuracy: 0.6071 - mse: 0.1778 - val_loss: 1.1442 - val_accuracy: 0.0000e+00 - val_mse: 0.2582\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.8684 - accuracy: 0.6201 - mse: 0.1751 - val_loss: 1.1296 - val_accuracy: 0.0000e+00 - val_mse: 0.2566\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.8502 - accuracy: 0.5922 - mse: 0.1722 - val_loss: 1.1170 - val_accuracy: 0.0074 - val_mse: 0.2553\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.8424 - accuracy: 0.5978 - mse: 0.1698 - val_loss: 1.1087 - val_accuracy: 0.0148 - val_mse: 0.2550\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.8216 - accuracy: 0.6313 - mse: 0.1652 - val_loss: 1.1003 - val_accuracy: 0.0222 - val_mse: 0.2546\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.8155 - accuracy: 0.6518 - mse: 0.1646 - val_loss: 1.0873 - val_accuracy: 0.0370 - val_mse: 0.2528\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.8055 - accuracy: 0.6387 - mse: 0.1622 - val_loss: 1.0728 - val_accuracy: 0.0444 - val_mse: 0.2505\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.8016 - accuracy: 0.6443 - mse: 0.1613 - val_loss: 1.0512 - val_accuracy: 0.0444 - val_mse: 0.2459\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.7988 - accuracy: 0.6238 - mse: 0.1607 - val_loss: 1.0323 - val_accuracy: 0.0667 - val_mse: 0.2420\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.7856 - accuracy: 0.6425 - mse: 0.1571 - val_loss: 1.0164 - val_accuracy: 0.0889 - val_mse: 0.2387\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.7668 - accuracy: 0.6629 - mse: 0.1536 - val_loss: 1.0107 - val_accuracy: 0.0889 - val_mse: 0.2382\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.7711 - accuracy: 0.6499 - mse: 0.1543 - val_loss: 0.9988 - val_accuracy: 0.0963 - val_mse: 0.2356\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.7564 - accuracy: 0.6629 - mse: 0.1517 - val_loss: 0.9669 - val_accuracy: 0.1185 - val_mse: 0.2271\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.7348 - accuracy: 0.6704 - mse: 0.1472 - val_loss: 0.9270 - val_accuracy: 0.1778 - val_mse: 0.2162\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.7400 - accuracy: 0.6927 - mse: 0.1462 - val_loss: 0.8929 - val_accuracy: 0.2148 - val_mse: 0.2067\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.7424 - accuracy: 0.6909 - mse: 0.1475 - val_loss: 0.8717 - val_accuracy: 0.2444 - val_mse: 0.2010\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.7286 - accuracy: 0.6927 - mse: 0.1436 - val_loss: 0.8580 - val_accuracy: 0.3704 - val_mse: 0.1974\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.7090 - accuracy: 0.6983 - mse: 0.1390 - val_loss: 0.8365 - val_accuracy: 0.4296 - val_mse: 0.1916\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.7146 - accuracy: 0.7300 - mse: 0.1398 - val_loss: 0.8106 - val_accuracy: 0.4667 - val_mse: 0.1844\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6938 - accuracy: 0.7449 - mse: 0.1350 - val_loss: 0.7791 - val_accuracy: 0.5037 - val_mse: 0.1755\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6949 - accuracy: 0.7356 - mse: 0.1350 - val_loss: 0.7704 - val_accuracy: 0.5259 - val_mse: 0.1733\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.7010 - accuracy: 0.7318 - mse: 0.1344 - val_loss: 0.7567 - val_accuracy: 0.5407 - val_mse: 0.1696\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6736 - accuracy: 0.7263 - mse: 0.1313 - val_loss: 0.7395 - val_accuracy: 0.5778 - val_mse: 0.1648\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6686 - accuracy: 0.7654 - mse: 0.1279 - val_loss: 0.7322 - val_accuracy: 0.5852 - val_mse: 0.1628\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6522 - accuracy: 0.7691 - mse: 0.1240 - val_loss: 0.7289 - val_accuracy: 0.5926 - val_mse: 0.1622\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6560 - accuracy: 0.7505 - mse: 0.1262 - val_loss: 0.7261 - val_accuracy: 0.6148 - val_mse: 0.1615\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.6398 - accuracy: 0.7821 - mse: 0.1211 - val_loss: 0.7121 - val_accuracy: 0.6222 - val_mse: 0.1574\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.6413 - accuracy: 0.7672 - mse: 0.1200 - val_loss: 0.6836 - val_accuracy: 0.7037 - val_mse: 0.1490\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.6313 - accuracy: 0.7747 - mse: 0.1190 - val_loss: 0.6673 - val_accuracy: 0.7037 - val_mse: 0.1442\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.6100 - accuracy: 0.8045 - mse: 0.1141 - val_loss: 0.6658 - val_accuracy: 0.7037 - val_mse: 0.1436\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.6104 - accuracy: 0.7691 - mse: 0.1142 - val_loss: 0.6688 - val_accuracy: 0.7037 - val_mse: 0.1442\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.5816 - accuracy: 0.7970 - mse: 0.1080 - val_loss: 0.6673 - val_accuracy: 0.7037 - val_mse: 0.1437\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.5884 - accuracy: 0.8045 - mse: 0.1090 - val_loss: 0.6569 - val_accuracy: 0.7037 - val_mse: 0.1408\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.5634 - accuracy: 0.8212 - mse: 0.1027 - val_loss: 0.6204 - val_accuracy: 0.7259 - val_mse: 0.1303\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.5705 - accuracy: 0.8212 - mse: 0.1031 - val_loss: 0.5742 - val_accuracy: 0.7481 - val_mse: 0.1171\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.5427 - accuracy: 0.8343 - mse: 0.0973 - val_loss: 0.5451 - val_accuracy: 0.7704 - val_mse: 0.1089\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.5396 - accuracy: 0.8399 - mse: 0.0971 - val_loss: 0.5280 - val_accuracy: 0.7778 - val_mse: 0.1041\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.5413 - accuracy: 0.8417 - mse: 0.0972 - val_loss: 0.5208 - val_accuracy: 0.7852 - val_mse: 0.1022\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.5136 - accuracy: 0.8547 - mse: 0.0900 - val_loss: 0.5218 - val_accuracy: 0.7852 - val_mse: 0.1027\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.5334 - accuracy: 0.8399 - mse: 0.0954 - val_loss: 0.5045 - val_accuracy: 0.7926 - val_mse: 0.0980\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.5120 - accuracy: 0.8454 - mse: 0.0902 - val_loss: 0.4831 - val_accuracy: 0.7926 - val_mse: 0.0921\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4892 - accuracy: 0.8827 - mse: 0.0859 - val_loss: 0.4767 - val_accuracy: 0.7926 - val_mse: 0.0904\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.5086 - accuracy: 0.8603 - mse: 0.0888 - val_loss: 0.4778 - val_accuracy: 0.7926 - val_mse: 0.0909\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4925 - accuracy: 0.8678 - mse: 0.0848 - val_loss: 0.4965 - val_accuracy: 0.7926 - val_mse: 0.0962\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4779 - accuracy: 0.8659 - mse: 0.0826 - val_loss: 0.5079 - val_accuracy: 0.7852 - val_mse: 0.0994\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4757 - accuracy: 0.8771 - mse: 0.0817 - val_loss: 0.5076 - val_accuracy: 0.7704 - val_mse: 0.0995\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.4818 - accuracy: 0.8585 - mse: 0.0849 - val_loss: 0.4774 - val_accuracy: 0.7926 - val_mse: 0.0914\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4688 - accuracy: 0.8603 - mse: 0.0814 - val_loss: 0.4689 - val_accuracy: 0.7926 - val_mse: 0.0894\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4522 - accuracy: 0.8696 - mse: 0.0783 - val_loss: 0.4731 - val_accuracy: 0.7926 - val_mse: 0.0909\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4567 - accuracy: 0.8678 - mse: 0.0776 - val_loss: 0.4757 - val_accuracy: 0.7704 - val_mse: 0.0920\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.4499 - accuracy: 0.8808 - mse: 0.0769 - val_loss: 0.4611 - val_accuracy: 0.7778 - val_mse: 0.0884\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4436 - accuracy: 0.8808 - mse: 0.0765 - val_loss: 0.4166 - val_accuracy: 0.8296 - val_mse: 0.0766\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4384 - accuracy: 0.8771 - mse: 0.0750 - val_loss: 0.3844 - val_accuracy: 0.8667 - val_mse: 0.0683\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4274 - accuracy: 0.8827 - mse: 0.0736 - val_loss: 0.3829 - val_accuracy: 0.8593 - val_mse: 0.0682\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.4097 - accuracy: 0.8883 - mse: 0.0696 - val_loss: 0.3954 - val_accuracy: 0.8444 - val_mse: 0.0716\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4222 - accuracy: 0.8790 - mse: 0.0716 - val_loss: 0.4054 - val_accuracy: 0.8444 - val_mse: 0.0741\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.4091 - accuracy: 0.8976 - mse: 0.0682 - val_loss: 0.4140 - val_accuracy: 0.8370 - val_mse: 0.0764\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3955 - accuracy: 0.9050 - mse: 0.0654 - val_loss: 0.4112 - val_accuracy: 0.8296 - val_mse: 0.0759\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.4219 - accuracy: 0.8715 - mse: 0.0717 - val_loss: 0.3798 - val_accuracy: 0.8593 - val_mse: 0.0676\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.3931 - accuracy: 0.8920 - mse: 0.0665 - val_loss: 0.3537 - val_accuracy: 0.8963 - val_mse: 0.0608\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3977 - accuracy: 0.8864 - mse: 0.0669 - val_loss: 0.3344 - val_accuracy: 0.9037 - val_mse: 0.0558\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3915 - accuracy: 0.8883 - mse: 0.0662 - val_loss: 0.3200 - val_accuracy: 0.9333 - val_mse: 0.0522\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.3953 - accuracy: 0.8845 - mse: 0.0662 - val_loss: 0.3038 - val_accuracy: 0.9630 - val_mse: 0.0484\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3896 - accuracy: 0.9069 - mse: 0.0646 - val_loss: 0.3032 - val_accuracy: 0.9481 - val_mse: 0.0484\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3746 - accuracy: 0.9143 - mse: 0.0613 - val_loss: 0.3140 - val_accuracy: 0.9333 - val_mse: 0.0512\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.3779 - accuracy: 0.8920 - mse: 0.0641 - val_loss: 0.3379 - val_accuracy: 0.8815 - val_mse: 0.0573\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3763 - accuracy: 0.8901 - mse: 0.0637 - val_loss: 0.3531 - val_accuracy: 0.8815 - val_mse: 0.0610\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.3557 - accuracy: 0.8976 - mse: 0.0594 - val_loss: 0.3485 - val_accuracy: 0.8889 - val_mse: 0.0597\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.3496 - accuracy: 0.9069 - mse: 0.0580 - val_loss: 0.3260 - val_accuracy: 0.8963 - val_mse: 0.0540\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.3391 - accuracy: 0.9143 - mse: 0.0552 - val_loss: 0.2920 - val_accuracy: 0.9630 - val_mse: 0.0457\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3350 - accuracy: 0.9088 - mse: 0.0559 - val_loss: 0.2667 - val_accuracy: 0.9852 - val_mse: 0.0397\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.3392 - accuracy: 0.9106 - mse: 0.0556 - val_loss: 0.2571 - val_accuracy: 0.9852 - val_mse: 0.0376\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.3680 - accuracy: 0.8957 - mse: 0.0619 - val_loss: 0.2675 - val_accuracy: 0.9704 - val_mse: 0.0404\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3373 - accuracy: 0.9050 - mse: 0.0557 - val_loss: 0.2801 - val_accuracy: 0.9185 - val_mse: 0.0438\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3363 - accuracy: 0.9088 - mse: 0.0552 - val_loss: 0.2880 - val_accuracy: 0.9037 - val_mse: 0.0461\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3265 - accuracy: 0.9236 - mse: 0.0520 - val_loss: 0.2800 - val_accuracy: 0.9185 - val_mse: 0.0445\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3360 - accuracy: 0.9181 - mse: 0.0552 - val_loss: 0.2639 - val_accuracy: 0.9259 - val_mse: 0.0409\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3197 - accuracy: 0.9218 - mse: 0.0522 - val_loss: 0.2429 - val_accuracy: 0.9704 - val_mse: 0.0364\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3156 - accuracy: 0.9162 - mse: 0.0519 - val_loss: 0.2298 - val_accuracy: 0.9704 - val_mse: 0.0337\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3183 - accuracy: 0.9125 - mse: 0.0523 - val_loss: 0.2291 - val_accuracy: 0.9704 - val_mse: 0.0337\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.3068 - accuracy: 0.9162 - mse: 0.0496 - val_loss: 0.2429 - val_accuracy: 0.9704 - val_mse: 0.0369\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3162 - accuracy: 0.9236 - mse: 0.0508 - val_loss: 0.2564 - val_accuracy: 0.9407 - val_mse: 0.0400\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3108 - accuracy: 0.9181 - mse: 0.0507 - val_loss: 0.2571 - val_accuracy: 0.9481 - val_mse: 0.0400\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3069 - accuracy: 0.9181 - mse: 0.0503 - val_loss: 0.2470 - val_accuracy: 0.9704 - val_mse: 0.0373\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.3179 - accuracy: 0.9032 - mse: 0.0525 - val_loss: 0.2291 - val_accuracy: 1.0000 - val_mse: 0.0329\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2914 - accuracy: 0.9218 - mse: 0.0480 - val_loss: 0.2114 - val_accuracy: 1.0000 - val_mse: 0.0287\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2835 - accuracy: 0.9255 - mse: 0.0448 - val_loss: 0.2028 - val_accuracy: 1.0000 - val_mse: 0.0268\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2784 - accuracy: 0.9255 - mse: 0.0449 - val_loss: 0.1994 - val_accuracy: 1.0000 - val_mse: 0.0262\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 0.2838 - accuracy: 0.9199 - mse: 0.0461 - val_loss: 0.2028 - val_accuracy: 1.0000 - val_mse: 0.0271\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2933 - accuracy: 0.9181 - mse: 0.0485 - val_loss: 0.2076 - val_accuracy: 1.0000 - val_mse: 0.0285\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2738 - accuracy: 0.9143 - mse: 0.0453 - val_loss: 0.2070 - val_accuracy: 1.0000 - val_mse: 0.0287\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2804 - accuracy: 0.9255 - mse: 0.0456 - val_loss: 0.2058 - val_accuracy: 1.0000 - val_mse: 0.0287\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2618 - accuracy: 0.9292 - mse: 0.0412 - val_loss: 0.2114 - val_accuracy: 1.0000 - val_mse: 0.0302\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.2672 - accuracy: 0.9292 - mse: 0.0441 - val_loss: 0.1997 - val_accuracy: 1.0000 - val_mse: 0.0278\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2684 - accuracy: 0.9181 - mse: 0.0436 - val_loss: 0.1772 - val_accuracy: 1.0000 - val_mse: 0.0228\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2667 - accuracy: 0.9255 - mse: 0.0435 - val_loss: 0.1656 - val_accuracy: 1.0000 - val_mse: 0.0204\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2652 - accuracy: 0.9218 - mse: 0.0428 - val_loss: 0.1680 - val_accuracy: 1.0000 - val_mse: 0.0211\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2688 - accuracy: 0.9255 - mse: 0.0439 - val_loss: 0.1800 - val_accuracy: 1.0000 - val_mse: 0.0238\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2605 - accuracy: 0.9274 - mse: 0.0422 - val_loss: 0.1888 - val_accuracy: 1.0000 - val_mse: 0.0258\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2480 - accuracy: 0.9330 - mse: 0.0405 - val_loss: 0.1839 - val_accuracy: 1.0000 - val_mse: 0.0247\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2489 - accuracy: 0.9311 - mse: 0.0397 - val_loss: 0.1726 - val_accuracy: 1.0000 - val_mse: 0.0221\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2536 - accuracy: 0.9255 - mse: 0.0414 - val_loss: 0.1552 - val_accuracy: 1.0000 - val_mse: 0.0183\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2513 - accuracy: 0.9292 - mse: 0.0408 - val_loss: 0.1437 - val_accuracy: 1.0000 - val_mse: 0.0159\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2509 - accuracy: 0.9255 - mse: 0.0411 - val_loss: 0.1325 - val_accuracy: 1.0000 - val_mse: 0.0136\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.2456 - accuracy: 0.9311 - mse: 0.0386 - val_loss: 0.1298 - val_accuracy: 1.0000 - val_mse: 0.0129\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2440 - accuracy: 0.9274 - mse: 0.0397 - val_loss: 0.1349 - val_accuracy: 1.0000 - val_mse: 0.0137\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2419 - accuracy: 0.9255 - mse: 0.0395 - val_loss: 0.1454 - val_accuracy: 1.0000 - val_mse: 0.0156\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2372 - accuracy: 0.9330 - mse: 0.0386 - val_loss: 0.1431 - val_accuracy: 1.0000 - val_mse: 0.0152\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2316 - accuracy: 0.9348 - mse: 0.0371 - val_loss: 0.1310 - val_accuracy: 1.0000 - val_mse: 0.0131\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.2371 - accuracy: 0.9330 - mse: 0.0384 - val_loss: 0.1200 - val_accuracy: 1.0000 - val_mse: 0.0113\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2289 - accuracy: 0.9348 - mse: 0.0369 - val_loss: 0.1143 - val_accuracy: 1.0000 - val_mse: 0.0105\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2377 - accuracy: 0.9348 - mse: 0.0382 - val_loss: 0.1186 - val_accuracy: 1.0000 - val_mse: 0.0114\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2387 - accuracy: 0.9292 - mse: 0.0391 - val_loss: 0.1238 - val_accuracy: 1.0000 - val_mse: 0.0123\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2363 - accuracy: 0.9348 - mse: 0.0379 - val_loss: 0.1318 - val_accuracy: 1.0000 - val_mse: 0.0138\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2363 - accuracy: 0.9311 - mse: 0.0385 - val_loss: 0.1370 - val_accuracy: 1.0000 - val_mse: 0.0147\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2213 - accuracy: 0.9367 - mse: 0.0367 - val_loss: 0.1309 - val_accuracy: 1.0000 - val_mse: 0.0135\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2191 - accuracy: 0.9348 - mse: 0.0358 - val_loss: 0.1282 - val_accuracy: 1.0000 - val_mse: 0.0129\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2293 - accuracy: 0.9292 - mse: 0.0374 - val_loss: 0.1251 - val_accuracy: 1.0000 - val_mse: 0.0122\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.2177 - accuracy: 0.9385 - mse: 0.0356 - val_loss: 0.1156 - val_accuracy: 1.0000 - val_mse: 0.0107\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 26ms/step - loss: 0.2162 - accuracy: 0.9404 - mse: 0.0349 - val_loss: 0.1047 - val_accuracy: 1.0000 - val_mse: 0.0090\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2242 - accuracy: 0.9423 - mse: 0.0362 - val_loss: 0.0931 - val_accuracy: 1.0000 - val_mse: 0.0073\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.2121 - accuracy: 0.9330 - mse: 0.0350 - val_loss: 0.0865 - val_accuracy: 1.0000 - val_mse: 0.0064\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.2165 - accuracy: 0.9330 - mse: 0.0355 - val_loss: 0.0861 - val_accuracy: 1.0000 - val_mse: 0.0063\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.2027 - accuracy: 0.9441 - mse: 0.0327 - val_loss: 0.0856 - val_accuracy: 1.0000 - val_mse: 0.0063\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2106 - accuracy: 0.9404 - mse: 0.0341 - val_loss: 0.0871 - val_accuracy: 1.0000 - val_mse: 0.0065\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2051 - accuracy: 0.9404 - mse: 0.0336 - val_loss: 0.0947 - val_accuracy: 1.0000 - val_mse: 0.0076\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2120 - accuracy: 0.9441 - mse: 0.0348 - val_loss: 0.0980 - val_accuracy: 1.0000 - val_mse: 0.0081\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.2080 - accuracy: 0.9497 - mse: 0.0329 - val_loss: 0.0943 - val_accuracy: 1.0000 - val_mse: 0.0075\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1955 - accuracy: 0.9404 - mse: 0.0318 - val_loss: 0.0867 - val_accuracy: 1.0000 - val_mse: 0.0063\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.2006 - accuracy: 0.9460 - mse: 0.0320 - val_loss: 0.0793 - val_accuracy: 1.0000 - val_mse: 0.0053\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.2031 - accuracy: 0.9367 - mse: 0.0344 - val_loss: 0.0781 - val_accuracy: 1.0000 - val_mse: 0.0052\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1922 - accuracy: 0.9385 - mse: 0.0313 - val_loss: 0.0779 - val_accuracy: 1.0000 - val_mse: 0.0051\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1967 - accuracy: 0.9479 - mse: 0.0315 - val_loss: 0.0773 - val_accuracy: 1.0000 - val_mse: 0.0051\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1921 - accuracy: 0.9441 - mse: 0.0309 - val_loss: 0.0756 - val_accuracy: 1.0000 - val_mse: 0.0049\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1955 - accuracy: 0.9479 - mse: 0.0314 - val_loss: 0.0809 - val_accuracy: 1.0000 - val_mse: 0.0055\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.1950 - accuracy: 0.9367 - mse: 0.0323 - val_loss: 0.0849 - val_accuracy: 1.0000 - val_mse: 0.0061\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1810 - accuracy: 0.9516 - mse: 0.0289 - val_loss: 0.0854 - val_accuracy: 1.0000 - val_mse: 0.0063\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1946 - accuracy: 0.9423 - mse: 0.0314 - val_loss: 0.0835 - val_accuracy: 1.0000 - val_mse: 0.0061\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1767 - accuracy: 0.9460 - mse: 0.0287 - val_loss: 0.0775 - val_accuracy: 1.0000 - val_mse: 0.0054\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1768 - accuracy: 0.9590 - mse: 0.0275 - val_loss: 0.0678 - val_accuracy: 1.0000 - val_mse: 0.0042\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1846 - accuracy: 0.9516 - mse: 0.0298 - val_loss: 0.0635 - val_accuracy: 1.0000 - val_mse: 0.0037\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1920 - accuracy: 0.9423 - mse: 0.0325 - val_loss: 0.0630 - val_accuracy: 1.0000 - val_mse: 0.0037\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1898 - accuracy: 0.9423 - mse: 0.0316 - val_loss: 0.0615 - val_accuracy: 1.0000 - val_mse: 0.0035\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1800 - accuracy: 0.9460 - mse: 0.0295 - val_loss: 0.0632 - val_accuracy: 1.0000 - val_mse: 0.0038\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1846 - accuracy: 0.9404 - mse: 0.0305 - val_loss: 0.0686 - val_accuracy: 1.0000 - val_mse: 0.0045\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1742 - accuracy: 0.9516 - mse: 0.0282 - val_loss: 0.0723 - val_accuracy: 1.0000 - val_mse: 0.0049\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 22ms/step - loss: 0.1916 - accuracy: 0.9404 - mse: 0.0321 - val_loss: 0.0732 - val_accuracy: 1.0000 - val_mse: 0.0050\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1847 - accuracy: 0.9479 - mse: 0.0300 - val_loss: 0.0683 - val_accuracy: 1.0000 - val_mse: 0.0044\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1779 - accuracy: 0.9534 - mse: 0.0284 - val_loss: 0.0649 - val_accuracy: 1.0000 - val_mse: 0.0039\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.1753 - accuracy: 0.9479 - mse: 0.0285 - val_loss: 0.0616 - val_accuracy: 1.0000 - val_mse: 0.0035\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1767 - accuracy: 0.9441 - mse: 0.0282 - val_loss: 0.0606 - val_accuracy: 1.0000 - val_mse: 0.0034\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1777 - accuracy: 0.9516 - mse: 0.0294 - val_loss: 0.0590 - val_accuracy: 1.0000 - val_mse: 0.0032\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1674 - accuracy: 0.9516 - mse: 0.0272 - val_loss: 0.0562 - val_accuracy: 1.0000 - val_mse: 0.0029\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1718 - accuracy: 0.9460 - mse: 0.0280 - val_loss: 0.0558 - val_accuracy: 1.0000 - val_mse: 0.0029\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1845 - accuracy: 0.9441 - mse: 0.0309 - val_loss: 0.0608 - val_accuracy: 1.0000 - val_mse: 0.0035\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1683 - accuracy: 0.9460 - mse: 0.0285 - val_loss: 0.0669 - val_accuracy: 1.0000 - val_mse: 0.0042\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 25ms/step - loss: 0.1611 - accuracy: 0.9534 - mse: 0.0258 - val_loss: 0.0666 - val_accuracy: 1.0000 - val_mse: 0.0041\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1619 - accuracy: 0.9572 - mse: 0.0264 - val_loss: 0.0638 - val_accuracy: 1.0000 - val_mse: 0.0038\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1609 - accuracy: 0.9590 - mse: 0.0259 - val_loss: 0.0591 - val_accuracy: 1.0000 - val_mse: 0.0032\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1658 - accuracy: 0.9553 - mse: 0.0266 - val_loss: 0.0554 - val_accuracy: 1.0000 - val_mse: 0.0028\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1751 - accuracy: 0.9367 - mse: 0.0309 - val_loss: 0.0556 - val_accuracy: 1.0000 - val_mse: 0.0029\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1495 - accuracy: 0.9609 - mse: 0.0238 - val_loss: 0.0591 - val_accuracy: 1.0000 - val_mse: 0.0033\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1507 - accuracy: 0.9516 - mse: 0.0245 - val_loss: 0.0610 - val_accuracy: 1.0000 - val_mse: 0.0036\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1724 - accuracy: 0.9553 - mse: 0.0272 - val_loss: 0.0582 - val_accuracy: 1.0000 - val_mse: 0.0033\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1479 - accuracy: 0.9572 - mse: 0.0237 - val_loss: 0.0506 - val_accuracy: 1.0000 - val_mse: 0.0025\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1686 - accuracy: 0.9516 - mse: 0.0278 - val_loss: 0.0438 - val_accuracy: 1.0000 - val_mse: 0.0019\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 21ms/step - loss: 0.1584 - accuracy: 0.9516 - mse: 0.0262 - val_loss: 0.0422 - val_accuracy: 1.0000 - val_mse: 0.0017\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1587 - accuracy: 0.9479 - mse: 0.0268 - val_loss: 0.0438 - val_accuracy: 1.0000 - val_mse: 0.0019\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1452 - accuracy: 0.9572 - mse: 0.0229 - val_loss: 0.0456 - val_accuracy: 1.0000 - val_mse: 0.0020\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 0.1577 - accuracy: 0.9534 - mse: 0.0264 - val_loss: 0.0458 - val_accuracy: 1.0000 - val_mse: 0.0021\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1448 - accuracy: 0.9572 - mse: 0.0236 - val_loss: 0.0471 - val_accuracy: 1.0000 - val_mse: 0.0022\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.1455 - accuracy: 0.9665 - mse: 0.0231 - val_loss: 0.0460 - val_accuracy: 1.0000 - val_mse: 0.0021\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1421 - accuracy: 0.9590 - mse: 0.0226 - val_loss: 0.0451 - val_accuracy: 1.0000 - val_mse: 0.0020\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1526 - accuracy: 0.9534 - mse: 0.0248 - val_loss: 0.0429 - val_accuracy: 1.0000 - val_mse: 0.0018\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1437 - accuracy: 0.9572 - mse: 0.0243 - val_loss: 0.0409 - val_accuracy: 1.0000 - val_mse: 0.0016\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1570 - accuracy: 0.9553 - mse: 0.0239 - val_loss: 0.0419 - val_accuracy: 1.0000 - val_mse: 0.0017\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1506 - accuracy: 0.9534 - mse: 0.0248 - val_loss: 0.0401 - val_accuracy: 1.0000 - val_mse: 0.0016\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.1337 - accuracy: 0.9553 - mse: 0.0221 - val_loss: 0.0361 - val_accuracy: 1.0000 - val_mse: 0.0013\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.1530 - accuracy: 0.9516 - mse: 0.0257 - val_loss: 0.0360 - val_accuracy: 1.0000 - val_mse: 0.0013\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1357 - accuracy: 0.9628 - mse: 0.0216 - val_loss: 0.0399 - val_accuracy: 1.0000 - val_mse: 0.0016\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1405 - accuracy: 0.9646 - mse: 0.0218 - val_loss: 0.0416 - val_accuracy: 1.0000 - val_mse: 0.0018\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 24ms/step - loss: 0.1507 - accuracy: 0.9572 - mse: 0.0243 - val_loss: 0.0386 - val_accuracy: 1.0000 - val_mse: 0.0016\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1405 - accuracy: 0.9572 - mse: 0.0227 - val_loss: 0.0362 - val_accuracy: 1.0000 - val_mse: 0.0013\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 17ms/step - loss: 0.1432 - accuracy: 0.9590 - mse: 0.0232 - val_loss: 0.0349 - val_accuracy: 1.0000 - val_mse: 0.0012\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 20ms/step - loss: 0.1250 - accuracy: 0.9628 - mse: 0.0204 - val_loss: 0.0334 - val_accuracy: 1.0000 - val_mse: 0.0011\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 19ms/step - loss: 0.1266 - accuracy: 0.9628 - mse: 0.0206 - val_loss: 0.0335 - val_accuracy: 1.0000 - val_mse: 0.0012\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 0.1368 - accuracy: 0.9609 - mse: 0.0223 - val_loss: 0.0344 - val_accuracy: 1.0000 - val_mse: 0.0013\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "c1IP_1j8RJYW",
        "outputId": "24487dd5-16f9-4919-fed6-0409eff539bd"
      },
      "source": [
        "plot.plot(history.history['accuracy'], label='train')\n",
        "plot.plot(history.history['val_accuracy'], label='test')\n",
        "plot.legend()\n",
        "plot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "K2-dIXLIRUd4",
        "outputId": "0763dbf7-94a0-41ae-b120-ffc9262f62ac"
      },
      "source": [
        "plot.plot(history.history['loss'], label='train')\n",
        "plot.plot(history.history['val_loss'], label='test')\n",
        "plot.legend()\n",
        "plot.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}